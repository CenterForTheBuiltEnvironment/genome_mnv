df_tmy <- rbind(df_tmy, df)
}
return(df_tmy)
}
# Function defined to adjust the plot scale
get_scale <- function(eload, range = 2){
min_y <- mean(eload, na.rm = T) - range * sd(eload, na.rm = T)
max_y <- mean(eload, na.rm = T) + range * sd(eload, na.rm = T)
return(c(min_y, max_y))
}
# Function defined to add chwst reset intervention
run_reset <- function(df_baseline){
mean <- mean(df_baseline$base_eload, na.rm = T) * ctr_params$chwl_perc
grad <- (ctr_params$swt_knots[2] - ctr_params$swt_knots[1]) /
(ctr_params$weather_knots[2] - ctr_params$weather_knots[1])
interc <- ctr_params$swt_knots[2] - (ctr_params$weather_knots[2] * grad)
df_interv <- df_baseline %>%
mutate(swt = t_out * grad + interc,
chwl = mean,
hour = hour(datetime)) %>%
mutate(swt = ifelse(swt > ctr_params$swt_knots[1], ctr_params$swt_knots[1], ifelse(swt < ctr_params$swt_knots[2], ctr_params$swt_knots[2], swt)),
temp_savings = ifelse(t_out >= ctr_params$enable_temp, (swt - ctr_params$conv_swt) * ctr_params$step_perc, 0),
time_adj = ifelse(hour %in% ctr_params$peak_hours, ctr_params$coe_peak, ctr_params$coe_off),
perc_savings = temp_savings * time_adj,
savings = chwl * perc_savings,
interv_eload = base_eload - savings) %>%
select(datetime, base_eload, interv_eload, t_out)
return(df_interv)
}
# Function defined to interpolate NAs
run_interpo <- function(df_all){
na_counts <- df_all %>%
mutate(date = as.Date(timestamp)) %>%
group_by(date) %>%
summarize(na_hours = sum(is.na(eload)))
# Filter out days with more than half of the hours having NAs
valid_days <- na_counts %>%
filter(na_hours <= 12) %>%
pull(date)
df_filtered <- df_all %>%
filter(as.Date(timestamp) %in% valid_days)
df_filtered <- df_filtered %>%
mutate(across(c(eload, t_out), ~ zoo::na.approx(., na.rm = FALSE))) %>%
rename(datetime = timestamp,
base_eload = eload)
return(df_filtered)
}
# Function defined to find end of blocking period
get_eob <- function(sprt_res, sprt_overlap_base, sprt_overlap_interv){
sprt_check <- sprt_res %>% filter(flag == 1) %>% slice(1) %>% .$n_weeks
bt_check <- sprt_overlap_base %>% filter(flag == 1) %>% .$n_weeks
it_check <- sprt_overlap_interv %>% filter(flag == 1) %>% .$n_weeks
eob <- seq(block_params$block_unit, sprt_param$n_weeks, by = block_params$block_unit)
return(eob[eob >= max(c(sprt_check, bt_check, it_check))][1])
}
# Function defined to get sequential test results timeline
get_timeline <- function(sprt_res, sprt_overlap_base, sprt_overlap_interv){
sprt <- sprt_res %>% filter(flag == 1) %>% slice(1) %>% .$n_weeks
if (identical(sprt, integer(0))) {
sprt <- 48
}
return(list(name = name,
site = site,
sprt = sprt,
base_temp = sprt_overlap_base %>% filter(flag == 1) %>% .$n_weeks,
interv_temp = sprt_overlap_interv %>% filter(flag == 1) %>% .$n_weeks,
eob = get_eob(sprt_res, sprt_overlap_base, sprt_overlap_interv),
final = sprt_param$n_weeks))
}
# Function defined to get sequential mean difference savings
get_mdsaving <- function(timeline, sprt_res){
sprt_check <- timeline$sprt
temp_check <- max(timeline$base_temp, timeline$interv_temp)
eob <- timeline$eob
final <- sprt_param$n_weeks
return(list(name = name,
site = site,
sprt = sprt_res %>% filter(n_weeks == sprt_check) %>% .$ns_stat,
temp = sprt_res %>% filter(n_weeks == temp_check) %>% .$ns_stat,
eob = sprt_res %>% filter(n_weeks == eob) %>% .$ns_stat,
final = sprt_res %>% filter(n_weeks == final) %>% .$ns_stat))
}
# Function defined to get sequential normalized annual savings
get_nmsaving <- function(timeline, sprt_res){
sprt_check <- timeline$sprt
temp_check <- max(timeline$base_temp, timeline$interv_temp)
eob <- timeline$eob
final <- sprt_param$n_weeks
return(list(name = name,
site = site,
sprt = sprt_res %>% filter(n_weeks == sprt_check) %>% .$annual_saving,
temp = sprt_res %>% filter(n_weeks == temp_check) %>% .$annual_saving,
eob = sprt_res %>% filter(n_weeks == eob) %>% .$annual_saving,
final = sprt_res %>% filter(n_weeks == final) %>% .$annual_saving))
}
# Function defined to get sequential fractional savings
get_frsaving <- function(timeline, dataframe){
sprt_check <- timeline$sprt
temp_check <- max(timeline$base_temp, timeline$interv_temp)
eob <- timeline$eob
final <- sprt_param$n_weeks
return_l <- list()
for (i in c(sprt_check, temp_check, eob, final)){
df <- dataframe %>%
mutate(week = interval(min(datetime), datetime) %>% as.numeric('weeks') %>% floor()) %>%
filter(week <= i)
# saving calculation as mean difference
FS_rand <- (mean(df %>% filter(strategy == 1) %>% .$eload) -
mean(df %>% filter(strategy == 2) %>% .$eload)) /
mean(df %>% filter(strategy == 1) %>% .$eload) * 100
return_l <- c(return_l, FS_rand)
}
return(list(name = name,
site = site,
sprt = return_l[[1]],
temp = return_l[[2]],
eob = return_l[[3]],
final = return_l[[4]]))
}
#### READ DATA ####
readfile_path <- str_glue("../readfiles/{run_params$type}/")
summaryfigs_path <- str_glue("../figs/{run_params$type}/site_summary/")
combifigs_path <- str_glue("../figs/{run_params$type}/comb_analysis/")
df_energy <- read_rds(paste0(readfile_path, "df_energy.rds"))
df_meta <- read_rds(paste0(readfile_path, "df_meta.rds"))
df_weather <- read_rds(paste0(readfile_path, "df_weather.rds"))
all_sites <- df_energy %>%
select(site) %>%
distinct() %>%
arrange(site)
all_types <- df_energy %>%
select(type) %>%
mutate(type = as.factor(type)) %>%
distinct()
all_names <- df_energy %>%
select(name) %>%
distinct(name)
df_tmy <- get_tmy(all_sites$site)
if (run_params$interval){
exclude <- data.frame(date = NA)
# 2-day consecutive sampling
schedule <- data.frame()
for (sample_start in as.list(seq(as.Date(block_params$start_date), as.Date(block_params$start_date) + weeks(block_params$n_weeks), by = block_params$block_unit * 7))){
sample_end = sample_start + weeks(block_params$block_unit) - days(1)
block <- rand_seq(sample_start, sample_end, 2, 2, exclude)
schedule <- rbind(schedule, block$decision_data)
}
df_schedule_2 <- schedule %>%
rename(datetime = date)
# 3-day consecutive sampling
schedule <- data.frame()
for (sample_start in as.list(seq(as.Date(block_params$start_date), as.Date(block_params$start_date) + weeks(block_params$n_weeks), by = block_params$block_unit * 7))){
sample_end = sample_start + weeks(block_params$block_unit) - days(1)
block <- rand_seq(sample_start, sample_end, 2, 2, exclude)
schedule <- rbind(schedule, block$decision_data)
}
df_schedule_3 <- schedule %>%
rename(datetime = date)
}
#### INDIVIDUAL ####
# storing results
FS_ref <- list()
MD_ref <- list()
model_acc <- list()
seq_timeline <- list()
seq_mdsaving <- list()
seq_nmsaving <- list()
seq_frsaving <- list()
cont_saving <- list()
interval_saving <- list()
FS_tmy <- list()
n = 1
name <- all_names$name[n]
site_info <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(site, type) %>%
distinct()
site <- site_info$site
ifelse(!dir.exists(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), dir.create(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), FALSE)
sitefigs_path <- str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}")
site_weather <- df_weather %>%
filter(site == site_info$site) %>%
select(timestamp, t_out) %>%
group_by(timestamp) %>%
summarise(t_out = mean(t_out)) %>%
ungroup()
site_tmy <- df_tmy %>%
filter(site == site_info$site)
df_all <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(timestamp, eload) %>%
left_join(site_weather, by = "timestamp")
# length check
if (nrow(df_all) != (366 + 365) * 24){
print("Incomplete/duplicate timestamp, please check")
} else {
print(paste0(name, " at ", site_info$site, " start"))
}
# Linear interpolation of baseline
df_all <- df_all %>%
run_interpo()
plot_scale <- get_scale(df_all$base_eload)
df_hourly_conv <- df_all %>%
run_reset()
# separate baseline and intervention
df_base_conv <- df_hourly_conv %>%
select(datetime,
eload = base_eload,
t_out) %>%
drop_na()
df_interv_conv <- df_hourly_conv %>%
select(datetime,
eload = interv_eload,
t_out) %>%
drop_na()
# Check prediction accuracy
towt_base <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
model_fit()
df_towt <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
select(time = datetime,
temp = t_out,
eload)
base_proj <- model_pred(df_towt, towt_base) %>%
rename("datetime" = "time") %>%
mutate(error = eload - towt)
cv_rmse <- mean(sqrt(base_proj$error ^ 2)) / mean(base_proj$eload) * 100
base_proj %>%
ggplot() +
geom_point(aes(x = datetime, y = eload, color = "Measurement"), alpha = 0.2, size = 0.2) +
geom_point(aes(x = datetime, y = towt, color = "Prediction"), alpha = 0.2, size = 0.2) +
geom_smooth(aes(x = datetime, y = eload, color = "Measurement"), formula = y ~ x, method = "loess", linewidth = 0.7) +
geom_smooth(aes(x = datetime, y = towt, color = "Prediction"), formula = y ~ x, method = "loess", linewidth = 0.7) +
annotate(geom = "text",
x = median(base_proj$datetime),
y = median(base_proj$eload) + 2 * sd(base_proj$eload),
label = paste0("CV(RMSE): ", round(cv_rmse, digits = 2), "%")) +
scale_color_brewer(palette = "Set1") +
scale_x_datetime(date_breaks = "2 months",
date_labels = "%b")  +
scale_y_continuous(expand = c(0, 0),
breaks = breaks_pretty(n = 3),
labels = number_format(suffix = " kW")) +
labs(x = NULL,
y = NULL,
color = NULL,
title = "TOWT model prediction results") +
theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
legend.direction = "horizontal",
legend.position = "bottom",
plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
# TOWT baseline project for post retrofit period
towt_base <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
model_fit()
#### RANDOMIZATION ####
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1: 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_schedule
#### RANDOMIZATION ####
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1: 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_schedule
#### RANDOMIZATION ####
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1, 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_schedule
#### RANDOMIZATION ####
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1, 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_schedule
#### RANDOMIZATION ####
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1: 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_rand <- df_hourly_conv %>%
left_join(df_schedule, by = "datetime") %>%
fill(strategy, .direction = "down") %>%
filter(datetime <= as.Date("2016-01-01") + weeks(block_params$n_weeks)) %>%
pivot_longer(c(base_eload, interv_eload), names_to = "eload_type", values_to = "eload") %>%
filter((strategy == 1 & eload_type == "base_eload") | (strategy == 2 & eload_type == "interv_eload")) %>%
select(-eload_type) %>%
drop_na()
name <- all_names$name[n]
site_info <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(site, type) %>%
distinct()
site <- site_info$site
ifelse(!dir.exists(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), dir.create(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), FALSE)
sitefigs_path <- str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}")
site_weather <- df_weather %>%
filter(site == site_info$site) %>%
select(timestamp, t_out) %>%
group_by(timestamp) %>%
summarise(t_out = mean(t_out)) %>%
ungroup()
site_tmy <- df_tmy %>%
filter(site == site_info$site)
df_all <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(timestamp, eload) %>%
left_join(site_weather, by = "timestamp")
# length check
if (nrow(df_all) != (366 + 365) * 24){
print("Incomplete/duplicate timestamp, please check")
} else {
print(paste0(name, " at ", site_info$site, " start"))
}
# Linear interpolation of baseline
df_all <- df_all %>%
run_interpo()
plot_scale <- get_scale(df_all$base_eload)
df_hourly_conv <- df_all %>%
mutate(interv_eload = base_eload)
# separate baseline and intervention
df_base_conv <- df_hourly_conv %>%
select(datetime,
eload = base_eload,
t_out) %>%
drop_na()
df_interv_conv <- df_hourly_conv %>%
select(datetime,
eload = interv_eload,
t_out) %>%
drop_na()
# Check prediction accuracy
towt_base <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
model_fit()
df_towt <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
select(time = datetime,
temp = t_out,
eload)
base_proj <- model_pred(df_towt, towt_base) %>%
rename("datetime" = "time")
# TOWT baseline project for post retrofit period
towt_base <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
model_fit()
df_towt <- df_interv_conv %>%
filter(datetime >= as.Date("2017-01-01")) %>%
select(time = datetime,
temp = t_out,
eload)
base_proj <- model_pred(df_towt, towt_base) %>%
rename("datetime" = "time")
# Repeat randomization
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1: 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_rand <- df_hourly_conv %>%
left_join(df_schedule, by = "datetime") %>%
fill(strategy, .direction = "down") %>%
filter(datetime <= as.Date("2016-01-01") + weeks(block_params$n_weeks)) %>%
pivot_longer(c(base_eload, interv_eload), names_to = "eload_type", values_to = "eload") %>%
filter((strategy == 1 & eload_type == "base_eload") | (strategy == 2 & eload_type == "interv_eload")) %>%
select(-eload_type) %>%
drop_na()
# saving calculation as mean difference
FS_true <- (mean(df_base_conv$eload) - mean(df_interv_conv$eload)) / mean(df_base_conv$eload) * 100
FS_conv <- (mean(base_proj$towt) - mean(base_proj$eload)) / mean(base_proj$towt) * 100
FS_rand <- (mean(df_rand %>% filter(strategy == 1) %>% .$eload) -
mean(df_rand %>% filter(strategy == 2) %>% .$eload)) /
mean(df_rand %>% filter(strategy == 1) %>% .$eload) * 100
MD_true <- mean(df_base_conv$eload) - mean(df_interv_conv$eload)
MD_conv <- mean(base_proj$towt) - mean(base_proj$eload)
MD_rand <- mean(df_rand %>% filter(strategy == 1) %>% .$eload) -
mean(df_rand %>% filter(strategy == 2) %>% .$eload)
FS_ref_nsprt[[n]] <- tibble("name" = name,
"site" = site,
"ref_true" = FS_true,
"ref_conv" = FS_conv,
"ref_rand" = FS_rand)
# get true savings
df_week <- df_base_conv %>%
select(datetime,
base_eload = eload) %>%
left_join(df_interv_conv, by = "datetime") %>%
mutate(savings = eload - base_eload) %>%
mutate(week = interval(min(datetime), datetime) %>% as.numeric('weeks') %>% floor()) %>%
filter(week <= sprt_param$n_weeks)
true_saving_nsprt <- list()
for (i in 2:sprt_param$n_weeks){
saving <- df_week %>%
filter(week <= i)
true_saving_nsprt[[i]] <- tibble("n_weeks" = i,
savings = mean(saving %>% .$savings))
}
true_saving_nsprt <- bind_rows(true_saving_nsprt)
seq_res <- try(seq_run(sprt_param, df_rand, site_tmy), silent = TRUE)
if (inherits(seq_res, "try-error")) {
message("An error occurred. Skipping this part...")
failed_sprt <- failed_sprt + 1
} else {
annual_saving <- seq_res$annual_saving
df_means <- seq_res$df_means
sprt_res <- seq_res$sprt_res
sprt_overlap_base <- seq_res$sprt_overlap_base
sprt_overlap_interv <- seq_res$sprt_overlap_interv
# get sequential test timeline
eob <- get_eob(sprt_res, sprt_overlap_base, sprt_overlap_interv)
seq_timeline_nsprt[[n]] <- get_timeline(sprt_res, sprt_overlap_base, sprt_overlap_interv)
# plot overall results
seq_plot(df_means, sprt_res, sprt_overlap_base, sprt_overlap_interv, annual_saving, true_saving_nsprt, eob)
ggsave(filename = "overall_seq_nsprt.png", path = sitefigs_path, units = "in", height = 9, width = 8, dpi = 300)
# savings at timeline
seq_mdsaving_nsprt[[n]] <- get_mdsaving(seq_timeline_nsprt[[n]], sprt_res)
seq_nmsaving_nsprt[[n]] <- get_nmsaving(seq_timeline_nsprt[[n]], sprt_res)
seq_frsaving_nsprt[[n]] <- get_frsaving(seq_timeline_nsprt[[n]], df_rand)
}
# running on 2-day sampling interval
df_rand_new <- df_hourly_conv %>%
left_join(df_schedule_2, by = "datetime") %>%
fill(strategy, .direction = "down") %>%
filter(datetime <= as.Date("2016-01-01") + weeks(block_params$n_weeks)) %>%
pivot_longer(c(base_eload, interv_eload), names_to = "eload_type", values_to = "eload") %>%
filter((strategy == 1 & eload_type == "base_eload") | (strategy == 2 & eload_type == "interv_eload")) %>%
select(-eload_type) %>%
drop_na()
# saving calculation as mean difference
rand_fs_2 <- (mean(df_rand_new %>% filter(strategy == 1) %>% .$eload) -
mean(df_rand_new %>% filter(strategy == 2) %>% .$eload)) /
mean(df_rand_new %>% filter(strategy == 1) %>% .$eload) * 100
# savings at timeline
rand_md_2 <- mean(df_rand_new %>% filter(strategy == 1) %>% .$eload) - mean(df_rand_new %>% filter(strategy == 2) %>% .$eload)
rand_tmy_2 <- saving_norm(df_rand_new %>% mutate(week = NA), site_tmy)
# running on 3-day sampling interval
df_rand_new <- df_hourly_conv %>%
left_join(df_schedule_3, by = "datetime") %>%
fill(strategy, .direction = "down") %>%
filter(datetime <= as.Date("2016-01-01") + weeks(block_params$n_weeks)) %>%
pivot_longer(c(base_eload, interv_eload), names_to = "eload_type", values_to = "eload") %>%
filter((strategy == 1 & eload_type == "base_eload") | (strategy == 2 & eload_type == "interv_eload")) %>%
select(-eload_type) %>%
drop_na()
# saving calculation as mean difference
rand_fs_3 <- (mean(df_rand_new %>% filter(strategy == 1) %>% .$eload) -
mean(df_rand_new %>% filter(strategy == 2) %>% .$eload)) /
mean(df_rand_new %>% filter(strategy == 1) %>% .$eload) * 100
# savings at timeline
rand_md_3 <- mean(df_rand_new %>% filter(strategy == 1) %>% .$eload) - mean(df_rand_new %>% filter(strategy == 2) %>% .$eload)
rand_tmy_3 <- saving_norm(df_rand_new %>% mutate(week = NA), site_tmy)
source("~/Documents/Mac/ClassMaterial/ARCH299/Genome/cbe_genome_mnv/code/1_analysis.R", echo=TRUE)
View(df_interval_nsprt)
source("~/Documents/Mac/ClassMaterial/ARCH299/Genome/cbe_genome_mnv/code/1_analysis.R", echo=TRUE)
