parameter = "power_ave",
label = "power",
resamp = c(2, 8),
cont_weeks = 36)
# NRE: Occupancy change
occ_params <- list(change_start = c(1, 5, 9),
change_end = c(4, 8, 12),
change = 20)
#### FUNCTIONS ####
# Function defined to extract legend using get_legend function
get_legend <- function(myggplot){
tmp <- ggplot_gtable(ggplot_build(myggplot))
leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
legend <- tmp$grobs[[leg]]
return(legend)
}
# Function defined to read downloaded tmy files
get_tmy <- function(all_sites){
df_tmy <- data.frame()
for (site in all_sites){
df <- read_csv(paste0(readfile_path, "tmy/", str_glue("{site}.epw")),
skip = 8, col_types = "ddddd-d---------------------------------",
col_names = c("year", "month", "day", "hour", "min", "tmy")) %>%
mutate(year = 2017,
time = ymd_h(paste(paste(year, month, day, sep = "-"), hour, sep = " ")),
temp = tmy) %>%
dplyr::select(time, temp) %>%
mutate(site = site)
df_tmy <- rbind(df_tmy, df)
}
return(df_tmy)
}
# Function defined to adjust the plot scale
get_scale <- function(eload, range = 2){
min_y <- mean(eload, na.rm = T) - range * sd(eload, na.rm = T)
max_y <- mean(eload, na.rm = T) + range * sd(eload, na.rm = T)
return(c(min_y, max_y))
}
# Function defined to add chwst reset intervention
run_reset <- function(df_baseline){
mean <- mean(df_baseline$base_eload, na.rm = T) * ctr_params$chwl_perc
grad <- (ctr_params$swt_knots[2] - ctr_params$swt_knots[1]) /
(ctr_params$weather_knots[2] - ctr_params$weather_knots[1])
interc <- ctr_params$swt_knots[2] - (ctr_params$weather_knots[2] * grad)
df_interv <- df_baseline %>%
mutate(swt = t_out * grad + interc,
chwl = mean,
hour = hour(datetime)) %>%
mutate(swt = ifelse(swt > ctr_params$swt_knots[1], ctr_params$swt_knots[1], ifelse(swt < ctr_params$swt_knots[2], ctr_params$swt_knots[2], swt)),
temp_savings = ifelse(t_out >= ctr_params$enable_temp, (swt - ctr_params$conv_swt) * ctr_params$step_perc, 0),
time_adj = ifelse(hour %in% ctr_params$peak_hours, ctr_params$coe_peak, ctr_params$coe_off),
perc_savings = temp_savings * time_adj,
savings = chwl * perc_savings,
interv_eload = base_eload - savings) %>%
select(datetime, base_eload, interv_eload, t_out)
return(df_interv)
}
# Function defined to interpolate NAs
run_interpo <- function(df_all){
na_counts <- df_all %>%
mutate(date = as.Date(timestamp)) %>%
group_by(date) %>%
summarize(na_hours = sum(is.na(eload)))
# Filter out days with more than half of the hours having NAs
valid_days <- na_counts %>%
filter(na_hours <= 12) %>%
pull(date)
df_filtered <- df_all %>%
filter(as.Date(timestamp) %in% valid_days)
df_filtered <- df_filtered %>%
mutate(across(c(eload, t_out), ~ zoo::na.approx(., na.rm = FALSE))) %>%
rename(datetime = timestamp,
base_eload = eload)
return(df_filtered)
}
# Function defined to find end of blocking period
get_eob <- function(sprt_res, sprt_overlap_base, sprt_overlap_interv){
sprt_check <- sprt_res %>% filter(flag == 1) %>% slice(1) %>% .$n_weeks
bt_check <- sprt_overlap_base %>% filter(flag == 1) %>% .$n_weeks
it_check <- sprt_overlap_interv %>% filter(flag == 1) %>% .$n_weeks
eob <- seq(block_params$block_unit, sprt_param$n_weeks, by = block_params$block_unit)
return(eob[eob >= max(c(sprt_check, bt_check, it_check))][1])
}
# Function defined to get sequential test results timeline
get_timeline <- function(sprt_res, sprt_overlap_base, sprt_overlap_interv){
sprt <- sprt_res %>% filter(flag == 1) %>% slice(1) %>% .$n_weeks
if (identical(sprt, integer(0))) {
sprt <- 48
}
return(list(name = name,
site = site,
sprt = sprt,
base_temp = sprt_overlap_base %>% filter(flag == 1) %>% .$n_weeks,
interv_temp = sprt_overlap_interv %>% filter(flag == 1) %>% .$n_weeks,
eob = get_eob(sprt_res, sprt_overlap_base, sprt_overlap_interv),
final = sprt_param$n_weeks))
}
# Function defined to get sequential mean difference savings
get_mdsaving <- function(timeline, sprt_res){
sprt_check <- timeline$sprt
temp_check <- max(timeline$base_temp, timeline$interv_temp)
eob <- timeline$eob
final <- sprt_param$n_weeks
return(list(name = name,
site = site,
sprt = sprt_res %>% filter(n_weeks == sprt_check) %>% .$ns_stat,
temp = sprt_res %>% filter(n_weeks == temp_check) %>% .$ns_stat,
eob = sprt_res %>% filter(n_weeks == eob) %>% .$ns_stat,
final = sprt_res %>% filter(n_weeks == final) %>% .$ns_stat))
}
# Function defined to get sequential normalized annual savings
get_nmsaving <- function(timeline, sprt_res){
sprt_check <- timeline$sprt
temp_check <- max(timeline$base_temp, timeline$interv_temp)
eob <- timeline$eob
final <- sprt_param$n_weeks
return(list(name = name,
site = site,
sprt = sprt_res %>% filter(n_weeks == sprt_check) %>% .$annual_saving,
temp = sprt_res %>% filter(n_weeks == temp_check) %>% .$annual_saving,
eob = sprt_res %>% filter(n_weeks == eob) %>% .$annual_saving,
final = sprt_res %>% filter(n_weeks == final) %>% .$annual_saving))
}
# Function defined to get sequential fractional savings
get_frsaving <- function(timeline, dataframe){
sprt_check <- timeline$sprt
temp_check <- max(timeline$base_temp, timeline$interv_temp)
eob <- timeline$eob
final <- sprt_param$n_weeks
return_l <- list()
for (i in c(sprt_check, temp_check, eob, final)){
df <- dataframe %>%
mutate(week = interval(min(datetime), datetime) %>% as.numeric('weeks') %>% floor()) %>%
filter(week <= i)
# saving calculation as mean difference
FS_rand <- (mean(df %>% filter(strategy == 1) %>% .$eload) -
mean(df %>% filter(strategy == 2) %>% .$eload)) /
mean(df %>% filter(strategy == 1) %>% .$eload) * 100
return_l <- c(return_l, FS_rand)
}
return(list(name = name,
site = site,
sprt = return_l[[1]],
temp = return_l[[2]],
eob = return_l[[3]],
final = return_l[[4]]))
}
#### READ DATA ####
readfile_path <- str_glue("../readfiles/{run_params$type}/")
summaryfigs_path <- str_glue("../figs/{run_params$type}/site_summary/")
combifigs_path <- str_glue("../figs/{run_params$type}/comb_analysis/")
df_energy <- read_rds(paste0(readfile_path, "df_energy.rds"))
df_meta <- read_rds(paste0(readfile_path, "df_meta.rds"))
df_weather <- read_rds(paste0(readfile_path, "df_weather.rds"))
all_sites <- df_energy %>%
select(site) %>%
distinct() %>%
arrange(site)
all_types <- df_energy %>%
select(type) %>%
mutate(type = as.factor(type)) %>%
distinct()
all_names <- df_energy %>%
select(name) %>%
distinct(name)
df_tmy <- get_tmy(all_sites$site)
# storing results
FS_ref <- list()
FS_occ <- list()
MD_ref <- list()
model_acc <- list()
seq_timeline <- list()
seq_mdsaving <- list()
seq_nmsaving <- list()
seq_frsaving <- list()
cont_mdsaving <- list()
cont_frsaving <- list()
energy <- list()
n = 1
name <- all_names$name[n]
site_info <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(site, type) %>%
distinct()
site <- site_info$site
ifelse(!dir.exists(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), dir.create(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), FALSE)
sitefigs_path <- str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}")
site_weather <- df_weather %>%
filter(site == site_info$site) %>%
select(timestamp, t_out) %>%
group_by(timestamp) %>%
summarise(t_out = mean(t_out)) %>%
ungroup()
site_tmy <- df_tmy %>%
filter(site == site_info$site)
df_all <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(timestamp, eload) %>%
left_join(site_weather, by = "timestamp")
# length check
if (nrow(df_all) != (366 + 365) * 24){
print("Incomplete/duplicate timestamp, please check")
} else {
print(paste0(name, " at ", site_info$site, " start"))
}
# Linear interpolation of baseline
df_all <- df_all %>%
run_interpo()
plot_scale <- get_scale(df_all$base_eload)
df_hourly_conv <- df_all %>%
run_reset()
# power-temp plot
p1 <- df_hourly_conv %>%
pivot_longer(c(base_eload, interv_eload), names_to = "strategy", values_to = "eload") %>%
mutate(strategy = as.factor(strategy),
strategy = recode_factor(strategy, "base_eload" = "Baseline", "interv_eload" = "Intervention")) %>%
ggplot(aes(x = t_out, y = eload, color = strategy)) +
geom_point(data= .%>%
group_by(strategy) %>%
slice_sample(n = 1000),
size = 0.7,
alpha = 0.4,
shape = 16) +
geom_smooth(formula = y ~ x, method = "loess", linewidth = 1.25, alpha = 0.15) +
scale_x_continuous(expand = c(0, 0),
breaks = breaks_pretty(n = 4),
labels = number_format(suffix = " Â°C")) +
scale_y_continuous(expand = c(0, 0),
breaks = breaks_pretty(n = 4),
labels = number_format(suffix = " kW")) +
scale_color_manual(values = ls_colors) +
coord_cartesian(ylim = plot_scale) +
labs(x = NULL,
y = NULL,
color = NULL,
subtitle = "by outdoor drybulb temperature") +
theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
legend.direction = "horizontal",
legend.position = "bottom",
plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
# hour-of-day visualization
p2 <- df_hourly_conv %>%
pivot_longer(c(base_eload, interv_eload), names_to = "strategy", values_to = "eload") %>%
mutate(strategy = as.factor(strategy),
strategy = recode_factor(strategy, "base_eload" = "Baseline", "interv_eload" = "Intervention")) %>%
ggplot(aes(x = hour(datetime), y = eload, color = strategy)) +
geom_point(data= .%>%
group_by(strategy) %>%
slice_sample(n = 1000),
size = 0.7,
alpha = 0.4,
shape = 16) +
geom_smooth(formula = y ~ x, method = "loess", linewidth = 1.25, alpha = 0.15) +
scale_x_continuous(breaks = c(0, 6, 12, 18),
labels = c("12 AM", "6 AM", "12 PM", "6 PM")) +
scale_y_continuous(expand = c(0, 0),
breaks = breaks_pretty(n = 4),
labels = number_format(suffix = " kW")) +
scale_color_manual(values = ls_colors) +
coord_cartesian(ylim = plot_scale) +
labs(x = NULL,
y = NULL,
color = NULL,
subtitle = "by each hour of the day") +
theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
axis.text.y = element_blank(),
legend.direction = "horizontal",
legend.position = "bottom",
plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
ggarrange(p1, p2,
ncol=2, nrow=1,
labels = c("a)", "b)"),
widths = c(1, 1),
align = "v",
common.legend = TRUE,
legend="bottom") +
plot_annotation(title = "Case study building power consumption (hourly average)")
df_hourly_conv
# separate baseline and intervention
df_base_conv <- df_hourly_conv %>%
select(datetime,
eload = base_eload,
t_out) %>%
drop_na()
df_interv_conv <- df_hourly_conv %>%
select(datetime,
eload = interv_eload,
t_out) %>%
drop_na()
df_base_conv
change <- mean(df_base_conv$eload) * occ_params$change
change
change <- mean(df_base_conv$eload) * occ_params$change / 100
change
occ_params$change
change <- mean(df_base_conv$eload) * occ_params$change / 100
change_start <- occ_params$change_start[time]
change_end <- occ_params$change_end[time]
time = 1
change <- mean(df_base_conv$eload) * occ_params$change / 100
change_start <- occ_params$change_start[time]
change_end <- occ_params$change_end[time]
base_pre_meas <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload - change, eload))
interv_pre_true <- df_interv_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload - change, eload))
# Baseline projection
towt_base <- base_pre_meas %>%
model_fit()
df_towt <- df_interv_conv %>%
filter(datetime >= as.Date("2017-01-01")) %>%
select(time = datetime,
temp = t_out,
eload)
base_pos_proj <- model_pred(df_towt, towt_base) %>%
rename("datetime" = "time") %>%
select(datetime, towt, eload)
base_pos_true <- df_base_conv %>%
filter(datetime >= as.Date("2017-01-01"))
interv_pos_meas <- df_interv_conv %>%
filter(datetime >= as.Date("2017-01-01"))
rand <- df_rand %>%
filter(datetime < as.Date("2017-01-01")) %>%
mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload - change, eload)) %>%
rbind(df_rand %>% filter(datetime >= as.Date("2017-01-01")))
#### RANDOMIZATION ####
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1, 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_rand <- df_hourly_conv %>%
left_join(df_schedule, by = "datetime") %>%
fill(strategy, .direction = "down") %>%
filter(datetime <= as.Date("2016-01-01") + weeks(block_params$n_weeks)) %>%
pivot_longer(c(base_eload, interv_eload), names_to = "eload_type", values_to = "eload") %>%
filter((strategy == 1 & eload_type == "base_eload") | (strategy == 2 & eload_type == "interv_eload")) %>%
select(-eload_type) %>%
drop_na()
rand <- df_rand %>%
filter(datetime < as.Date("2017-01-01")) %>%
mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload - change, eload)) %>%
rbind(df_rand %>% filter(datetime >= as.Date("2017-01-01")))
prepost_plot(base_pre_meas,
base_pos_proj,
base_pos_true,
interv_pos_meas,
str_glue("Building energy consumption over a 2-year period\n(2016-{change_start}-1 ~ 2016-{change_end}-30: {change}% change)"))
n = 6
name <- all_names$name[n]
site_info <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(site, type) %>%
distinct()
site <- site_info$site
ifelse(!dir.exists(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), dir.create(file.path(str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}"))), FALSE)
sitefigs_path <- str_glue("../figs/{run_params$type}/site_analysis/{site}/{name}")
site_weather <- df_weather %>%
filter(site == site_info$site) %>%
select(timestamp, t_out) %>%
group_by(timestamp) %>%
summarise(t_out = mean(t_out)) %>%
ungroup()
site_tmy <- df_tmy %>%
filter(site == site_info$site)
df_all <- df_energy %>%
filter(name == all_names$name[n]) %>%
select(timestamp, eload) %>%
left_join(site_weather, by = "timestamp")
# length check
if (nrow(df_all) != (366 + 365) * 24){
print("Incomplete/duplicate timestamp, please check")
} else {
print(paste0(name, " at ", site_info$site, " start"))
}
# Linear interpolation of baseline
df_all <- df_all %>%
run_interpo()
plot_scale <- get_scale(df_all$base_eload)
df_hourly_conv <- df_all %>%
run_reset()
# separate baseline and intervention
df_base_conv <- df_hourly_conv %>%
select(datetime,
eload = base_eload,
t_out) %>%
drop_na()
df_interv_conv <- df_hourly_conv %>%
select(datetime,
eload = interv_eload,
t_out) %>%
drop_na()
# Check prediction accuracy
towt_base <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
model_fit()
df_towt <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
select(time = datetime,
temp = t_out,
eload)
base_proj <- model_pred(df_towt, towt_base) %>%
rename("datetime" = "time") %>%
mutate(error = eload - towt)
cv_rmse <- mean(sqrt(base_proj$error ^ 2)) / mean(base_proj$eload) * 100
base_proj %>%
ggplot() +
geom_point(aes(x = datetime, y = eload, color = "Measurement"), alpha = 0.2, size = 0.2) +
geom_point(aes(x = datetime, y = towt, color = "Prediction"), alpha = 0.2, size = 0.2) +
geom_smooth(aes(x = datetime, y = eload, color = "Measurement"), formula = y ~ x, method = "loess", linewidth = 0.7) +
geom_smooth(aes(x = datetime, y = towt, color = "Prediction"), formula = y ~ x, method = "loess", linewidth = 0.7) +
annotate(geom = "text",
x = median(base_proj$datetime),
y = median(base_proj$eload) + 2 * sd(base_proj$eload),
label = paste0("CV(RMSE): ", round(cv_rmse, digits = 2), "%")) +
scale_color_brewer(palette = "Set1") +
scale_x_datetime(date_breaks = "2 months",
date_labels = "%b")  +
scale_y_continuous(expand = c(0, 0),
breaks = breaks_pretty(n = 3),
labels = number_format(suffix = " kW")) +
labs(x = NULL,
y = NULL,
color = NULL,
title = "TOWT model prediction results") +
theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
legend.direction = "horizontal",
legend.position = "bottom",
plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
# TOWT baseline project for post retrofit period
towt_base <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
model_fit()
df_towt <- df_interv_conv %>%
filter(datetime >= as.Date("2017-01-01")) %>%
select(time = datetime,
temp = t_out,
eload)
base_proj <- model_pred(df_towt, towt_base) %>%
rename("datetime" = "time")
#### RANDOMIZATION ####
schedule <- blocking(start_date = block_params$start_date,
n_weeks = block_params$n_weeks,
n_seasons = block_params$n_seasons,
seed = sample(1, 2^15, 1),
searches = 20,
jumps = 20,
treatments = 2,
consec = 1)
# schedule summary
schedule$weekday_summary
df_schedule <- schedule$schedule %>%
select(datetime = date,
strategy)
df_rand <- df_hourly_conv %>%
left_join(df_schedule, by = "datetime") %>%
fill(strategy, .direction = "down") %>%
filter(datetime <= as.Date("2016-01-01") + weeks(block_params$n_weeks)) %>%
pivot_longer(c(base_eload, interv_eload), names_to = "eload_type", values_to = "eload") %>%
filter((strategy == 1 & eload_type == "base_eload") | (strategy == 2 & eload_type == "interv_eload")) %>%
select(-eload_type) %>%
drop_na()
# Occupancy change
scenario <- 1
tibble_occ <- list()
tibble_occ[[n]] <- tibble("name" = name,
"site" = site)
time = 2
change <- mean(df_base_conv$eload) * occ_params$change / 100
change_start <- occ_params$change_start[time]
change_end <- occ_params$change_end[time]
change
base_pre_meas <- df_base_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload - change, eload))
interv_pre_true <- df_interv_conv %>%
filter(datetime < as.Date("2017-01-01")) %>%
mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload - change, eload))
# Baseline projection
towt_base <- base_pre_meas %>%
model_fit()
df_towt <- df_interv_conv %>%
filter(datetime >= as.Date("2017-01-01")) %>%
select(time = datetime,
temp = t_out,
eload)
base_pos_proj <- model_pred(df_towt, towt_base) %>%
rename("datetime" = "time") %>%
select(datetime, towt, eload)
base_pos_true <- df_base_conv %>%
filter(datetime >= as.Date("2017-01-01"))
interv_pos_meas <- df_interv_conv %>%
filter(datetime >= as.Date("2017-01-01"))
rand <- df_rand %>%
filter(datetime < as.Date("2017-01-01")) %>%
mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload - change, eload)) %>%
rbind(df_rand %>% filter(datetime >= as.Date("2017-01-01")))
prepost_plot(base_pre_meas,
base_pos_proj,
base_pos_true,
interv_pos_meas,
str_glue("Building energy consumption over a 2-year period\n(2016-{change_start}-1 ~ 2016-{change_end}-30: {change}% change)"))
