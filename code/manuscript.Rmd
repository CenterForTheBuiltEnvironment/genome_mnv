---
title: "Reliability Demonstration of A Novel Randomized Measurement and Verification Method for Switchable Control Retrofit Using Large-scale Public Dataset"
author:
  - Aoyu Zou ^[Center for the Built Environment, University of California Berkeley, USA], Paul Raftery ^1^, Stefano Schiavon ^1^, Carlos Durate ^1^
abstract: Conventional measurement and verification (M&V) methods for whole-building energy savings estimation are both time-consuming and unreliable, especially when non-routine events occur during the M&V process. Those events are unrelated to the proposed intervention strategy but have substaintial impacts on the building energy consumption. In this study, we argue that for switchable interventions (e.g. most of the control retrofits) can benefit from random sampling where the analyst randomly decide which strategy (i.e. baseline or intervention) to implement each day. We tested the novel randomized M&V method on a large public dataset which covers multiple climate zones and types of commercial buildings. We applied a virtual chilled water supply temperature reset based on outdoor weather as a control retrofit intervention. Our study shows that the new M&V method can estimate the savings accurately much quicker than the conventional method and most importantly, the estimation results are much more robust compared to the conventional method when non-routine events are present. 

output:
  bookdown::word_document2:
    reference_docx: "../paper/template.docx"
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
biblio-style: apalike

knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../paper"
    )
  })
---

```{r setup, include = FALSE, cache = FALSE}

# knitr setup
knitr::opts_chunk$set(echo = F, 
                      message = F,
                      warning = F,
                      dev = "jpeg",
                      # cache = T,
                      dpi = 300,
                      fig.show = "hold",
                      fig.pos = "b", 
                      fig.path = "../figs/manuscript/figs/")

# str(knitr::opts_chunk$get()) # see for all options

```

```{r prep, include = FALSE, cache = FALSE}
#### LIBRARIES ####
require(pacman)

# load packages using pacman
pacman::p_load(tidyverse, lubridate, here, stats, zoo, scales, ggpubr, patchwork, RColorBrewer)

# turn off scientific notation
options(scipen = 999, digits = 15)

# set directory
here::i_am("manuscript.rmd")

# set default theme for ggplot
theme_set(theme_minimal())

# define base ggplot theme
theme_update(plot.title = element_text(size = 14, colour = "grey20", face = "bold", hjust = 0.5),
             plot.subtitle = element_text(size = 10, colour = "grey20", face = "italic", hjust = 0.5, margin = margin(b = 10)),
             plot.caption = element_text(size = 10, colour = "grey20", face = "italic", hjust = 0.5),
             plot.background = element_rect(fill = "white", colour = NA),
             panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             axis.text = element_text(size = 10),
             strip.text = element_text(size = 10, color = "grey20", face = "bold"),
             strip.background = element_blank())

# colors
ls_colors <- c("Baseline" = "#a6bddb",
               "Measured baseline" = "#a6bddb",
               "Adjusted baseline" = "#a6bddb",
               "Projected baseline\n(no change)" = "#2b8cbe",
               "Intervention" = "#fdbb84",
               "Measured interv" = "#fdbb84",
               "True savings" = "black",
               "True normalized savings" = "black")

# parameters
ctr_params <- list(peak_hours = 10:16,
                   chwl_perc = 0.25,
                   step_perc = 0.08,
                   conv_swt = 6,
                   weather_knots = c(15, 25),
                   swt_knots = c(12, 7),
                   coe_peak = 0.8,
                   coe_off = 1.2,
                   enable_temp = 8)

occ_params <- list(change_start = 5,
                   change_end = 8,
                   change = 20)





#### FUNCTIONS ####
# Function defined to read downloaded tmy files
get_tmy <- function(all_sites, readfile_path){
  
  df_tmy <- data.frame()
  
  for (site in all_sites){
    df <- read_csv(paste0(readfile_path, "tmy/", str_glue("{site}.epw")),
                   skip = 8, col_types = "ddddd-d---------------------------------",
                   col_names = c("year", "month", "day", "hour", "min", "tmy")) %>%
      mutate(year = 2017,
             time = ymd_h(paste(paste(year, month, day, sep = "-"), hour, sep = " ")),
             temp = tmy) %>%
      dplyr::select(time, temp) %>% 
      mutate(site = site)
    
    df_tmy <- rbind(df_tmy, df)
  }
  
  return(df_tmy)
}

# Function defined to add chwst reset intervention
run_reset <- function(df_baseline){
  
  mean <- mean(df_baseline$base_eload, na.rm = T) * ctr_params$chwl_perc
  
  grad <- (ctr_params$swt_knots[2] - ctr_params$swt_knots[1]) / 
    (ctr_params$weather_knots[2] - ctr_params$weather_knots[1])
  
  interc <- ctr_params$swt_knots[2] - (ctr_params$weather_knots[2] * grad)
  
  df_interv <- df_baseline %>% 
    mutate(swt = t_out * grad + interc, 
           chwl = mean,
           hour = hour(datetime)) %>% 
    mutate(swt = ifelse(swt > ctr_params$swt_knots[1], ctr_params$swt_knots[1], ifelse(swt < ctr_params$swt_knots[2], ctr_params$swt_knots[2], swt)), 
           temp_savings = ifelse(t_out >= ctr_params$enable_temp, (swt - ctr_params$conv_swt) * ctr_params$step_perc, 0), 
           time_adj = ifelse(hour %in% ctr_params$peak_hours, ctr_params$coe_peak, ctr_params$coe_off), 
           perc_savings = temp_savings * time_adj, 
           savings = chwl * perc_savings, 
           interv_eload = base_eload - savings) %>% 
    select(datetime, base_eload, interv_eload, t_out)
  
  return(df_interv)
}

# Function defined to interpolate NAs
run_interpo <- function(df_all){
  
  na_counts <- df_all %>%
    mutate(date = date(timestamp)) %>%
    group_by(date) %>%
    summarize(na_hours = sum(is.na(eload)))
  
  # Filter out days with more than half of the hours having NAs
  valid_days <- na_counts %>%
    filter(na_hours <= 12) %>%
    pull(date)
  
  df_filtered <- df_all %>%
    filter(date(timestamp) %in% valid_days)
  
  df_filtered <- df_filtered %>%
    mutate(across(c(eload, t_out), ~ zoo::na.approx(., na.rm = FALSE))) %>% 
    rename(datetime = timestamp, 
           base_eload = eload)
  
  return(df_filtered)
}

# Function defined to adjust the plot scale
get_scale <- function(eload, range = 2){
  
  min_y <- mean(eload, na.rm = T) - range * sd(eload, na.rm = T)
  max_y <- mean(eload, na.rm = T) + range * sd(eload, na.rm = T)
  
  return(c(min_y, max_y))
}
```

```{r readdata, include=FALSE}

#### READ DATA ####
# Tidy dataset
readfile_tidy <- str_glue("../readfiles/tidy/")
fig_path = "../figs/manuscript/"

df_energy_tidy <- read_rds(paste0(readfile_tidy, "df_energy.rds"))
df_meta_tidy <- read_rds(paste0(readfile_tidy, "df_meta.rds"))
df_weather_tidy <- read_rds(paste0(readfile_tidy, "df_weather.rds"))
df_sprt_all_tidy <- read_rds(paste0(readfile_tidy, "df_sprt_all.rds"))
df_seq_FS_tidy <- read_rds(paste0(readfile_tidy, "df_seq_FS.rds"))
df_NRE_occ <- read_rds(paste0(readfile_tidy, "df_NRE_occ.rds"))
df_MD_tidy <- read_rds(paste0(readfile_tidy, "df_MD.rds"))
df_FS_tidy <- read_rds(paste0(readfile_tidy, "df_FS.rds"))
df_eui_tidy <- read_rds(paste0(readfile_tidy, "df_eui.rds"))
df_cont_MD_tidy <- read_rds(paste0(readfile_tidy, "df_cont_MD.rds"))
df_cont_FS_tidy <- read_rds(paste0(readfile_tidy, "df_cont_FS.rds"))
df_model_acc_tidy <- read_rds(paste0(readfile_tidy, "df_model_acc.rds"))

all_sites_tidy <- df_energy_tidy %>%
  select(site) %>%
  distinct() %>%
  arrange(site)

all_types_tidy <- df_energy_tidy %>%
  select(type) %>%
  mutate(type = as.factor(type)) %>%
  distinct()

all_names_tidy <- df_energy_tidy %>%
  select(name) %>%
  distinct(name)

# Messy dataset
readfile_messy <- str_glue("../readfiles/messy/")

df_energy_messy <- read_rds(paste0(readfile_messy, "df_energy.rds"))
df_meta_messy <- read_rds(paste0(readfile_messy, "df_meta.rds"))
df_weather_messy <- read_rds(paste0(readfile_messy, "df_weather.rds"))
df_sprt_all_messy <- read_rds(paste0(readfile_messy, "df_sprt_all.rds"))
df_seq_FS_messy <- read_rds(paste0(readfile_messy, "df_seq_FS.rds"))
df_MD_messy <- read_rds(paste0(readfile_messy, "df_MD.rds"))
df_FS_messy <- read_rds(paste0(readfile_messy, "df_FS.rds"))
df_eui_messy <- read_rds(paste0(readfile_messy, "df_eui.rds"))
df_cont_MD_messy <- read_rds(paste0(readfile_messy, "df_cont_MD.rds"))
df_cont_FS_messy <- read_rds(paste0(readfile_messy, "df_cont_FS.rds"))
df_FS_nsprt <- read_rds(paste0(readfile_messy, "df_FS_nsprt.rds"))
df_MD_nsprt <- read_rds(paste0(readfile_messy, "df_MD_nsprt.rds"))
df_seq_FS_nsprt <- read_rds(paste0(readfile_messy, "df_seq_FS_nsprt.rds"))
df_model_acc_messy <- read_rds(paste0(readfile_messy, "df_model_acc.rds"))

all_sites_messy <- df_energy_messy %>%
  select(site) %>%
  distinct() %>%
  arrange(site)

all_types_messy <- df_energy_messy %>%
  select(type) %>%
  mutate(type = as.factor(type)) %>%
  distinct()

all_names_messy <- df_energy_messy %>%
  select(name) %>%
  distinct(name)

# read functions
function_path <- "../functions/"
source(paste0(function_path, "model_fit.R"))
source(paste0(function_path, "model_pred.R"))
source(paste0(function_path, "prepost_plot.R"))
```

# Introduction

## Background

### Conventional M&V

Measurement and Verification (M&V) is the process of quantifying energy
savings from energy efficiency projects by comparing actual energy
consumption against a baseline, adjusting for factors like weather and
occupancy. This process ensures that improvements in energy performance
are accurately evaluated. In the United States, practitioners often
refer to ASHRAE Guideline 14, the International Performance Measurement
and Verification Protocol (IPMVP), and the Federal Energy Management
Program (FEMP) for standard guidelines [@doe_mv_2008,
@efficiency_valuation_organisation_international_2007,
@ashrae_ashrae_2014]. These guidelines outline standardized methods for
quantifying energy savings, whether through calibrated simulations or
monitored measurements for specific equipment or systems (isolation
methods) or for entire buildings (whole-building methods). In this
study, we will focus on the energy savings quantified at the
whole-building level where the measurements are obtained from utility
bills or whole-building meters. The corresponding M&V process required
by ASHRAE Guideline 14, which we refer to as the convention method in
this paper. Typically, the process begins with baseline measurements
taken over a year before implementing any energy-efficiency retrofit,
followed by the same measurement procedure during the intervention
period. After collecting two years of data, an M&V analyst fits an
energy prediction model, using variables such as outdoor temperature and
time [@mathieu_quantifying_2011] to project baseline energy consumption
in the post-retrofit period. The difference between the counter-factual
baseline and the measured intervention represents the energy savings. A
key drawback of this method is its reliance on a two-year timeline to
quantify savings, during which baseline measurements can become outdated
due to changes in building performance caused by non-routine events
unrelated to the intervention. This limitation reduces the feasibility
of rapid M&V and complicates the quantification of estimation
uncertainty, thus impacting the accuracy and timeliness of savings
assessments.

### Randomized M&V

To address the limitations of conventional M&V methods and the
challenges posed by non-routine events, we propose a novel M&V method
that adopts the randomized crossover design, a concept borrowed from
medical and agricultural studies. Another improvement is that we
proposed a sequential evaluation framework and defined stopping criteria
to end the M&V if the target effect is detected. This is to avoid
unnecessary measurement collection over the full 2-year M&V cycle. The
full framework is detailed in another study with all stopping criteria
outlined <!--# TODO: Add references -->. In summary, this method
provides M&V analysts with a randomized schedule that alternates between
baseline and intervention implementation while ensuring balanced
sampling across days of the week. For example, given a 10-week M&V
period for 1 intervention, the balanced randomized schedule would
equally sample 5 Mondays with the baseline and 5 Mondays with the
intervention. The limitation of the randomized M&V is that it is only
applicable to a subset of retrofit projects such as control
interventions. However, for all applicable use cases, it allows analysts
to detect energy savings sequentially as the study progresses meaning
once the desired savings target is achieved, analysts can terminate the
M&V. The key advantage of randomization, which is one of the study
objectives, is that if control strategies are sampled with equal
probability, the influence of non-routine events is likely to be evenly
distributed between the baseline and intervention measurements. This
means that the savings estimate, calculated as the difference between
the two, effectively 'cancels out' the impact of these disturbances,
leading to a more accurate and unbiased assessment of the intervention's
impact.

### BDG2 dataset

The Building Genome Dataset 2 (BGD2) is an extensive open-access dataset
designed to advance research and development in building energy
efficiency and control strategies acting as a test-bed for modeling,
simulation, and algorithm development [@miller_building_2020]. BGD2
contains over 500 buildings' metadata and realistic operational
information from across North America and Europe, making it one of the
most comprehensive collections of building-related data available for
scientific use. The dataset includes various commercial building types
such as offices, education facilities, public, and retail buildings, and
provides detailed information on their physical characteristics (e.g.
enregy ratings, heating types and floor area) and hourly measurements of
chilled and hot water, electricity, gas usage as well as site outdoor
weather conditions.

For the purpose of this study, we only queried whole-building hourly
electricity usage. All building names and precise locations were erased
but the climate zone and city name were provided, which are shown in
Figure \@ref(fig:tidysite) and Figure \@ref(fig:messysite). In addition,
the measurements were pre-processed by the authors with timestamp
already converted to local time. We further described the data filtering
process in Section \@ref(method).

## Literature review

### Measurement and verification energy-efficient measures

Most research related to M&V for whole-building approach focuses on the
accuracy of baseline modeling, exploring model performance from simple
regression models to more complex machine learning techniques. One study
reviewed various models suitable for M&V applications as well as
selected input features [@alrobaie_review_2022] and another study
provided a definitive methodology to apply machine learning models for
M&V use cases [@gallagher_development_2018]. In addition, a few studies
investigated the critical performance metrics to evaluate the developed
baseline models [@granderson_automated_2015,
@granderson_development_2014] and compared a variety of models using
those metrics [@granderson_accuracy_2016]. These studies made
significant contributions by emphasizing the uncertainty associated with
the model-fitting process, a key factor in accurately determining energy
savings. Furthermore, other researchers addressed this issue by
leveraging statistical formulation and inference to improve baseline
energy models [@burkhart_measurement_2014, @heo_calibration_2012,
@walter_uncertainty_2014]. However, a gap still remains in the
literature regarding the rigorous quantification of uncertainties
directly associated with calculated savings, for instance, accounting
for the potential bias that baseline model might deteriorate (i.e.
becomes 'stale') over an extended period of pre- and post-analysis.

### Impact of non-routine events on building energy usage

A common non-routine event in energy-saving M&V projects is a change in
occupancy or a significant shift in occupant behavior. These changes can
greatly affect measured energy consumption in buildings and are
typically unrelated to the intervention strategy. For instance, during
the COVID-19 pandemic in 2020, most commercial buildings were
unoccupied, leading to a noticeable drop in energy bills despite no
energy-efficiency measures being implemented [@cai_impact_2024,
@chihib_impact_2021, @gaspar_assessing_2022, @kang_changes_2021].
Furthermore, subsequent research has shown that hybrid working modes,
allowing employees to work remotely, have persisted after the outbreak
of the pandemic [@aksoy_working_2022], adding further complexity to
energy consumption patterns due to evolved occupant behaviors with
higher flexibility [@gui_impact_2021, @mantesi_office_2022,
@xie_does_2021]. One study realized the limitation of current M&V
methods, which only consider adjusting for outdoor weather, is
insufficient and emphasized the importance of requiring matched
comparison groups to control for exogenous factors beyond weather
differences when comparing between baseline and intervention
[@demand_side_analytics_population_2022]. Another type of non-routin
event is filter clogging in air handling units due to particle
accumulation. This can cause supply fans to gradually consume more
energy to maintain required duct static pressure [@feng_newly_2019,
@zhai_full-scale_2017]. If M&V analysts are unaware of such changes and
lack an appropriate adjustment method (e.g., replacing filters before
the intervention begins), the savings could be underestimated as
increased energy use is incorrectly attributed to the intervention
rather than the mechanical issue.

### Randomized experimental design in other scientific fields

To address the research gap identified in existing M&V studies,
particularly the challenges posed by non-routine events, we applied a
randomized experimental design method---an approach commonly used in
other scientific disciplines. In clinical trials, for instance,
randomization is employed to determine the effect of a medical treatment
(such as a new drug) by randomly assigning participants to either a
control group (e.g., receiving a placebo) or a treatment group
[@angus_adaptive_2019, @burger_importance_2021, @lim_randomization_2019,
@wiley_crossover_2016]. The primary purpose of randomization is to block
confounding variables such as age, gender, and socioeconomic status when
evaluating treatment effects. In the context of buildings, a "treatment"
refers to a control retrofit, while the "placebo" represents the
existing baseline control. Unlike clinical trials, where participants
are randomly assigned, our approach randomizes treatment assignment
longitudinally for each individual building---an approach known as
n-of-1 trials [@gabler2011]. Although the BDG2 dataset allows for
randomization at a population level (e.g., across climate zones),
building owners are generally more interested in understanding the
effects of control retrofits specific to their own buildings, rather
than at a generalized population level.

## Objectives

As mentioned, the goal of an M&V project is to determine the
effect---typically energy savings---of an energy-efficient intervention.
In this study, we focus primarily on switchable interventions, which
often involve control retrofits. An example of such an intervention is a
control retrofit developed by a software-as-a-service company that
adjusts the chilled water plant's supply water temperature based on
outdoor weather conditions [@roa_field_2023,
@lee_simulationoptimization_2012, @qiu_chilled_2022]. Therefore, we
defined the M&V scenario as follows:

*"A company aims to sell its supply temperature reset control software
package to a customer, such as a building owner, with a guarantee that
it will reduce the building's electricity usage. If the building owner
decides to purchase the service, the company agrees to charge a service
fee based on a percentage of the measured energy savings."*

As required by the M&V scenario, we assessed the performance of both the
conventional and the novel randomized M&V methods by estimating the
intervention energy savings for all valid buildings in the dataset. By
conducting such analysis, we aim to:

1.  Compare the energy saving estimation accuracy between the
    conventional and the randomized method. This study extends the
    comparison to a large sample of buildings, covering a variety of
    types and climate zones. The comparison metrics include both
    estimation accuracy and M&V finishing timeline.

2.  Verify the enhanced robustness of the randomized method. By using
    realistic measurements from real-world buildings, which include
    various sources of noise, we aim to reflect the challenges faced by
    building analysts in real projects. As will be demonstrated in the
    following sections, the randomized approach is less impacted by
    non-routine events (i.e., measurement 'noise'), resulting in more
    reliable energy savings estimates.

3.  Demonstrate the implementation of the proposed randomized M&V method
    using a public available dataset. We ensured the reproducibility of
    the method by making the analysis code open source including
    randomized schedule generation, sequential statistical analysis,
    energy modeling and normalized saving calculation. Using the
    available open resources, building analysts should be able to
    seamlessly integrate and apply them in their own M&V projects.

# Method

We outlined the methodology of the study in Figure \@ref(fig:flowchart)
and extended several key components in this section.

```{r flowchart, fig.cap = "Workflow summary of the methodology of this paper", out.width="50%"}

knitr::include_graphics(paste0(fig_path, "flowchart.png"))
```

## Building filtering

### 'Tidy' subset

In this study, we extracted the electricity measurements from the BDG2
dataset. On a first pass, we filtered out buildings with less noise
based on the following criteria:

1.  Missing values \< 1000: given the hourly resolution of all
    measurements, this is equivalent to 1.5 months of missing days.

2.  Mean electricity usage \> 0 kWh: target buildings should have active
    electricity usage.

3.  No statistical significant difference (P-value \> 0.05) between the
    two-year electricity usage: target buildings should have no change
    in the electricity usage between the two years.

4.  Target buildings should have known site location: buildings with
    anonymous location are excluded due to unavailable typical
    meteorological weather.

5.  Warehouse and parking types are excluded: target buildings have less
    demand flexibility to implement a chilled water setpoint reset
    control.

Therefor the resulting subset contains all buildings with 'tidy'
measurements. Figure \@ref(fig:tidysite) shows in total, the subset
contains `r nrow(all_names_tidy)` buildings in `r nrow(all_types_tidy)`
types from `r nrow(all_sites_tidy)` different climate zones.

```{r tidysite, fig.cap = "Site summary of the tidy building subset", fig.width=8, fig.height=6}

set3 <- colorRampPalette(brewer.pal('Set3',n=12))
type_colors <- setNames(set3(13), all_types_messy$type)
  
p1 <- df_energy_tidy %>%
    group_by(type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, n, .desc = F)) %>%
    ggplot(aes(x = 1, y = n, fill = as.factor(type))) +
    geom_col() +
    geom_text(aes(label = ifelse(n > 10, as.character(n), "")), color = "black", position = position_stack(vjust = 0.5)) +
    scale_y_continuous(expand = c(0, 0),
                       breaks = breaks_pretty(n = 4)) +
    scale_x_discrete(expand = c(0, 0.1)) +
    scale_fill_manual(values = type_colors) +
    labs(x = NULL,
         y = NULL,
         subtitle = "Across all sites",
         fill = NULL) +
    theme(axis.text.x = element_blank(),
          panel.grid.major.y = element_line(color = "grey80"),
          legend.direction = "horizontal",
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  p2 <- df_energy_tidy %>%
    group_by(site, type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    mutate(proportion = n / sum(n),
           total = sum(n),
           ymax = cumsum(proportion),
           ymin = c(0, head(ymax, n = -1))) %>%
    mutate(label_pos = (ymax + ymin) / 2) %>%
    ungroup() %>%
    group_by(type) %>%
    mutate(order = sum(n)) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, order, .desc = F)) %>%
    ggplot(aes(ymax = ymax, ymin = ymin, xmax = 4, xmin = 3, fill = type)) +
    geom_rect() +
    coord_polar(theta = "y") +
    xlim(c(2, 4)) +
    facet_wrap(~ site, nrow = 3) +
    labs(x = NULL,
         y = NULL,
         subtitle = "For each site",
         fill = NULL) +
    scale_fill_manual(values = type_colors) +
    geom_text(aes(x = 3.5, y = label_pos, label = n)) +
    geom_text(aes(x = 2, y = 0, label = paste0("Total\n", total)), color = "black") +
    theme(legend.direction = "horizontal",
          axis.text = element_blank(),
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  ggarrange(p1, p2,
            ncol = 2, nrow = 1,
            widths = c(0.2, 1),
            common.legend = TRUE,
            legend = "bottom") +
    plot_annotation(title = "Case study building type summary")
```

### 'Messy' subset

In reality, the 'tidy' subset is less representative of the measurements
typically collected from existing building stock, as whole-building
electricity measurements collected over two years often show more
variability the the filtering critieron. These variations can stem from
issues like sensor calibration errors or inherent changes in the
building, which, as previously discussed, can bias M&V results.
Therefore, to more realistically assess the robustness of the two M&V
method, we included an additional 'messy' subset which first exclude the
'tidy' subset and then re-apply the filtering rule with one amendment:

3.  $\frac{abs(E_{2017} - E_{2016})}{E_{2016}}< 25\%$: any increase or
    decrease of building electricity usage in the second year should be
    less than 25% of that in th first year.

Figure \@ref(fig:messysite) shows the summary of the 'messy' dataset,
which contains `r nrow(all_names_messy)` buildings in
`r nrow(all_types_messy)` types from `r nrow(all_sites_messy)` different
climate zones.

```{r messysite, fig.cap = "Site summary of the messy building subset", fig.width=8, fig.height=8}

p1 <- df_energy_messy %>%
    group_by(type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, n, .desc = F)) %>%
    ggplot(aes(x = 1, y = n, fill = as.factor(type))) +
    geom_col() +
    geom_text(aes(label = ifelse(n > 10, as.character(n), "")), color = "black", position = position_stack(vjust = 0.5)) +
    scale_y_continuous(expand = c(0, 0),
                       breaks = breaks_pretty(n = 4)) +
    scale_x_discrete(expand = c(0, 0.1)) +
    scale_fill_manual(values = type_colors) +
    labs(x = NULL,
         y = NULL,
         subtitle = "Across all sites",
         fill = NULL) +
    theme(axis.text.x = element_blank(),
          panel.grid.major.y = element_line(color = "grey80"),
          legend.direction = "horizontal",
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  p2 <- df_energy_messy %>%
    group_by(site, type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    mutate(proportion = n / sum(n),
           total = sum(n),
           ymax = cumsum(proportion),
           ymin = c(0, head(ymax, n = -1))) %>%
    mutate(label_pos = (ymax + ymin) / 2) %>%
    ungroup() %>%
    group_by(type) %>%
    mutate(order = sum(n)) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, order, .desc = F)) %>%
    ggplot(aes(ymax = ymax, ymin = ymin, xmax = 4, xmin = 3, fill = type)) +
    geom_rect() +
    coord_polar(theta = "y") +
    xlim(c(2, 4)) +
    facet_wrap(~ site, nrow = 3) +
    labs(x = NULL,
         y = NULL,
         subtitle = "For each site",
         fill = NULL) +
    scale_fill_manual(values = type_colors) +
    geom_text(aes(x = 3.5, y = label_pos, label = n)) +
    geom_text(aes(x = 2, y = 0, label = paste0("Total\n", total)), color = "black") +
    theme(legend.direction = "horizontal",
          axis.text = element_blank(),
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  ggarrange(p1, p2,
            ncol = 2, nrow = 1,
            widths = c(0.2, 1),
            common.legend = TRUE,
            legend = "bottom") +
    plot_annotation(title = "Case study building type summary")
```

## Apply control intervention

Figure \@ref(fig:chwst) shows the algorithm for the proposed control
intervention that reset the chiller supply temperature based on the
outdoor weather conditions, which can be commonly found in the
literature [@lee_chilled_2022, @congradac_recognition_2012]. For both
strategies, we assume that the chiller is activated when the outdoor
temperature exceeds 10°C. The baseline strategy, representing the
existing measurements from the dataset, operates with a constant water
supply temperature. The intervention strategy, as illustrated in the
figure, adjusts the water supply temperature dynamically, resetting it
from 7°C to 12°C.

```{r chwst, fig.cap = "Proposed chilled water supply temperature reset based on outdoor temperature", fig.width=8, fig.height=4}

data.frame(
  OAT = c(10, 15, 25, 30),  
  Baseline = c(6, 6, 6, 6),      
  Intervention = c(12, 12, 7, 7)
  ) %>% 
  pivot_longer(cols = c("Baseline", "Intervention"), names_to = "strategy", values_to = "value") %>% 
  ggplot(aes(x = OAT, y = value, color = strategy)) +
  geom_line() +
  scale_color_manual(values = ls_colors) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = c(4, 6, 8, 10, 12),
                     limits = c(5.5, 12.5),
                     labels = number_format(suffix = " °C")) +
  scale_x_continuous(expand = c(0, 0), 
                     breaks = c(10, 15, 20, 25, 30), 
                     labels = number_format(suffix = " °C")) +
  labs(x = "Outdoor temperature", 
       y = "Chilled water supply temperature", 
       title = "Chilled water supply temperature reset on outdoor temperature") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

```

We mapped the chilled water supply temperature reset to the electrical
energy savings as: $$
\text{savings} = mean(E_{CHW}) \times 25\% \times \text{perc_savings}
$$

We assume on average, HVAC systems account for approximately 50% of a
building's total electricity consumption, and the chilled water plant
further consumes around 50% of the HVAC electricity. While this
assumption largely simplifies the diverse energy usage across various
building types, for the scope of this paper, we assume that 25% of the
total building electricity is used by the chilled water plant, $E_{CHW}$
[@us2012commercial]. Typically, the savings from an intervention are not
proportional to the building's hourly electricity usage, which is
generally the challenge for M&V. To address this, we mapped the
resulting electricity savings as a percentage of the plant's normal
operation, calculated as its mean electricity usage over the two-year
period. This percentage is influenced by factors such as outdoor
temperature ($OAT$), intervention supply water temperature
($T_{interv}$), baseline supply water temperature ($T_{base}$) and hour
of the day ($\delta_{occ}$, binary indicator whether during peak hours
from 9 AM to 4 PM).

$$
\text{perc_savings = }
\begin{cases}
0, & \text{ if } OAT < 10°C \\
\left[ \beta_{\text{temp}} \times (\text{T}_{\text{interv}} - \text{T}_{\text{base}}) \right] \times 
\left[ \beta_{\text{occ}} \times \delta_{occ} + \beta_{\text{unocc}} \times (1 - \delta_{occ})\right], & \text{ otherwise}
\end{cases}
$$

Parameters and their pre-defined values are summarized in the table
below. For simplicity, those parameters were not rigorously calibrated
for each building, and were used uniformly across the dataset.

|                        |                                                                  |       |
|------------------|------------------------------------|------------------|
| Parameter              | Description                                                      | Value |
| $\beta_{temp}$         | \% savings from setting $T_{interv}$ 1 °C higher than $T_{base}$ | 0.08  |
| $\beta_{occ}$          | \% savings adjustment during occupied hours                      | 1.2   |
| $\beta_{\text{unocc}}$ | \% savings adjustment during unoccupied hours                    | 0.8   |

: Table 1. Parameters for calculating the intervention savings.

## Apply example non-routine event

To illustrate the impact of non-routine events that are not properly
adjusted for, we developed three scenarios to quantify the energy
consumption changes associated with a hypothetical increase in
occupancy. Specifically, we assume that a rise in occupancy in 2016
(referred to as the "baseline year" when using the conventional method)
led to a 20% increase in whole-building measured electricity usage. The
three scenarios describe when this increase occurred: January to April
(S1), May to August (S2), and September to December (S3). This
hypothetical change was applied only to buildings in the "tidy" subset,
as these buildings were initially more likely to be free from
non-routine events.

## Run M&V methods

### Conventional M&V

As described in Section \@ref(introduction), conventional method for M&V
is time-consuming and unable to separate non-routine event impact from
measured savings. If using conventional method to estimate intervention
energy savings, the result (after 24 months) is calculated by the
difference between measured intervention and projected baseline in the
post-retrofit period. In this study, we leveraged a piece-wise linear
regression considering time-of-week and outdoor temperature (TOWT) as
independent variables for projection for the counter-factual baseline in
the post-retrofit period and normalization on typical meteorological
year [@mathieu_quantifying_2011].

### Randomized M&V

Compared to the conventional method, the randomized M&V approach offers
a more rapid and reliable estimation of energy savings. To apply this
method, analysts first define the target savings and stopping criteria.
Then they design a randomized switchback schedule and perform sequential
statistical tests, such as the sequential probability ratio test (SPRT),
to monitor savings as data is collected. Once the target savings are
detected, the analysts fit a prediction model (e.g. TOWT) to adjust for
differences in outdoor temperature, following the same adjustment
process as the conventional method. We provide example switchback
experimental design and stopping criteria thresholds below:

-   The HVAC system operates from 06:00 to 22:00 each day, so we use a
    daily sampling interval with the sampling time at midnight each day.

-   Block by day of the week with a block period of 12 weeks.

Stopping criteria are:

-   A minimum and maximum of 12 and 108 weeks respectively. The
    randomized schedule covers the entire two-year period but stopping
    criteria enables an early stop at the end of satisfied blocking
    period.

-   At least 80% of the drybulb temperature range in the annual TMY data
    sampled by both strategies.

-   Test for no carryover effect using a t-test with a p-value not
    exceeding a defined significance threshold of 0.05.

-   90% confidence that energy savings exceed or do not exceed 0% using
    the SPRT test. Medium effect size (d = 0.5) quantified by cohen's d
    and calculated SPRT statistics either falls below the lower
    threshold or exceeds the upper threshold.

-   As no baseline data is available, test with an equal sampling ratio
    (50% baseline, 50% intervention).

To remain consistency, savings normalization on typical meteorological
year is also modeled through TOWT.

# Results

## M&V methods comparison

In this section, we compare the performance of two M&V methods. The key
aspects of the assessment include: 1) time required to reach a saving
estimation: in most cases, a shorter M&V timeline reduces associated
cost and interruption for the building owner; and 2) saving estimation
accuracy: this is particularly important for any software-as-a-service
company to set a reasonable price with their customers.

### Savings estimation time

ASHRAE Guideline 14 offering minimum requirements for whole-building
measurement path states that the baseline period is either a full range
of all independent variables (typically outdoor weather conditions)
under normal facility operation or a 12-month worth of continuous
measurements. The same requirements also applies to intervention
installed in the post-retrofit period. Thus, the conventional M&V method
is likely to run over 24 months or even longer due to missing data. For
example if an analyst is asked to determine the energy savings of a
chilled water plant retrofit but missed most of the cooling season due
to delay in retrofit deployment, he/she needs to wait until the next
cooling season to measure the savings. If the randomized M&V method is
applicable, delays in retrofit deployment pose less risk to the building
owner since baseline are continuously monitored. Sampling at equal
probabilities (e.g., 50%/50% between baseline and intervention) helps to
balance the distribution of independent variables like outdoor weather
conditions across all sampled control strategies. As long as one
strategy is not sampled over a large number of consecutive days (e.g., 7
days or more), it is unlikely that the measurements of independent
variables will differ significantly between the control strategies.

Figure \@ref(fig:timeline) shows the average estimated timeline for each
building across all sites and climate conditions if using randomized M&V
for the applied chilled water supply temperature reset intervention. The
timeline figures for each individual building are attached in the
supplementary material. The figure shows for all climate zones, covering
a sufficient range of outdoor weather condition is the most stringent
requirement. However, most buildings can achieve 80% of the required
range within 6 months, and all buildings can determine the savings,
including associated uncertainties, within 9 months---significantly
shorter than the baseline measurement period required by the
conventional method.

```{r timeline, fig.cap="Average randomized M&V timeline summary for all buildings at each site", fig.width=8, fig.height=4}

df_sprt_all_messy %>% 
  filter(seq == "eob" | seq == "temp") %>% 
  bind_rows(df_sprt_all_tidy %>% filter(seq == "eob" | seq == "temp")) %>% 
  group_by(site, seq) %>% 
  summarise(n_weeks = mean(n_weeks)) %>% 
  ungroup() %>% 
  mutate(seq = as.factor(seq), 
         seq = recode_factor(seq, "temp" = "80% temperature range satisfied", "eob" = "remaining weeks till finish")) %>% 
  ggplot() +
  geom_col(aes(x = site, y = n_weeks, fill = seq), position = "identity") +
  labs(x = "Site", 
       y = "Number of weeks", 
       fill = NULL, 
       title = "Average finishing time for all buildings using randomized M&V") +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = seq(0, 36, by = 12)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

```

### Savings estimation accuracy

In this study, we define the overall target savings (i.e., the ground
truth) that an M&V analyst aims to detect as the mean reduction in
electricity consumption over a two-year period resulting from the
implementation of the chilled water supply temperature reset control.
This target can be expressed in terms of normalized savings, fractional
savings, or simply the measured difference between baseline and
intervention measurements. Due to different timeline required by the two
methods, we compare the conventional M&V savings at the end of the
two-year period with the randomized M&V savings after all criteria are
satisfied. Since those are the time in reality, an M&V analyst report
estimated savings.

Figure \@ref(fig:norm) shows the overall comparison of the savings
estimated for all buildings in both subsets normalized on the typical
meteorological year weather conditions of each site. In subplot a), the
narrower range and clustering of the true savings indicates the
dependence of the intervention effect on the outdoor weather condition,
which is intended after weather normalization. Sites with mild climate
all year round such as locations in California shows higher savings
potential above 10%, while locations with more extreme climate such as
Washington DC shows only 6% savings annually. In subplot b), the results
indicate that the conventional M&V method tends to estimate savings with
greater uncertainty, and its distribution median deviates from the true
savings. In comparison, the randomized method, which requires much less
time, shows more accurate and precise estimation results.

```{r norm, fig.cap="Comparison of normalized annual fractional savings on site TMY between conventional and randomized method", fig.width=8, fig.height=4}

p1 <- df_FS_tidy %>% 
  bind_rows(df_FS_messy) %>% 
  filter(method == "true") %>% 
  mutate(method = "true savings") %>% 
  ggplot(aes(x = method, y = savings, fill = method)) +
  geom_errorbar(aes(ymin = min(savings), ymax = max(savings)), width = 0.2) +
  geom_boxplot(outlier.size = 2) +
  geom_text(aes(x = 1.35, 
                y = median(savings) + 1.5, 
                label = paste0("median: ", round(median(savings), digits = 0), "%")), 
            check_overlap = T) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     limits = c(0, 30), 
                     labels = number_format(suffix = "%")) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

p2 <- df_FS_tidy %>% 
  bind_rows(df_FS_messy) %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  left_join(df_seq_FS_tidy %>% 
              bind_rows(df_seq_FS_messy) %>% 
              filter(seq == "eob"), by = c("name", "site")) %>% 
  pivot_longer(c(conv, rand, FS), names_to = "parameter", values_to = "value") %>% 
  filter(parameter != "rand") %>% 
  mutate(parameter = as.factor(parameter), 
         parameter = recode_factor(parameter, 
                                   "conv" = "Conventional M&V", 
                                   "FS" = "Randomized M&V")) %>% 
  ggplot(aes(x = parameter, y = value, fill = parameter)) +
  stat_boxplot(geom ='errorbar', width = 0.5) +
  geom_boxplot(outlier.shape = NA) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     limits = c(0, 30)) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

  ggarrange(p1, p2,
            labels = c("a)", "b)"),
            ncol = 2, nrow = 1,
            widths = c(0.5, 1),
            legend = "none") +
    plot_annotation(title = "Normalized annual fractional savings estimation comparison", 
                    subtitle = "between convention and randomized M&V (with early stop)")
```

To assess accuracy at the individual building level, we calculated the
mean absolute difference between the true savings and the estimated
savings and normalized the results as a fraction of measured baseline
for generalized comparison. Figure \@ref(fig:tidyfrac) shows the
comparison between the two methods using the tidy subset. In this
comparison, we focused on the median and the 95th percentile of the
plotted distribution to minimize the influence of outliers. The results
indicate that the conventional method outperforms the randomized method
in this subset. This is mostly because the tidy subset includes
buildings with consistent usage patterns throughout the two-year period,
meaning the data used for regression model fitting closely aligns with
the conditions during the model prediction phase. In our case, this
justifies the TOWT model selection when there is no additional
adjustment needed for such pre- and post-analysis.

```{r tidyfrac, fig.cap="Absolute deviation of M&V estimated fractional savings from true target savings using tidy subset", fig.width=8, fig.height=4}

plot_data <- df_seq_FS_tidy %>% 
  filter(seq == "eob") %>% 
  left_join(df_FS_tidy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - FS), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p1 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_tidy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.5)) +
  labs(x = "Number of buildings", 
       y = "Absolute difference in measured savings", 
       subtitle = "Randomized M&V at early stop") +
  coord_cartesian(ylim = c(0, 10)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

plot_data <- df_FS_tidy %>% 
  filter(method != "rand") %>% 
  group_by(name) %>% 
  summarise(abs_diff = abs(diff(savings))) %>% 
  ungroup() %>% 
  mutate(plot_max = max(abs_diff))

median_conv <- median(plot_data$abs_diff)
upper_conv <- quantile(plot_data$abs_diff, probs = 0.95)

p2 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_tidy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_conv + 0.5,
           x = 20, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_conv, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_conv + 0.5, 
           x = 20,
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_conv, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.1)) +
  labs(x = "Number of buildings", 
       y = NULL, 
       subtitle = "Conventional M&V") +
  coord_cartesian(ylim = c(0, 10)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 0, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          align = "hv",
          legend="bottom") +
  plot_annotation(title = str_glue("M&V method accuracy comparison"), 
                  subtitle = "fractional savings in tidy subset")


```

While these findings suggest that the conventional M&V method is
advantageous when a building maintains stable electricity usage over
time, such stability is rarely guaranteed in real-world scenarios. A
more realistic condition is shown in the messy subset in Figure
\@ref(fig:messyfrac). In this case, the randomized method demonstrates
consistent accuracy in savings estimation, even in the presence of
random measurement noise, implying strong robustness. In contrast, the
conventional method shows deviations exceeding 5% in approximately 50%
of all cases. Non-normalized mean absolute difference comparison plots
are provided in the supplementary material. The much deteriorated
performance of the conventional method highlights its vulnerability when
changes in electricity usage occur. This is because baseline projections
based on regression models become unreliable when non-routine events are
present. Non-routine events encompass all influential factors not
accounted for in the regression model, either due to their
impracticality to measure---such as hourly occupancy rates---or their
unpredictability---such as a sudden change in building use where an
office being converted into a warehouse. We dedicate the following
section to continue the discussion of the impact.

```{r messyfrac, fig.cap="Absolute deviation of M&V estimated fractional savings from true target savings using messy subset", fig.width=8, fig.height=4}

plot_data <- df_seq_FS_messy %>% 
  filter(seq == "eob") %>% 
  left_join(df_FS_messy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - FS), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p1 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_messy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 1, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 1, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.5)) +
  labs(x = "Number of buildings", 
       y = "Absolute difference in measured savings", 
       subtitle = "Randomized M&V at early stop") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

plot_data <- df_FS_messy %>% 
  filter(method != "rand") %>% 
  group_by(name) %>% 
  summarise(abs_diff = abs(diff(savings))) %>% 
  ungroup() %>% 
  mutate(plot_max = max(abs_diff))


median_conv <- median(plot_data$abs_diff)
upper_conv <- quantile(plot_data$abs_diff, probs = 0.95)

p2 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_messy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_conv + 1,
           x = 220, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_conv, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_conv + 1, 
           x = 220,
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_conv, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.1)) +
  labs(x = "Number of buildings", 
       y = NULL, 
       subtitle = "Conventional M&V") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 0, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          align = "hv",
          legend="bottom") +
  plot_annotation(title = str_glue("M&V method accuracy comparison"), 
                  subtitle = "fractional savings in messy subset")


```

## Non-routine events impact on savings estimation

In this section, we demonstrate the influence of non-routine events
through two scenarios. In the first scenario, we quantified how much
occupancy would influence the electricity usage and added to the tidy
measurement set as they originally indicates no such change. In the
second scenario, we simply run the two M&V methods on the messy
measurement sets before adding the chilled water supply temperature
reset. Since no intervention is added, the more reliable method should
detect a 'saving' closer to 0.

### Occupancy change

Although occupancy can be approximated in several ways in commercial
buildings, such as through counting check-in, or WIFI connections, or
monitoring indoor CO2 concentration as a proxy. However, it is not a
cost-efficient measures to add to the routine operation for most of the
buildings and there might be privacy concerns associated. The scenario
proposed hypothesize that during the baseline measurement period of the
target building, a floor of tenants moved out leaving the space
unoccupied for four months before new tenants moved in and this led to a
20% electricity decrease as a result. The M&V protocol only requires the
M&V analyst to fit a TOWT model so he/she only measures outdoor
temperature. We showed one example in Figure \@ref(fig:occchange) where
from May 1st 2016 to August 31st 2016, the target building electricity
decreased 20% due to reduced occupancy. Thus subplot (a) shows measured
baseline with such change, which are then used for TOWT model fitting.
Starting from 2017, the occupancy returns normal and only intervention
strategy was measured and the projected baseline is the prediction
results from the fitted model. To demonstrate, we also plotted the
original measurements before adding the intervention effect as 'adjusted
baseline'. This assumes that we can accurately adjust the baseline in
the post-retrofit period on back-to-normal occupancy. Subplot (b) shows
that the fitted regression underestimate the true baseline condition
leading to underestimated savings.

```{r occ}

n = 6

site_info <- df_energy_tidy %>%
    filter(name == all_names_tidy$name[n]) %>%
    select(site, type) %>%
    distinct()
  
site <- site_info$site
  
site_weather <- df_weather_tidy %>%
    filter(site == site_info$site) %>%
    select(timestamp, t_out) %>%
    group_by(timestamp) %>%
    summarise(t_out = mean(t_out)) %>%
    ungroup()

df_all <- df_energy_tidy %>%
    filter(name == all_names_tidy$name[n]) %>%
    select(timestamp, eload) %>%
    left_join(site_weather, by = "timestamp")
  
# Linear interpolation of baseline
df_all <- df_all %>%
  run_interpo()

plot_scale <- get_scale(df_all$base_eload)

df_hourly_conv <- df_all %>%
  run_reset()
  
df_base_conv <- df_hourly_conv %>%
    select(datetime,
           eload = base_eload,
           t_out) %>% 
    drop_na()

df_interv_conv <- df_hourly_conv %>%
    select(datetime,
           eload = interv_eload,
           t_out) %>% 
    drop_na()

    
change_start <- occ_params$change_start
change_end <- occ_params$change_end
change <- occ_params$change

base_pre_meas <- df_base_conv %>%
  filter(datetime < as.Date("2017-01-01")) %>%
  mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload * (1 - change / 100), eload))

# Baseline projection
towt_base <- base_pre_meas %>%
  model_fit()

df_towt <- df_interv_conv %>%
  filter(datetime >= as.Date("2017-01-01")) %>%
  select(time = datetime,
         temp = t_out,
         eload)

base_pos_proj <- model_pred(df_towt, towt_base) %>%
  rename("datetime" = "time") %>%
  select(datetime, towt, eload)

base_pos_true <- df_base_conv %>%
  filter(datetime >= as.Date("2017-01-01"))

interv_pos_meas <- df_interv_conv %>%
  filter(datetime >= as.Date("2017-01-01"))
```

```{r occchange, fig.cap="Occupancy change impact on TOWT model fitting and baseline projection", fig.width=8, fig.height=4}

p1 <- ggplot() +
    geom_point(data = base_pre_meas %>%
                 slice_sample(n = 500),
               aes(x = datetime, y = eload, color = "Measured baseline"),
               size = 0.5,
               alpha = 0.3) +
    geom_smooth(data = base_pre_meas,
                aes(x = datetime, y = eload, color = "Measured baseline"), 
                formula = y ~ x, method = "loess") +
    scale_x_datetime(date_breaks = "2 months",
                     date_labels = "%b")  +
    scale_y_continuous(expand = c(0, 0),
                       breaks = breaks_pretty(n = 3),
                       labels = number_format(suffix = " kW")) +
    scale_color_manual(values = ls_colors) +
    coord_cartesian(ylim = plot_scale) +
    labs(x = NULL,
         y = NULL,
         color = NULL,
         title = NULL,
         subtitle = "Pre-retrofit period") +
    theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
          legend.direction = "horizontal",
          legend.position = "bottom",
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
p2 <- ggplot() +
  geom_point(data = base_pos_proj %>%
               slice_sample(n = 500),
             aes(x = datetime, y = towt, color = "Projected baseline\n(no change)"),
             size = 0.5,
             alpha = 0.3) +
  geom_smooth(data = base_pos_proj,
              aes(x = datetime, y = towt, color = "Projected baseline\n(no change)"), 
              formula = y ~ x, method = "loess") +
  geom_point(data = base_pos_true %>%
               slice_sample(n = 500),
             aes(x = datetime, y = eload, color = "Adjusted baseline"),
             size = 0.5,
             alpha = 0.3) +
  geom_smooth(data = base_pos_true,
              aes(x = datetime, y = eload, color = "Adjusted baseline"), 
              formula = y ~ x, method = "loess") +
  geom_point(data = interv_pos_meas %>%
               slice_sample(n = 500),
             aes(x = datetime, y = eload, color = "Measured interv"),
             size = 0.5,
             alpha = 0.3) +
  geom_smooth(data = interv_pos_meas,
              aes(x = datetime, y = eload, color = "Measured interv"), 
              formula = y ~ x, method = "loess") +
  scale_x_datetime(date_breaks = "2 months",
                   date_labels = "%b")  +
  scale_y_continuous(expand = c(0, 0),
                     breaks = breaks_pretty(n = 3),
                     labels = number_format(suffix = " kW")) +
  scale_color_manual(values = ls_colors) +
  coord_cartesian(ylim = plot_scale) +
  labs(x = NULL,
       y = NULL,
       color = NULL,
       title = NULL,
       subtitle = "post-retrofit period") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        axis.text.y = element_blank(),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          widths = c(1, 1),
          align = "v",
          legend="bottom") +
  plot_annotation(title = "Building energy consumption with added occupancy change")
```

Figure \@ref(fig:occsavings) shows the normalized fractional savings
estimation accuracy comparison between the two M&V methods. To compare
which method is more robust to the applied occupancy change, a
difference-in-difference value is plotted in the figure. Each bar in the
plot represent the difference in the deviation of savings estimated by
the convention method and the randomized method:

$$
|S_{conv} - S_{true}| - |S_{rand} - S_{true}|
$$

In other words, a positive value indicate the randomized method provides
an estimation more aligned to the true target saving. Thus the plot
shows the randomized method shows uniformly superior robustness for all
sites. The red-dotted line shows the absolute deviation in savings
estimation by the randomized method as a reference for scaling. The
deviation is consistent with the distribution shown in Figure
\@ref(fig:tidysite) and thus further reinforce its robustness to
non-routine events.

```{r occsavings, fig.cap="Savings estimation accuracy comparison between two M&V methods with added occupancy change (with the site average difference-in-difference displayed at the bottom of each site).", fig.width=8, fig.height=4}
s <- 'S4'
dev_conv <- df_NRE_occ %>%
  filter(scenario == s) %>%
  filter(method != "rand") %>%
  group_by(name, site, scenario) %>%
  summarise(conv = abs(diff(savings))) %>%
  ungroup()

dev_rand <- df_NRE_occ %>%
  filter(scenario == s) %>%
  filter(method != "conv") %>%
  group_by(name, site, scenario) %>%
  summarise(rand = abs(diff(savings))) %>%
  ungroup()

dev_FS <- dev_rand %>%
  left_join(dev_conv, by = c("name", "site", "scenario")) %>% 
  mutate(diff_in_diff = conv - rand)

mean_diff <- dev_FS %>%
  group_by(site) %>%
  summarise(mean = round(mean(diff_in_diff), digits = 1),
            pos = round(n() / 2) + 1,
            .groups = 'keep')

dev_FS %>%
  ggplot(aes(group = site)) +
  geom_col(aes(x = name, y = diff_in_diff), position = "identity", alpha = 0.3) +
  facet_wrap(~site, nrow = 1, scales = "free_x") +
  scale_y_continuous(expand = c(0.1, 0),
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = "%")) +
  geom_text(data = mean_diff,
            aes(x = pos, y = -.5, group = site, label = paste0(mean, "%"))) +
  geom_line(aes(x = name, y = rand, color = "Absolute deviation of randomized method"), alpha = 0.5) +
  geom_point(aes(x = name, y = rand, color = "Absolute deviation of randomized method"), alpha = 0.5) +
  labs(x = NULL,
       fill = NULL,
       y = NULL,
       color = NULL, 
       title = "Difference-in-difference of fractional savings") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        axis.text.x = element_blank(),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

### No-saving detection

The outbreak of pandemic in 2020 followed by working-from-home policy is
a practical example of building energy consumption influenced by
non-routine events. During unoccupied months, building energy
consumption dropped compared to the same time period in the previous
years. If an energy-efficient intervention was deployed onsite starting
at 2020, given by the whole-building electricity measurements, the large
difference in measured electricity usage in 2020 and projected baseline
usage in 2019 can hardly indicate the retrofit intervention effect.

In other words, if there is no intervention applied or the intervention
effect is known to be null (e.g. constantly being overriden by the
baseline), an ideal M&V method should detect no savings. To test this,
we kept the randomized schedule as sampled earlier but removed added
chilled water plant intervention effect. Therefore, all sampled
intervention days are considered additional sampled baseline days.
Earlier we mentioned the 'messy' dataset contains some change in the
two-year electricity measurement, which is purely due to various
non-routine events such as occupancy change or even other measures
implemented by the building manager but irrelevant to the chilled water
set point reset intervention. Thus, when the proposed chilled water
reset intervention was removed, a more reliable M&V method should
overcome the influence of those confounding factors and inform no
savings.

```{r nosavings, fig.cap="Distribution of savings estimation results by the convention M&V method and randomized method.", fig.width=6, fig.height=3}

df_FS_nsprt %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  left_join(df_seq_FS_nsprt %>% filter(seq == "eob"), by = c("name", "site")) %>% 
  pivot_longer(c(true, conv, rand, FS), names_to = "parameter", values_to = "value") %>% 
  filter(parameter != "true" & parameter != "rand") %>% 
  mutate(parameter = as.factor(parameter), 
         parameter = recode_factor(parameter, 
                                   "conv" = "Conventional M&V", 
                                   "FS" = "Randomized M&V")) %>% 
  ggplot(aes(x = parameter, y = value, fill = parameter)) +
  stat_boxplot(geom ='errorbar', width = 0.2) +
  geom_boxplot(outlier.shape = NA, width = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1.2) +
  annotate(geom = "text", 
           x = 0.7, 
           y = 0.5, 
           color = 'red',
           label = "True savings = 0") +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 5), 
                     limits = c(0, 16), 
                     labels = number_format(suffix = "%")) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL, 
       title = "Savings estimation comparison", 
       subtitle = "between convention and randomized M&V (with early stop)") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "none",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

Figure \@ref(fig:nosavings) shows that the conventional M&V method in
average detected 5% savings when there should be none. The randomized
method on the other hand is able to provide an estimation much closer to
0% with reduced uncertainty. This is mostly because when there is a
change or disturbance applied to the target building such as lighting
retrofit or thermal leakage in the envelope, randomly sample the two
strategies at 50%/50% ensures the resulting effect either as a decrease
in lighting electricity or an increase in reheat electricity is balanced
between the two implemented strategies.

# Discussion

## TOWT modeling accuracy

Although this paper argues that regression model prediction results can
be largely biased by various non-routine events, we further clarify that
this is not related to regression model fitting accuracy. Time-of-week
temperature model is used here for its simplicity and convenience since
the model only requires outdoor temperature measurements and time of
week as independent variables. The model assumes a linear composition of
building energy consumption as temperature-dependent and time-dependent
load. The time-dependent component accounts for day-to-day variations
and the temperature dependent component considers a piecewise linear
relationship across a range of temperature intervals.
<!--# TODO: add reference -->

To assess modeling accuracy, we used the Coefficient of Variation of
Root-Mean Squared Error, or CV(RMSE) as the error metric. Since this
metric is calculated as a normalized value, it is useful to compare
different model fitting results. Guideline 14 requires that
whole-building baseline model fitting accuracy should maintain a
CV(RMSE) lower than 30%. In addition One study focusing on the baseline
energy data-driven model fitting indicates that TOWT performs as
accurate as other more advanced machine learning models
<!--# TODO: reference --> and the calculated CV(RMSE) distribution for a
large sample of commercial buildings indicates a median of 20%. Figure
\@ref(fig:towtacc) plots the distribution of the model fitting accuracy
calculated separately for the two measurement sets. The result shows
that the TOWT model performance is even better in this study compared to
the literature and no significant difference between the two subsets.
This is mostly related to low-quality measurement sets filtering in the
pre-processing mentioned in Section \@ref(method).

```{r towtacc, fig.cap="TOWT model fitting accuracy distribution on filtered measurement set", fig.width=8, fig.height=4}

df_model_acc_messy %>% 
  mutate(group = "messy") %>% 
  rbind(df_model_acc_tidy %>% mutate(group = "tidy")) %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x = group, y = cvrmse, fill = group)) +
  stat_boxplot(geom ='errorbar', width = 0.5) +
  geom_boxplot(outlier.shape = NA) +
  geom_text(aes(x = group, y = median, label = paste0(round(median, digits = 0), " %")), 
            data = . %>% 
              group_by(group) %>% 
              summarise(median = median(cvrmse)) %>% 
              ungroup(), 
            position = position_nudge(y = 1)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %")) +
  coord_cartesian(ylim = c(0, 38)) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL, 
       title = "TOWT model accuracy", 
       subtitle = "CV(RMSE)") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

In general, the limitation is that despite the regression model tends to
capture well on the mean building energy consumption but can
underestimate and peak or overestimate the lower base load
<!--# TODO: reference -->, which is not useful for assessing demand
response. Additionally, a data-driven model can learn the training
dataset very efficiently with high accuracy, but the well-trained model
is challenging to generalize prediction outside the range of training
set.

## Sampling ratio for randomized M&V

Another advantage of using randomized M&V is the flexibility of changing
sampling ratio after the M&V. For example, the building owner can
continue sampling at 50%/50% between the baseline and the intervention
to further reduce the uncertainty range associated with the savings. Or
he/she can switch to 100% intervention to optimize energy savings, but
whether the existing baseline becomes outdated is unknown. A compromised
approach is to continue sampling at 80%/20% between the baseline and the
intervention. Figure \@ref(fig:cont) shows after the analyst reports the
randomized M&V results, a new schedule sampling at 80%/20% was
implemented till the end of 2016 and the plot shows the saving
estimation accuracy calculated as difference-in-difference when sampled
baseline reduced.

```{r cont, fig.cap="Savings estimation accuracy for continue sampling at 80%/20% for 36 weeks after all stopping criteria are satisfied", fig.width=8, fig.height=4}

plot_data <- df_cont_FS_tidy %>% 
  rename(cont = FS) %>% 
  left_join(df_FS_tidy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - cont), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p1 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_tidy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.5)) +
  labs(x = "Number of buildings", 
       y = "Absolute difference in measured savings", 
       subtitle = "Tidy subset") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))


plot_data <- df_cont_FS_messy %>% 
  rename(cont = FS) %>% 
  left_join(df_FS_messy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - cont), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p2 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_messy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 0.5, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 0.5, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.1)) +
  labs(x = "Number of buildings", 
       y = NULL, 
       subtitle = "Messy subset") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 0, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          align = "hv",
          legend="bottom") +
  plot_annotation(title = str_glue("Continue sampling accuracy comparison"), 
                  subtitle = "with 80%/20% intervention/baseline")
```

The deviation remains consistent in the tidy subset but increases
slightly in the messy subset, suggesting that changes in energy usage in
the target building create a trade-off between measured savings and
estimation accuracy when down-sampling the baseline. In this scenario,
opting for an early stop at 24 or 36 weeks with 50% intervention and
re-sampling 80% till the year end (\~ another 20 weeks) still allows the
building owner to capture 65% of the full-range savings. On the other
hand, if the building owner follows the conventional method, there is no
savings due to required baseline measurement.

## Limitations and future study

We identify two key limitations of this study: 1) the application of
control intervention, and 2) the design of the randomized switchback
experiment. While the primary focus is on accurately detecting
intervention effects rather than validating their impact, the simulated
intervention remains somewhat generic due to the diverse building types
and climate zones in the BDG2 dataset. For simplicity, we applied the
parameters listed in Table 1 uniformly across all buildings. However,
the electricity savings from raising the water temperature by 1°C may be
less than 8% in some cases, due to increased chilled water pump speeds.
Similarly, the randomized schedule design is also simplified. We assumed
a daily sampling interval would be sufficient for most commercial
buildings, but exceptions exist, particularly for buildings with
significant thermal lag in heating and cooling systems, such as those
with heavy concrete construction, hot water tanks, or Thermally Active
Building Systems (TABS). In such cases, the effect of a previously
sampled strategy can carry over and influence subsequent measurements
due to thermal storage, known as the carryover effect. For example, if
the intervention pre-charges chilled water in thermal mass one day
before switching back to the baseline, the analyst is likely to observe
reduced energy usage in the following days. This study does not account
for the carryover effect, but in practice, we recommend using a 3-day
sampling interval and dropping non-consecutive days to 'wash out' any
lingering effects from previous strategies.

Our major goal for the future study is to further extend the application
of the proposed randomized M&V. For example, a customer maybe interested
in whether a Model Predictive Control (MPC) can reduce energy bill under
dynamic pricing structure and what the estimated Return of Investment
(ROI) for the retrofit is. Or whether a load shift control can save
operational cost by shifting load from grid peak to off peak period.
Furthermore, the novel method is generalizable beyond energy saving M&V
cases. The statistical analytic framework is applicable to a variety of
retrofit metrics such as indoor air quality, operational carbon
emissions and thermal comfort.

# Conclusion

This research demonstrated the usage and assessed the performance of a
novel whole-building measurement and verification method using a large
open-source commericial dataset. The method leverages the randomized
experimental design concept from other scientific research fields and
statistical sequential inference techniques to inform whether a target
savings are detected. We made an example of chilled water setpoint reset
based on outdoor weather condition as a virtual control retrofit use
case and applied to over 500 filtered commercial buildings. By comparing
the savings estimation by the conventional method (outlined in ASHRAE
Guideline 14) and the novel randomized method, we showed that the
randomized method is able to provide a more rapid and robust saving
estimation.

We showed that throughout 7 climate zones we assessed, the randomized
M&V can provide a saving estimation by 36 weeks (with the majority
finishes by 24 weeks) once all stopping criteria satisfied. Meanwhile,
the conventional method requires a full range measurement of both
baseline and intervention performance under normal operation conditions,
which normally translate to 6 - 9 months each. We further showed the
estimated savings at early stop has acceptable accuracy when compared
with the true calculated savings.

We also assessed the impact of non-routine events on the proposed M&V
method both by 1) adding a known change, such as occupancy-induced
energy reduction, and 2) detecting removed intervention when no reset is
applied to buildings with a marginal difference in energy consumption.
We showed in both scenarios that baseline model fitting can be biased
while the randomization can efficiently block those confounding effect
to ensure the robustness of the saving estimation.

Although the limitation of the method is that it only applies to a
subset of all M&V use cases (i.e. only if the strategy can be switched
on and off), we believe the true value lies in the usefulness for most
control retrofit validation in the field test.

# Acknowledgements

# References
