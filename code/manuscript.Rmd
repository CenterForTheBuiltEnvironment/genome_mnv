---
title: "Reliability Demonstration of A Novel Randomized Measurement and Verification Method for Switchable Control Retrofit Using Large Open Source Dataset"
author:
  - Aoyu Zou ^[Center for the Built Environment, University of California Berkeley, USA], Paul Raftery ^1^, Stefano Schiavon ^1^, Carlos Duarte ^1^, Gail Brager ^1^
abstract: Conventional measurement and verification (M&V) methods relying on pre- and post-  retrofit comparison for estimating whole-building energy savings are often time-consuming and unreliable, especially when non-routine events such as reduced occupancy occur during the M&V process. Those events are unrelated to the intervention but significantgly affect building energy consumption and thus when the analyst applies conventiona M&V suggested by industry guidelines, the results can be largely confounded. In this study, we demonstrated that switchable interventions, such as most of the control retrofits, can benefit from random sampling, where the analyst randomly selects whether to implement the baseline or the intervention strategy each day. We tested this novel randomized M&V method on a large public dataset covering various climate zones and commercial building types, using a virtual chilled water supply temperature reset based on outdoor weather as the intervention. Our results show that comparing to the conventional method, the randomized method provides faster and more accurate savings estimation. Additionaly we found that the randomized M&V approach estimates much closer to the true savings when non-routine events are present demonstrating much improved reliability.

output:
  bookdown::word_document2:
    reference_docx: "../paper/template.docx"
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
biblio-style: apalike

knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../paper"
    )
  })
---

```{r setup, include = FALSE, cache = FALSE}

# knitr setup
knitr::opts_chunk$set(echo = F, 
                      message = F,
                      warning = F,
                      dev = "jpeg",
                      # cache = T,
                      dpi = 300,
                      fig.show = "hold",
                      fig.pos = "b", 
                      fig.path = "../figs/manuscript/figs/")

# str(knitr::opts_chunk$get()) # see for all options

```

```{r prep, include = FALSE, cache = FALSE}
#### LIBRARIES ####
require(pacman)

# load packages using pacman
pacman::p_load(tidyverse, lubridate, here, stats, zoo, scales, lvplot, ggpubr, gridExtra, patchwork, RColorBrewer)

# turn off scientific notation
options(scipen = 999, digits = 15)

# set directory
here::i_am("manuscript.rmd")

# set default theme for ggplot
theme_set(theme_minimal())

# define base ggplot theme
theme_update(plot.title = element_text(size = 14, colour = "grey20", face = "bold", hjust = 0.5),
             plot.subtitle = element_text(size = 10, colour = "grey20", face = "italic", hjust = 0.5, margin = margin(b = 10)),
             plot.caption = element_text(size = 10, colour = "grey20", face = "italic", hjust = 0.5),
             plot.background = element_rect(fill = "white", colour = NA),
             panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             axis.text = element_text(size = 10),
             strip.text = element_text(size = 10, color = "grey20", face = "bold"),
             strip.background = element_blank())

# colors
ls_colors <- c("Baseline" = "#c6dbef",
               "Measured baseline" = "#c6dbef",
               "Adjusted baseline" = "#c6dbef",
               "Projected baseline" = "#2171b5",
               "Intervention" = "#fdbb84",
               "Measured interv" = "#fdbb84",
               "Conventional" = "grey70", 
               "Randomized" = "#99d8c9", 
               "Randomized\n(24 months)" = "#66c2a4",
               "Randomized\n(50/50)" = "#66c2a4",
               "Daily\nsampling" = "#66c2a4",
               "2-day\nsampling" = "#66c2a4",
               "3-day\nsampling" = "#66c2a4",
               "7-day\nsampling" = "#66c2a4",
               "Dropped" = "#41ae76",
               "Kept" = "#006d2c",
               "Randomized\n(20/80)" = "#ccece6",
               "Buildings finishing randomized M&V" =  "#99d8c9",
               "Buildings satisfying 80% TMY range" = "black", 
               "Buildings satisfying SPRT" = "black")

# parameters
ctr_params <- list(peak_hours = 10:16,
                   chwl_perc = 0.25,
                   step_perc = 0.08,
                   conv_swt = 6,
                   weather_knots = c(15, 25),
                   swt_knots = c(12, 7),
                   coe_peak = 0.8,
                   coe_off = 1.2,
                   enable_temp = 8)

occ_params <- list(change_start = 5,
                   change_end = 8,
                   change = 20)





#### FUNCTIONS ####
# Function defined to read downloaded tmy files
get_tmy <- function(all_sites, readfile_path){
  
  df_tmy <- data.frame()
  
  for (site in all_sites){
    df <- read_csv(paste0(readfile_path, "tmy/", str_glue("{site}.epw")),
                   skip = 8, col_types = "ddddd-d---------------------------------",
                   col_names = c("year", "month", "day", "hour", "min", "tmy")) %>%
      mutate(year = 2017,
             time = ymd_h(paste(paste(year, month, day, sep = "-"), hour, sep = " ")),
             temp = tmy) %>%
      dplyr::select(time, temp) %>% 
      mutate(site = site)
    
    df_tmy <- rbind(df_tmy, df)
  }
  
  return(df_tmy)
}

# Function defined to add chwst reset intervention
run_reset <- function(df_baseline){
  
  mean <- mean(df_baseline$base_eload, na.rm = T) * ctr_params$chwl_perc
  
  grad <- (ctr_params$swt_knots[2] - ctr_params$swt_knots[1]) / 
    (ctr_params$weather_knots[2] - ctr_params$weather_knots[1])
  
  interc <- ctr_params$swt_knots[2] - (ctr_params$weather_knots[2] * grad)
  
  df_interv <- df_baseline %>% 
    mutate(swt = t_out * grad + interc, 
           chwl = mean,
           hour = hour(datetime)) %>% 
    mutate(swt = ifelse(swt > ctr_params$swt_knots[1], ctr_params$swt_knots[1], ifelse(swt < ctr_params$swt_knots[2], ctr_params$swt_knots[2], swt)), 
           temp_savings = ifelse(t_out >= ctr_params$enable_temp, (swt - ctr_params$conv_swt) * ctr_params$step_perc, 0), 
           time_adj = ifelse(hour %in% ctr_params$peak_hours, ctr_params$coe_peak, ctr_params$coe_off), 
           perc_savings = temp_savings * time_adj, 
           savings = chwl * perc_savings, 
           interv_eload = base_eload - savings) %>% 
    select(datetime, base_eload, interv_eload, t_out)
  
  return(df_interv)
}

# Function defined to interpolate NAs
run_interpo <- function(df_all){
  
  na_counts <- df_all %>%
    mutate(date = date(timestamp)) %>%
    group_by(date) %>%
    summarize(na_hours = sum(is.na(eload)))
  
  # Filter out days with more than half of the hours having NAs
  valid_days <- na_counts %>%
    filter(na_hours <= 12) %>%
    pull(date)
  
  df_filtered <- df_all %>%
    filter(date(timestamp) %in% valid_days)
  
  df_filtered <- df_filtered %>%
    mutate(across(c(eload, t_out), ~ zoo::na.approx(., na.rm = FALSE))) %>% 
    rename(datetime = timestamp, 
           base_eload = eload)
  
  return(df_filtered)
}

# Function defined to adjust the plot scale
get_scale <- function(eload, range = 2){
  
  min_y <- mean(eload, na.rm = T) - range * sd(eload, na.rm = T)
  max_y <- mean(eload, na.rm = T) + range * sd(eload, na.rm = T)
  
  return(c(min_y, max_y))
}
```

```{r readdata, include=FALSE}

#### READ DATA ####
df_loc <- read.csv("../readfiles/loc_map.csv")

# stable dataset
readfile_stable <- str_glue("../readfiles/stable/")
fig_path = "../figs/manuscript/"

df_energy_stable <- read_rds(paste0(readfile_stable, "df_energy.rds"))
df_weather_stable <- read_rds(paste0(readfile_stable, "df_weather.rds"))
df_sprt_all_stable <- read_rds(paste0(readfile_stable, "df_sprt_all.rds"))
df_seq_fs_stable <- read_rds(paste0(readfile_stable, "df_seq_fs.rds"))
df_fs_stable <- read_rds(paste0(readfile_stable, "df_fs.rds"))
df_cont_stable <- read_rds(paste0(readfile_stable, "df_cont.rds"))
df_fs_null_stable <- read_rds(paste0(readfile_stable, "df_fs_null.rds"))
df_seq_fs_null_stable <- read_rds(paste0(readfile_stable, "df_seq_fs_null.rds"))
df_model_acc_stable <- read_rds(paste0(readfile_stable, "df_model_acc.rds"))
df_fs_tmy_stable <- read_rds(paste0(readfile_stable, "df_fs_tmy.rds"))
df_fs_tmy_null_stable <- read_rds(paste0(readfile_stable, "df_fs_tmy_null.rds"))
df_interval_drop_stable <- read_rds(paste0(readfile_stable, "df_interval_drop.rds"))
df_interval_keep_stable <- read_rds(paste0(readfile_stable, "df_interval_keep.rds"))
df_seq_interval_fs_drop_stable <- read_rds(paste0(readfile_stable, "df_seq_interval_fs_drop.rds"))
df_seq_interval_fs_keep_stable <- read_rds(paste0(readfile_stable, "df_seq_interval_fs_keep.rds"))
df_seq_interval_nm_drop_stable <- read_rds(paste0(readfile_stable, "df_seq_interval_nm_drop.rds"))
df_seq_interval_nm_keep_stable <- read_rds(paste0(readfile_stable, "df_seq_interval_nm_keep.rds"))
df_seq_interval_tl_drop_stable <- read_rds(paste0(readfile_stable, "df_seq_interval_tl_drop.rds"))
df_seq_interval_tl_keep_stable <- read_rds(paste0(readfile_stable, "df_seq_interval_tl_keep.rds"))


all_sites_stable <- df_energy_stable %>%
  select(site) %>%
  distinct() %>%
  arrange(site)

all_types_stable <- df_energy_stable %>%
  select(type) %>%
  mutate(type = as.factor(type)) %>%
  distinct()

all_names_stable <- df_energy_stable %>%
  select(name) %>%
  distinct(name)

# variable dataset
readfile_variable <- str_glue("../readfiles/variable/")

df_energy_variable <- read_rds(paste0(readfile_variable, "df_energy.rds"))
df_weather_variable <- read_rds(paste0(readfile_variable, "df_weather.rds"))
df_sprt_all_variable <- read_rds(paste0(readfile_variable, "df_sprt_all.rds"))
df_seq_fs_variable <- read_rds(paste0(readfile_variable, "df_seq_fs.rds"))
df_fs_variable <- read_rds(paste0(readfile_variable, "df_fs.rds"))
df_cont_variable <- read_rds(paste0(readfile_variable, "df_cont.rds"))
df_fs_null_variable <- read_rds(paste0(readfile_variable, "df_fs_null.rds"))
df_seq_fs_null_variable <- read_rds(paste0(readfile_variable, "df_seq_fs_null.rds"))
df_model_acc_variable <- read_rds(paste0(readfile_variable, "df_model_acc.rds"))
df_fs_tmy_variable <- read_rds(paste0(readfile_variable, "df_fs_tmy.rds"))
df_fs_tmy_null_variable <- read_rds(paste0(readfile_variable, "df_fs_tmy_null.rds"))
df_interval_drop_variable <- read_rds(paste0(readfile_variable, "df_interval_drop.rds"))
df_interval_keep_variable <- read_rds(paste0(readfile_variable, "df_interval_keep.rds"))
df_seq_interval_fs_drop_variable <- read_rds(paste0(readfile_variable, "df_seq_interval_fs_drop.rds"))
df_seq_interval_fs_keep_variable <- read_rds(paste0(readfile_variable, "df_seq_interval_fs_keep.rds"))
df_seq_interval_nm_drop_variable <- read_rds(paste0(readfile_variable, "df_seq_interval_nm_drop.rds"))
df_seq_interval_nm_keep_variable <- read_rds(paste0(readfile_variable, "df_seq_interval_nm_keep.rds"))
df_seq_interval_tl_drop_variable <- read_rds(paste0(readfile_variable, "df_seq_interval_tl_drop.rds"))
df_seq_interval_tl_keep_variable <- read_rds(paste0(readfile_variable, "df_seq_interval_tl_keep.rds"))

all_sites_variable <- df_energy_variable %>%
  select(site) %>%
  distinct() %>%
  arrange(site)

all_types_variable <- df_energy_variable %>%
  select(type) %>%
  mutate(type = as.factor(type)) %>%
  distinct()

all_names_variable <- df_energy_variable %>%
  select(name) %>%
  distinct(name)

# read functions
function_path <- "../functions/"
source(paste0(function_path, "model_fit.R"))
source(paste0(function_path, "model_pred.R"))
source(paste0(function_path, "prepost_plot.R"))

S_building <- nrow(df_fs_tmy_stable)
V_building <- nrow(df_fs_tmy_variable)
A_building <- S_building + V_building
```

# Introduction

## Background

### Conventional M&V

Measurement and Verification (M&V) is the process of quantifying energy
savings from energy efficiency projects by comparing actual energy
consumption against a baseline, adjusting for factors like weather and
occupancy. This process ensures that improvements in energy performance
are accurately evaluated. In the United States, practitioners often
refer to ASHRAE Guideline 14, the International Performance Measurement
and Verification Protocol (IPMVP), and the Federal Energy Management
Program (FEMP) for standard guidelines [@doe_mv_2008,
@efficiency_valuation_organisation_international_2007,
@ashrae_ashrae_2014]. These guidelines outline standardized methods for
quantifying energy savings, whether through calibrated simulations or
monitored measurements for specific equipment or systems (isolation
methods) or for entire buildings (whole-building methods). In this
study, we will focus on the energy savings quantified at the
whole-building level where the measurements are obtained from utility
bills or whole-building meters. The corresponding M&V process required
by ASHRAE Guideline 14, which we refer to as the convention method in
this paper. Typically, the process begins with baseline measurements
taken over a year before implementing any energy-efficiency retrofit,
followed by the same measurement procedure during the intervention
period. After collecting two years of data, an M&V analyst fits an
energy prediction model, using variables such as outdoor temperature and
time [@mathieu_quantifying_2011] to project baseline energy consumption
in the post-retrofit period. The difference between the counter-factual
baseline and the measured intervention represents the energy savings. A
key drawback of this method is its reliance on a two-year timeline to
quantify savings, during which baseline measurements can become outdated
due to changes in building performance caused by non-routine events
unrelated to the intervention. This limitation reduces the feasibility
of rapid M&V and complicates the quantification of estimation
uncertainty, thus impacting the accuracy and timeliness of savings
assessments.

### Randomized M&V

To address the limitations of conventional M&V methods and the
challenges posed by non-routine events, we propose a novel M&V method
that adopts the randomized crossover design, a concept borrowed from
medical and agricultural studies. Another improvement is that we
proposed a sequential evaluation framework and defined stopping criteria
to end the M&V if the target effect is detected. This is to avoid
unnecessary measurement collection over the full 2-year M&V cycle. The
full framework is detailed in another study with all stopping criteria
outlined <!--# TODO: Add references -->. In summary, this method
provides M&V analysts with a randomized schedule that alternates between
baseline and intervention implementation while ensuring balanced
sampling across days of the week. For example, given a 10-week M&V
period for 1 intervention, the balanced randomized schedule would
equally sample 5 Mondays with the baseline and 5 Mondays with the
intervention. The limitation of the randomized M&V is that it is only
applicable to a subset of retrofit projects such as control
interventions. However, for all applicable use cases, it allows analysts
to detect energy savings sequentially as the study progresses meaning
once the desired savings target is achieved, analysts can terminate the
M&V. The key advantage of randomization, which is one of the study
objectives, is that if control strategies are sampled with equal
probability, the influence of non-routine events is likely to be evenly
distributed between the baseline and intervention measurements. This
means that the savings estimate, calculated as the difference between
the two, effectively 'cancels out' the impact of these disturbances,
leading to a more accurate and unbiased assessment of the intervention's
impact.

### BDG2 dataset

The Building Genome Dataset 2 (BGD2) is an extensive open-access dataset
designed to advance research and development in building energy
efficiency and control strategies acting as a test-bed for modeling,
simulation, and algorithm development [@miller_building_2020]. BGD2
contains over 500 buildings' metadata and realistic operational
information from across North America and Europe, making it one of the
most comprehensive collections of building-related data available for
scientific use. The dataset includes various commercial building types
such as offices, education facilities, public, and retail buildings, and
provides detailed information on their physical characteristics (e.g.
enregy ratings, heating types and floor area) and hourly measurements of
chilled and hot water, electricity, gas usage as well as site outdoor
weather conditions.

## Literature review

### Whole building approach

Most research related to M&V for whole-building approach focuses on the
accuracy of baseline modeling, exploring model performance from simple
regression models to more complex machine learning techniques. One study
reviewed various models suitable for M&V applications as well as
selected input features [@alrobaie_review_2022] and another study
provided a definitive methodology to apply machine learning models for
M&V use cases [@gallagher_development_2018]. In addition, a few studies
investigated the critical performance metrics to evaluate the developed
baseline models [@granderson_automated_2015,
@granderson_development_2014] and compared a variety of models using
those metrics [@granderson_accuracy_2016]. These studies made
significant contributions by emphasizing the uncertainty associated with
the model-fitting process, a key factor in accurately determining energy
savings. Furthermore, other researchers addressed this issue by
leveraging statistical formulation and inference to improve baseline
energy models [@burkhart_measurement_2014, @heo_calibration_2012,
@walter_uncertainty_2014]. However, a gap still remains in the
literature regarding the rigorous quantification of uncertainties
directly associated with calculated savings, for instance, accounting
for the potential bias that baseline model might deteriorate (i.e.
becomes 'stale') over an extended period of pre- and post-analysis.

### Non-routine events impact

A common non-routine event in energy-saving M&V projects is a change in
occupancy or a significant shift in occupant behavior. These changes can
greatly affect measured energy consumption in buildings and are
typically unrelated to the intervention strategy. For instance, during
the COVID-19 pandemic in 2020, most commercial buildings were
unoccupied, leading to a noticeable drop in energy bills despite no
energy-efficiency measures being implemented [@cai_impact_2024,
@chihib_impact_2021, @gaspar_assessing_2022, @kang_changes_2021].
Furthermore, subsequent research has shown that hybrid working modes,
allowing employees to work remotely, have persisted after the outbreak
of the pandemic [@aksoy_working_2022], adding further complexity to
energy consumption patterns due to evolved occupant behaviors with
higher flexibility [@gui_impact_2021, @mantesi_office_2022,
@xie_does_2021]. One study realized the limitation of current M&V
methods, which only consider adjusting for outdoor weather, is
insufficient and emphasized the importance of requiring matched
comparison groups to control for exogenous factors beyond weather
differences when comparing between baseline and intervention
[@demand_side_analytics_population_2022]. Another type of non-routin
event is filter clogging in air handling units due to particle
accumulation. This can cause supply fans to gradually consume more
energy to maintain required duct static pressure [@feng_newly_2019,
@zhai_full-scale_2017]. If M&V analysts are unaware of such changes and
lack an appropriate adjustment method (e.g., replacing filters before
the intervention begins), the savings could be underestimated as
increased energy use is incorrectly attributed to the intervention
rather than the mechanical issue.

## Objectives

As mentioned, the goal of an M&V project is to determine the
effect---typically energy savings---of an energy-efficient intervention.
In this study, we focus primarily on switchable interventions, which
often involve control retrofits. An example of such an intervention is a
control retrofit developed by a software-as-a-service company that
adjusts the chilled water plant's supply water temperature based on
outdoor weather conditions [@roa_field_2023,
@lee_simulationoptimization_2012, @qiu_chilled_2022]. Therefore, we
defined the M&V scenario as follows:

*"A company aims to sell its supply temperature reset control software
package to a customer, such as a building owner, with a guarantee that
it will reduce the building's electricity usage. If the building owner
decides to purchase the service, the company agrees to charge a service
fee based on a percentage of the measured energy savings."*

As required by the M&V scenario, we assessed the performance of both the
conventional and the novel randomized M&V methods by estimating the
intervention energy savings for all valid buildings in the dataset. By
conducting such analysis, we aim to:

1.  Compare the energy saving estimation accuracy between the
    conventional and the randomized method. This study extends the
    comparison to a large sample of buildings, covering a variety of
    types and climate zones. The comparison metrics include both
    estimation accuracy and M&V finishing timeline.

2.  Verify the enhanced robustness of the randomized method. By using
    realistic measurements from real-world buildings, which include
    various sources of noise, we aim to reflect the challenges faced by
    building analysts in real projects. As will be demonstrated in the
    following sections, the randomized approach is less impacted by
    non-routine events (i.e., measurement 'noise'), resulting in more
    reliable energy savings estimates.

3.  Demonstrate the implementation of the proposed randomized M&V method
    using a public available dataset. We ensured the reproducibility of
    the method by making the analysis code open source including
    randomized schedule generation, sequential statistical analysis,
    energy modeling and normalized saving calculation. Using the
    available open resources, building analysts should be able to
    seamlessly integrate and apply them in their own M&V projects.

# Method

We outlined the methodology of the study in Figure \@ref(fig:flowchart)
and extended several key components in this section.

```{r flowchart, fig.cap = "Flow chat showing the methodology for comparing the estimated savings of randomized M&V with the conventional M&V", out.width="50%"}

knitr::include_graphics(paste0(fig_path, "flowchart.png"))
```

## Data filtering

In this study, we extracted the electricity measurements from the BDG2
dataset and filter out all qualified buildings based on the following
criteria:

1.  Missing values \< 1000: given the hourly resolution of all
    measurements, this is equivalent to 1.5 months of missing days.

2.  Mean electricity usage \> 0 kWh: target buildings should have active
    electricity usage.

3.  $\frac{abs(E_{2017} - E_{2016})}{E_{2016}}< 25\%$: any increase or
    decrease of building electricity usage in the second year should be
    less than 25% of that in the first year.

4.  Warehouse and parking types are excluded: target buildings have less
    demand flexibility to implement a chilled water setpoint reset
    control.

```{r stablesite, fig.cap = "Site summary of the stable building subset (counts < 2 are omitted for visualization; left: aggragated counts of buildings for each type; right: breakdown building counts for each building type at each location)", fig.width=8, fig.height=6}

set3 <- colorRampPalette(brewer.pal('Set3',n=12))
type_colors <- setNames(set3(13), all_types_variable$type)
  
stable_set <- df_energy_stable %>%
  group_by(type) %>%
  distinct(name) %>%
  summarise(stable = n()) %>%
  ungroup()

variable_set <- df_energy_variable %>%
  group_by(type) %>%
  distinct(name) %>%
  summarise(variable = n()) %>%
  ungroup()

p1 <- variable_set %>% 
  left_join(stable_set, by = "type") %>% 
  mutate(stable = replace_na(stable, 0)) %>% 
  group_by(type) %>% 
  summarise(n = stable + variable) %>% 
  ungroup() %>% 
  mutate(type = fct_reorder(type, n, .desc = F)) %>%
  ggplot(aes(x = 1, y = n, fill = as.factor(type))) +
  geom_col() +
  geom_text(aes(label = ifelse(n > 10, as.character(n), "")), color = "black", position = position_stack(vjust = 0.5)) +
  scale_y_continuous(expand = c(0, 0),
                     breaks = breaks_pretty(n = 4)) +
  scale_x_discrete(expand = c(0, 0.1)) +
  scale_fill_manual(values = type_colors) +
  labs(x = NULL,
       y = NULL,
       subtitle = "Combining all locations",
       fill = NULL) +
  theme(axis.text.x = element_blank(),
        panel.grid.major.y = element_line(color = "grey80"),
        legend.direction = "horizontal",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

stable_set <- df_energy_stable %>% 
  group_by(type, site) %>%
  distinct(name) %>%
  summarise(stable = n()) %>%
  ungroup()

variable_set <- df_energy_variable %>% 
  group_by(type, site) %>% 
  distinct(name) %>%
  summarise(variable = n()) %>%
  ungroup()

p2 <- variable_set %>% 
  left_join(stable_set, by = c("type", "site")) %>% 
  mutate(stable = replace_na(stable, 0)) %>% 
  group_by(site, type) %>% 
  summarise(n = stable + variable) %>% 
  ungroup() %>% 
  left_join(df_loc, by = "site") %>% 
  group_by(location) %>%
  mutate(proportion = n / sum(n),
         total = sum(n),
         ymax = cumsum(proportion),
         ymin = c(0, head(ymax, n = -1))) %>%
  mutate(label_pos = (ymax + ymin) / 2) %>%
  ungroup() %>%
  group_by(type) %>%
  mutate(order = sum(n)) %>%
  ungroup() %>%
  mutate(type = fct_reorder(type, order, .desc = F)) %>%
  ggplot(aes(ymax = ymax, ymin = ymin, xmax = 4, xmin = 3, fill = type)) +
  geom_rect() +
  coord_polar(theta = "y") +
  xlim(c(2, 4)) +
  facet_wrap(~ location, nrow = 3) +
  labs(x = NULL,
       y = NULL,
       subtitle = "For each location",
       fill = NULL) +
  scale_fill_manual(values = type_colors) +
  geom_text(aes(x = 3.5, y = label_pos, label = ifelse(n > 10, as.character(n), "")), check_overlap = T) +
  geom_text(aes(x = 2, y = 0, label = paste0("Total\n", total)), color = "black", check_overlap = T) +
  theme(legend.direction = "horizontal",
        axis.text = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

ggarrange(p1, p2,
          ncol = 2, nrow = 1,
          widths = c(0.2, 1),
          common.legend = TRUE,
          legend = "bottom") +
  plot_annotation(title = "Summary of building numbers and types")
```

For all qualified buildings, we further added one more stringent
criterion to label all buildings with very stable electricity usage
between the two years:

1.  No statistical significant difference (P-value \> 0.05) between the
    two-year electricity usage.

As a results, there are only 66 buildings were labeled in this 'stable'
subset (out of total of 600 buildings) implying that the assumption made
by conventional M&V is overly simplified. As mentioned in Section
\@ref(non-routine-events-impact), such variability is largely associated
with non-routine events and according to the statistics, is more
commonly observed in real applications.

## Control intervention implementation

Figure \@ref(fig:chwst) shows the algorithm for the proposed control
intervention that reset the chiller supply temperature based on the
outdoor weather conditions, which can be commonly found in the
literature [@lee_chilled_2022, @congradac_recognition_2012]. For both
strategies, we assume that the chiller is activated when the outdoor
temperature exceeds 10°C. The baseline strategy, representing the
existing measurements from the dataset, operates with a constant water
supply temperature. The intervention strategy, as illustrated in the
figure, adjusts the water supply temperature dynamically, resetting it
from 7°C to 12°C.

```{r chwst, fig.cap = "Proposed intervention strategy: chilled water supply temperature reset based on outdoor temperature", fig.width=8, fig.height=4}

data.frame(
  OAT = c(10, 15, 25, 30),  
  Baseline = c(6, 6, 6, 6),      
  Intervention = c(12, 12, 7, 7)
  ) %>% 
  pivot_longer(cols = c("Baseline", "Intervention"), names_to = "strategy", values_to = "value") %>% 
  ggplot(aes(x = OAT, y = value, color = strategy)) +
  geom_line(linewidth = 1.2) +
  scale_color_manual(values = ls_colors) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = c(4, 6, 8, 10, 12),
                     limits = c(5.5, 12.5),
                     labels = number_format(suffix = " °C")) +
  scale_x_continuous(expand = c(0, 0), 
                     breaks = c(10, 15, 20, 25, 30), 
                     labels = number_format(suffix = " °C")) +
  labs(x = "Outdoor temperature", 
       y = "Chilled water supply temperature", 
       color = NULL, 
       title = "Intervention logic: chilled water supply temperature reset") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

```

We mapped the chilled water supply temperature reset to the electrical
energy savings as: $$
\text{savings} = mean(E_{CHW}) \times 25\% \times \text{perc_savings}
$$

We assume on average, HVAC systems account for approximately 50% of a
building's total electricity consumption, and the chilled water plant
further consumes around 50% of the HVAC electricity. While this
assumption largely simplifies the diverse energy usage across various
building types, for the scope of this paper, we assume that 25% of the
total building electricity is used by the chilled water plant, $E_{CHW}$
[@us2012commercial]. Typically, the savings from an intervention are not
proportional to the building's hourly electricity usage, which is
generally the challenge for M&V. To address this, we mapped the
resulting electricity savings as a percentage of the plant's normal
operation, calculated as its mean electricity usage over the two-year
period. This percentage is influenced by factors such as outdoor
temperature ($OAT$), intervention supply water temperature
($T_{interv}$), baseline supply water temperature ($T_{base}$) and hour
of the day ($\delta_{occ}$, binary indicator whether during peak hours
from 9 AM to 4 PM).

$$
\text{perc_savings = }
\begin{cases}
0, & \text{ if } OAT < 10°C \\
\left[ \beta_{\text{temp}} \times (\text{T}_{\text{interv}} - \text{T}_{\text{base}}) \right] \times 
\left[ \beta_{\text{occ}} \times \delta_{occ} + \beta_{\text{unocc}} \times (1 - \delta_{occ})\right], & \text{ otherwise}
\end{cases}
$$

Parameters and their pre-defined values are summarized in the table
below. For simplicity, those parameters were not rigorously calibrated
for each building, and were used uniformly across the dataset.

|                        |                                                                  |       |
|------------------------|------------------------------------------------------------------|-------|
| Parameter              | Description                                                      | Value |
| $\beta_{temp}$         | \% savings from setting $T_{interv}$ 1 °C higher than $T_{base}$ | 0.08  |
| $\beta_{occ}$          | \% savings adjustment during occupied hours                      | 1.2   |
| $\beta_{\text{unocc}}$ | \% savings adjustment during unoccupied hours                    | 0.8   |

: Table 1. Parameters for calculating the intervention savings.

## M&V methods comparison

We described more in detail the workflow of both conventional and
randomized M&V methos in the previous study [@RAFTERY2024111134].

In a similar style, we summarized the experimental design as:

-   The HVAC system operates from 06:00 to 22:00 each day, so we use a
    daily sampling interval with the sampling time at midnight each day.

-   Block by day of the week with a block period of 12 weeks.

and stopping criteria:

-   A minimum and maximum of 12 and 108 weeks respectively. The
    randomized schedule covers the entire two-year period but stopping
    criteria enables an early stop at the end of satisfied blocking
    period.

-   At least 80% of the drybulb temperature range in the annual TMY data
    sampled by both strategies.

-   Test for no carryover effect using a t-test with a p-value not
    exceeding a defined significance threshold of 0.05.

-   90% confidence that energy savings exceed or do not exceed 0% using
    the SPRT test. Medium effect size (d = 0.5) quantified by cohen's d
    and calculated SPRT statistics either falls below the lower
    threshold or exceeds the upper threshold.

-   As no baseline data is available, test with an equal sampling ratio
    (50% baseline, 50% intervention).

# Results

```{r abstract, fig.cap="Overall comparison results between the conventional M&V method and the proposed randomized M&V method (both at the stopping criteria and over a two-year period).", fig.height=12, fig.width=10}

#### GA ####
rand_eob_S <- df_seq_fs_stable %>% 
  filter(seq == "eob") %>% 
  left_join(df_fs_stable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(diff = abs(savings - fs), 
         method = "rand_eob") %>% 
  select(name, method, diff)

conv_S <- df_fs_stable %>% 
  filter(method != "rand") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = abs(true - conv), 
         method = "conv") %>% 
  select(name, method, diff)

rand_final_S <- df_fs_stable %>% 
  filter(method != "conv") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = abs(true - rand), 
         method = "rand_final") %>% 
  select(name, method, diff)

df_MW_S <- rbind(rand_eob_S, conv_S, rand_final_S)

# plot for the variable subset
rand_eob_V <- df_seq_fs_variable %>% 
  filter(seq == "eob") %>% 
  left_join(df_fs_variable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site"))%>% 
  mutate(diff = abs(savings - fs), 
         method = "rand_eob") %>% 
  select(name, method, diff)

conv_V <- df_fs_variable %>% 
  filter(method != "rand") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = abs(true - conv), 
         method = "conv") %>% 
  select(name, method, diff)

rand_final_V <- df_fs_variable %>% 
  filter(method != "conv") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = abs(true - rand), 
         method = "rand_final") %>% 
  select(name, method, diff)

df_MW_V <- rbind(rand_eob_V, conv_V, rand_final_V)

# plot for combined dataset
rand_eob_A <- bind_rows(rand_eob_V, rand_eob_S) %>% 
  select(name, rand_eob_diff = diff)

conv_A <- bind_rows(conv_V, conv_S) %>% 
  select(name, conv_diff = diff)

rand_final_A <- bind_rows(rand_final_V, rand_final_S) %>% 
  select(name, rand_final_diff = diff)

df_MW_A <- rand_eob_A %>% 
  left_join(conv_A, by = "name") %>% 
  left_join(rand_final_A, by = "name") %>% 
  pivot_longer(c(rand_eob_diff, conv_diff, rand_final_diff), names_to = "method", values_to = "diff")

p_top <- df_MW_A %>% 
  mutate(method = as.factor(method), 
         method = recode_factor(method, "conv_diff" = "Conventional", "rand_eob_diff" = "Randomized", "rand_final_diff" = "Randomized\n(24 months)")) %>% 
  ggplot(aes(x = method, y = diff, fill = method)) +
  geom_jitter(width = 0.2, alpha = 0.8, size = 0.5) +
  geom_lv(k = 4, outlier.shape = NA) +
  geom_boxplot(outlier.alpha = 0, coef = 0, fill = "#00000000", aes(color = method)) +
  geom_hline(yintercept = 0, color = "#fb8072", linewidth = 1.5, lty = "dashed") +
  geom_text(data = . %>% group_by(method) %>% summarise(mean = mean(diff)) %>% ungroup(), 
            aes(x = method, y = mean, label = paste0(round(mean, digits = 1), " %"))) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = " %")) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = c("grey80", "grey80", "grey80")) + 
  labs(fill = NULL, 
       color = NULL, 
       x = NULL, 
       y = "Absolute error in fractional savings", 
       subtitle = str_glue("All {A_building} buildings with measured weather")) +
  coord_cartesian(ylim = c(0, 23)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.position = "none",
        axis.text.x = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

# TMY
# plot for the stable subset
rand_eob_S <- df_sprt_all_stable %>% 
  filter(seq == "eob") %>% 
  select(-c(seq, n_weeks)) %>% 
  left_join(df_fs_stable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(diff = abs(savings - annual), 
         method = "rand_eob") %>% 
  select(name, method, diff)

conv_S <- df_fs_tmy_stable %>% 
  left_join(df_fs_stable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(diff = abs(savings - conv), 
         method = "conv") %>% 
  select(name, method, diff)

rand_final_S <- df_fs_tmy_stable %>% 
  left_join(df_fs_stable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(diff = abs(savings - rand), 
         method = "rand_final") %>% 
  select(name, method, diff)

df_TW_S <- rbind(rand_eob_S, conv_S, rand_final_S)

# plot for the variable subset
rand_eob_V <- df_sprt_all_variable %>% 
  filter(seq == "eob") %>% 
  select(-c(seq, n_weeks)) %>% 
  left_join(df_fs_variable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(diff = abs(savings - annual), 
         method = "rand_eob") %>% 
  select(name, method, diff)

conv_V <- df_fs_tmy_variable %>% 
  left_join(df_fs_variable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(diff = abs(savings - conv), 
         method = "conv") %>% 
  select(name, method, diff)

rand_final_V <- df_fs_tmy_variable %>% 
  left_join(df_fs_variable %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(diff = abs(savings - rand), 
         method = "rand_final") %>% 
  select(name, method, diff)

df_TW_V <- rbind(rand_eob_V, conv_V, rand_final_V)

# plot for combined dataset
rand_eob_A <- bind_rows(rand_eob_V, rand_eob_S) %>% 
  select(name, rand_eob_diff = diff)

conv_A <- bind_rows(conv_V, conv_S) %>% 
  select(name, conv_diff = diff)

rand_final_A <- bind_rows(rand_final_V, rand_final_S) %>% 
  select(name, rand_final_diff = diff)

df_TW_A <- rand_eob_A %>% 
  left_join(conv_A, by = "name") %>% 
  left_join(rand_final_A, by = "name") %>% 
  pivot_longer(c(rand_eob_diff, conv_diff, rand_final_diff), names_to = "method", values_to = "diff")

p_middle <- df_TW_A %>% 
  mutate(method = as.factor(method), 
         method = recode_factor(method, "conv_diff" = "Conventional", "rand_eob_diff" = "Randomized", "rand_final_diff" = "Randomized\n(24 months)")) %>% 
  ggplot(aes(x = method, y = diff, fill = method)) +
  geom_jitter(width = 0.2, alpha = 0.8, size = 0.5) +
  geom_lv(k = 4, outlier.shape = NA) +
  geom_boxplot(outlier.alpha = 0, coef = 0, fill = "#00000000", aes(color = method)) +
  geom_hline(yintercept = 0, color = "#fb8072", linewidth = 1.5, lty = "dashed") +
  geom_text(data = . %>% group_by(method) %>% summarise(mean = mean(diff)) %>% ungroup(), 
            aes(x = method, y = mean, label = paste0(round(mean, digits = 1), " %"))) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = " %")) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = c("grey80", "grey80", "grey80")) + 
  labs(fill = NULL, 
       x = NULL, 
       y = "Absolute error in fractional savings", 
       subtitle = str_glue("All {A_building} buildings with TMY weather")) +
  coord_cartesian(ylim = c(0, 23)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.position = "none",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

# Timeline plot
df_time <- df_sprt_all_variable %>% 
  filter(seq != "final") %>% 
  bind_rows(df_sprt_all_stable %>% filter(seq != "final")) %>% 
  select(name, seq, n_weeks)

count <- list()
n <- 1
for (i in seq(0, 36, by = 3)){
  
  df_sprt <- df_time %>% 
    filter(seq == "sprt",
           n_weeks <= i)
  
  df_eob <- df_time %>% 
    filter(seq == "eob",
           n_weeks <= i)
  
  df_temp <- df_time %>% 
    filter(seq == "temp", 
           n_weeks <= i)
  
  count[[n]] <- tibble("n_weeks" = i, 
                       "eob" = nrow(df_eob), 
                       "temp" = nrow(df_temp), 
                       "sprt" = nrow(df_sprt))
  
  n <- n + 1
}

count <- bind_rows(count) 

p_bottom <- count %>% 
  ggplot() +
  geom_bar(data = . %>% 
             filter(n_weeks == 24 | n_weeks == 36),
           aes(x = n_weeks, y = eob, fill = "Buildings finishing randomized M&V"), 
           stat = "identity", 
           position = "stack",
           alpha = 0.8, 
           width = 3) +
  geom_line(aes(x = n_weeks, y = temp, color = "Buildings satisfying 80% TMY range"),
            alpha = 0.2) +
  geom_point(aes(x = n_weeks, y = temp, color = "Buildings satisfying 80% TMY range"), 
             size = 1.5) +
  geom_line(aes(x = n_weeks, y = sprt, color = "Buildings satisfying SPRT"),
            alpha = 0.2) +
  geom_point(aes(x = n_weeks, y = sprt, color = "Buildings satisfying SPRT"), 
             size = 1.5, 
             shape = 17) +
  geom_segment(aes(x = 36.5, y = max(eob), xend = 95.5, yend = max(eob)),
               arrow = arrow(length = unit(0.25, "in")),   
               linewidth = 1.1,  
               color = "#fb8072") + 
  annotate(geom = "text", 
           x = 66, 
           y = max(count$eob) - 30, 
           size = 5,
           label = "Excess time by conventional M&V") +
  geom_vline(xintercept = c(12, 24, 36, 48, 96), lty = "dashed", color = "grey80") +
  annotate(geom = "text", 
           x = seq(6, 48, by = 12), 
           y = 200, 
           label = paste0("12-week\nblock"), 
           alpha = 0.5) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = ls_colors) +
  scale_x_continuous(expand = c(0, 0), 
                     limits = c(0, 100),
                     breaks = c(12, 24, 36, 48, 96), 
                     labels = c("12 weeks", "24 weeks", "36 weeks", "1 year", "2 years")) +
  coord_cartesian(ylim = c(0, 650)) +
  labs(x = NULL, 
       y = "Number of buildings", 
       fill = NULL, 
       color = NULL, 
       subtitle = "timeline comparison") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

ggarrange(p_top, p_middle, p_bottom, 
          ncol = 1, nrow = 3,
          heights = c(1.2, 1.2, 1),
          labels = c("a)", "b)", "c)"),
          common.legend = T, 
          legend = "bottom") +
  plot_annotation(title = "Overall comparison of conventional and randomized M&V")
```

Figure \@ref(fig:abstract) shows the overall results of M&V methods
comparison. The estimation error is calculated as the absolute deviation
from the true savings. Subplot a) shows the savings estimated from
measured weather condition and subplot b) shows the savings normalized
on typical meteorological weather file using TOWT model prediction. In
average, the conventional method could deviate 6% in saving estimation
while the the deviation from our proposed randomized method is much
smaller. If the analyst stops right after satisfying all stopping
criteria, then the deviation is around 2% and it further decreases to 1%
when the M&V extends over the same period of the convention method. By
comparing subplot a) and b), we noticed a slight improvement in
estimation accuracy in model fitting, which is less than 1% on average.
This implies that baseline projection to post-retrofit period through
data-driven models is inefficient when the actual baseline changes.
Subplot c) shows the overall timeline of the two methods. It is clear
that the all buildings satisfied all stopping criteria in 36 weeks and
given by the difference in weather conditions, a large portion of the
buildings can get an accurate M&V results in 24 weeks. Therefore, on
average a building analyst should expect 1% accuracy improvement over 1
year extension of the randomized M&V method. However, the red arrow also
implies the excess time required by the conventional M&V method that
only leads to more unreliable results.

## Non-routine events impact

```{r nosaving, fig.cap="Comparison between the two M&V methods in detecting no intervention effect", fig.height=9, fig.width=12}

#### MEAN-NULL ####
rand_eob_S <- df_seq_fs_null_stable %>% 
  filter(seq == "eob") %>% 
  mutate(diff = 0 - fs, 
         method = "rand_eob") %>% 
  select(name, method, diff)

conv_S <- df_fs_null_stable %>% 
  filter(method != "rand") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = true - conv, 
         method = "conv") %>% 
  select(name, method, diff)

rand_final_S <- df_fs_null_stable %>% 
  filter(method != "conv") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = true - rand, 
         method = "rand_final") %>% 
  select(name, method, diff)

df_MW_S <- rbind(rand_eob_S, conv_S, rand_final_S)

# plot for the variable subset
rand_eob_V <- df_seq_fs_null_variable %>% 
  filter(seq == "eob") %>% 
  mutate(diff = 0 - fs, 
         method = "rand_eob") %>% 
  select(name, method, diff)

conv_V <- df_fs_null_variable %>% 
  filter(method != "rand") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = true - conv, 
         method = "conv") %>% 
  select(name, method, diff)

rand_final_V <- df_fs_null_variable %>% 
  filter(method != "conv") %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  mutate(diff = true - rand, 
         method = "rand_final") %>% 
  select(name, method, diff)

df_MW_V <- rbind(rand_eob_V, conv_V, rand_final_V)

# plot for combined dataset
rand_eob_A <- bind_rows(rand_eob_V, rand_eob_S) %>% 
  select(name, rand_eob_diff = diff)

conv_A <- bind_rows(conv_V, conv_S) %>% 
  select(name, conv_diff = diff)

rand_final_A <- bind_rows(rand_final_V, rand_final_S) %>% 
  select(name, rand_final_diff = diff)

df_MW_A <- rand_eob_A %>% 
  left_join(conv_A, by = "name") %>% 
  left_join(rand_final_A, by = "name") %>% 
  pivot_longer(c(rand_eob_diff, conv_diff, rand_final_diff), names_to = "method", values_to = "diff")

df_MW_A %>% 
  mutate(method = as.factor(method), 
         method = recode_factor(method, 
                                "conv_diff" = "Conventional", 
                                "rand_eob_diff" = "Randomized", 
                                "rand_final_diff" = "Randomized\n(24 months)")) %>% 
  ggplot(aes(x = method, y = diff, fill = method)) +
  geom_jitter(width = 0.2, alpha = 0.8, size = 0.5) +
  geom_lv(k = 4, outlier.shape = NA) +
  geom_boxplot(outlier.alpha = 0, coef = 0, fill = "#00000000", aes(color = method)) +
  geom_hline(yintercept = 0, color = "#fb8072", linewidth = 1, lty = "dashed") +
  geom_text(data = . %>% group_by(method) %>% summarise(mean = mean(diff)) %>% ungroup(), 
            aes(x = method, y = mean, label = paste0(round(mean, digits = 1), " %"))) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 6), 
                     labels = number_format(suffix = " %")) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = c("grey80", "grey80", "grey80")) + 
  labs(fill = NULL, 
       x = NULL, 
       y = "Mean effect in fractional savings",
       title = "Mean effect of non-routine events when using different M&V methods", 
       subtitle = str_glue("All {A_building} buildings with measured weather")) +
  coord_cartesian(ylim = c(-18, 18)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.position = "none",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

As mentioned in Section \@ref(data-filtering), the majority of the
selected buildings are affected by non-routine events. To assess the
reliability of the two M&V methods, we repeated the M&V procedure on the
dataset before applying the reset control. In other words, the
intervention has no effect on building electricity usage and our
objective is to compare which method can provide an estimation closer to
0 savings. Figure \@ref(fig:nosaving) shows the randomized M&V method
(both at the time of satisfying all stopping criteria and at the end of
two-year period) produced consistent results. We included the absolute
deviation in the appendix. Similarly, stopping early can lead to larger
uncertainty similar to Figure \@ref(fig:abstract). As for conventional
method, the uncertainty range is much larger than the randomized method
and it shows on average the deviation is 1.3% meaning a building analyst
can claim 1.3% energy savings were detected when there isn't any.

# Discussion

## TOWT modeling accuracy

To assess modeling accuracy, we used the Coefficient of Variation of
Root-Mean Squared Error, or CV(RMSE) as the error metric. Since this
metric is calculated as a normalized value, it is useful to compare
different model fitting results. Guideline 14 requires that
whole-building baseline model fitting accuracy should maintain a
CV(RMSE) lower than 30% [@ashrae_ashrae_2014]. In addition one study
focusing on the baseline energy data-driven model fitting indicates that
TOWT performs as accurate as other more advanced machine learning models
[@granderson_accuracy_2016] and the calculated CV(RMSE) distribution for
a large sample of commercial buildings indicates a median of 20%. Figure
\@ref(fig:towtacc)shows the distribution of model fitting accuracy,
calculated separately for the two measurement sets. The box plot
provides a statistical summary of the accuracy data points, with error
bars highlighting any points beyond the range, which should be
considered outliers. The results show that the TOWT model's performance
in this study aligns with, and is even slightly better than, the
performance reported in the literature. The improved results can be
attributed to relatively strict data filtering criteria for
pre-processing described in Section \@ref(method).

```{r towtacc, fig.cap="TOWT model fitting accuracy distribution for all buildings included (with each data point representing one building)", fig.width=8, fig.height=4}

df_model_acc_variable %>% 
  mutate(group = "variable subset") %>% 
  rbind(df_model_acc_stable %>% mutate(group = "stable subset")) %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x = group, y = cvrmse)) +
  stat_boxplot(geom ='errorbar', width = 0.1) +
  geom_boxplot(outlier.shape = NA, width = 0.25) +
  geom_jitter(position = position_jitter(0.1), alpha = 0.1, size = 1.2) +
  geom_text(aes(x = group, y = median, label = paste0(round(median, digits = 0), " %")), 
            data = . %>% 
              group_by(group) %>% 
              summarise(median = median(cvrmse)) %>% 
              ungroup(), 
            position = position_nudge(x = 0.25)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %")) +
  coord_cartesian(ylim = c(0, 50)) +
  labs(x = NULL, 
       y = NULL, 
       color = NULL, 
       title = "TOWT model fitting accuracy", 
       subtitle = "CV(RMSE)") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "none",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

The limitation of regression models is that despite they capture well on
the mean building energy consumption but can underestimate the 15-min
daily peak load [@granderson2021], which is not useful for assessing
savings for demand response events.

## Sampling interval impact

```{r absinterv2, fig.cap="Comparison of different sampling interval impact on M&V estimation accuracy", fig.height=9, fig.width=12}

#### ABS-INT ####
# dropped version
df_interval_keep_stable %<>% 
  left_join(df_fs_stable %>% filter(method == "rand"), by = c("name", "site")) %>% 
  select(-c("scenario", "method")) %>% 
  rename("interval_fs_1" = "savings") %>% 
  left_join(df_fs_tmy_stable, by = c("name", "site")) %>% 
  select(-conv) %>% 
  rename("interval_tmy_1" = "rand")

df_interval_keep_variable %<>% 
  left_join(df_fs_variable %>% filter(method == "rand"), by = c("name", "site")) %>% 
  select(-c("scenario", "method")) %>% 
  rename("interval_fs_1" = "savings") %>% 
  left_join(df_fs_tmy_variable, by = c("name", "site")) %>% 
  select(-conv) %>% 
  rename("interval_tmy_1" = "rand")

df_drop <- df_interval_drop_stable %>% 
  bind_rows(df_interval_drop_variable) %>% 
  select(name, contains("fs")) %>% 
  rename("drop_1" = "interval_fs_1", 
         "drop_2" = "interval_fs_2", 
         "drop_3" = "interval_fs_3", 
         "drop_7" = "interval_fs_7")  

df_keep <- df_interval_keep_stable %>% 
  bind_rows(df_interval_keep_variable) %>% 
  select(name, contains("fs")) %>% 
  rename("keep_1" = "interval_fs_1",
         "keep_2" = "interval_fs_2", 
         "keep_3" = "interval_fs_3", 
         "keep_7" = "interval_fs_7")

df_MW_A <- df_drop %>% 
  left_join(df_keep, by = c("name")) %>%
  left_join(rbind(df_fs_stable %>% 
              filter(scenario == "ref" & method == "true"), 
              df_fs_variable %>% 
              filter(scenario == "ref" & method == "true")), by = "name") %>% 
  mutate(drop_1 = abs(drop_1 - savings),
         drop_2 = abs(drop_2 - savings), 
         drop_3 = abs(drop_3 - savings), 
         drop_7 = abs(drop_7 - savings), 
         keep_1 = abs(keep_1 - savings),
         keep_2 = abs(keep_2 - savings), 
         keep_3 = abs(keep_3 - savings), 
         keep_7 = abs(keep_7 - savings)) %>% 
  pivot_longer(cols = contains("drop") | contains("keep"),  
               names_to = c("group", "interval"),
               names_pattern = "(drop|keep)_(\\d+)",
               values_to = "diff")

p_top <- df_MW_A %>% 
  mutate(interval = as.factor(interval), 
         interval = recode_factor(interval, 
                                  "1" = "1-day\nsampling",
                                  "2" = "2-day\nsampling", 
                                  "3" = "3-day\nsampling", 
                                  "7" = "7-day\nsampling"), 
         group = as.factor(group), 
         group = recode_factor(group, "drop" = "Dropped", "keep" = "Kept")) %>% 
  ggplot(aes(x = interval, y = diff, fill = group)) +
  geom_jitter(alpha = 0.8, 
              size = 0.5, 
              position = position_jitterdodge(dodge.width = 0.75, jitter.width = 0.2)) +
  geom_lv(k = 4, outlier.shape = NA, position = position_dodge(width = 0.8)) +
  geom_boxplot(outlier.alpha = 0, coef = 0, fill = "#00000000", aes(color = group), width = 0.8) +
  geom_hline(yintercept = 0, color = "#fb8072", linewidth = 1, lty = "dashed") +
  geom_text(data = . %>% group_by(group, interval) %>% summarise(mean = mean(diff, na.rm = T)) %>% ungroup(), 
            aes(x = interval, y = mean, group = group,
                label = paste0(round(mean, digits = 1), " %")), 
            position = position_dodge(width = 0.75)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = " %")) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = c("grey80", "grey80")) + 
  labs(fill = NULL, 
       color = NULL, 
       x = NULL, 
       y = "Absolute error in fractional savings", 
       subtitle = str_glue("All {A_building} buildings with measured weather")) +
  coord_cartesian(ylim = c(0, 10)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.position = "bottom",
        axis.text.x = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

# TMY version
df_drop <- df_interval_drop_stable %>% 
  bind_rows(df_interval_drop_variable) %>% 
  select(name, contains("tmy")) %>% 
  rename("drop_1" = "interval_tmy_1",
         "drop_2" = "interval_tmy_2", 
         "drop_3" = "interval_tmy_3", 
         "drop_7" = "interval_tmy_7")  

df_keep <- df_interval_keep_stable %>% 
  bind_rows(df_interval_keep_variable) %>% 
  select(name, contains("tmy")) %>% 
  rename("keep_1" = "interval_tmy_1",
         "keep_2" = "interval_tmy_2", 
         "keep_3" = "interval_tmy_3", 
         "keep_7" = "interval_tmy_7")

df_TW_A <- df_drop %>% 
  left_join(df_keep, by = c("name")) %>%
  left_join(rbind(df_fs_stable %>% 
              filter(scenario == "ref" & method == "true"), 
              df_fs_variable %>% 
              filter(scenario == "ref" & method == "true")), by = "name") %>% 
  mutate(drop_1 = abs(drop_1 - savings), 
         drop_2 = abs(drop_2 - savings), 
         drop_3 = abs(drop_3 - savings), 
         drop_7 = abs(drop_7 - savings), 
         keep_1 = abs(keep_1 - savings), 
         keep_2 = abs(keep_2 - savings), 
         keep_3 = abs(keep_3 - savings), 
         keep_7 = abs(keep_7 - savings)) %>% 
  pivot_longer(cols = contains("drop") | contains("keep"),  
               names_to = c("group", "interval"),
               names_pattern = "(drop|keep)_(\\d+)",
               values_to = "diff")

p_bottom <- df_TW_A %>% 
  mutate(interval = as.factor(interval), 
         interval = recode_factor(interval, 
                                  "1" = "1-day\nsampling",
                                  "2" = "2-day\nsampling", 
                                  "3" = "3-day\nsampling", 
                                  "7" = "7-day\nsampling"), 
         group = as.factor(group), 
         group = recode_factor(group, "drop" = "Dropped", "keep" = "Kept")) %>% 
  ggplot(aes(x = interval, y = diff, fill = group)) +
  geom_jitter(alpha = 0.8, 
              size = 0.5, 
              position = position_jitterdodge(dodge.width = 0.75, jitter.width = 0.2)) +
  geom_lv(k = 4, outlier.shape = NA, position = position_dodge(width = 0.8)) +
  geom_boxplot(outlier.alpha = 0, coef = 0, fill = "#00000000", aes(color = group), width = 0.8) +
  geom_hline(yintercept = 0, color = "#fb8072", linewidth = 1, lty = "dashed") +
  geom_text(data = . %>% group_by(group, interval) %>% summarise(mean = mean(diff, na.rm = T)) %>% ungroup(), 
            aes(x = interval, y = mean, group = group,
                label = paste0(round(mean, digits = 1), " %")), 
            position = position_dodge(width = 0.75)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = " %")) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = c("grey80", "grey80")) + 
  labs(fill = NULL, 
       color = NULL, 
       x = NULL, 
       y = "Absolute error in fractional savings", 
       subtitle = str_glue("All {A_building} buildings with TMY weather")) +
  coord_cartesian(ylim = c(0, 10)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

ggarrange(p_top, p_bottom, 
          ncol = 1, nrow = 2, 
          align = "hv", 
          common.legend = T, 
          legend = "bottom") +
  plot_annotation(title = "Absolute error in savings estimation of ranodmized M&V by different sampling intervals")

```

```{r absinterv, fig.cap="Comparison of different sampling interval impact on M&V estimation accuracy", fig.height=9, fig.width=12}

# sprt sequence plot for different sampling intervals
df_seq_interval_fs_keep_stable %<>% 
  select(-c(sprt, temp, final)) %>% 
  bind_rows(df_seq_fs_stable %>% 
              filter(seq == "eob") %>% 
              select(-seq) %>% 
              mutate(interval = 1) %>% 
              rename(eob = "fs"))  

df_seq_interval_fs_keep_variable %<>% 
  select(-c(sprt, temp, final)) %>% 
  bind_rows(df_seq_fs_variable %>% 
              filter(seq == "eob") %>% 
              select(-seq) %>% 
              mutate(interval = 1) %>% 
              rename(eob = "fs")) 

df_drop <- df_seq_interval_fs_drop_stable %>% 
  bind_rows(df_seq_interval_fs_drop_variable) %>% 
  select(name, eob, interval) %>% 
  mutate(group = "drop")

df_keep <- df_seq_interval_fs_keep_stable %>% 
  bind_rows(df_seq_interval_fs_keep_variable) %>% 
  select(name, eob, interval) %>% 
  mutate(group = "keep")

df_MW_A <- df_drop %>% 
  bind_rows(df_keep) %>% 
  left_join(rbind(df_fs_stable %>% 
                    filter(scenario == "ref" & method == "true"), 
                  df_fs_variable %>% 
                    filter(scenario == "ref" & method == "true")), by = "name") %>% 
  mutate(diff = abs(eob - savings)) %>% 
  filter(is.finite(diff))

p_top <- df_MW_A %>% 
  mutate(interval = as.factor(interval), 
         interval = recode_factor(interval, 
                                  "1" = "1-day\nsampling",
                                  "2" = "2-day\nsampling", 
                                  "3" = "3-day\nsampling", 
                                  "7" = "7-day\nsampling"), 
         group = as.factor(group), 
         group = recode_factor(group, "drop" = "Dropped", "keep" = "Kept")) %>% 
   ggplot(aes(x = interval, y = diff, fill = group)) +
  geom_jitter(alpha = 0.8, 
              size = 0.5, 
              position = position_jitterdodge(dodge.width = 0.75, jitter.width = 0.2)) +
  geom_lv(k = 4, outlier.shape = NA, position = position_dodge(width = 0.8)) +
  geom_boxplot(outlier.alpha = 0, coef = 0, fill = "#00000000", aes(color = group), width = 0.8) +
  geom_hline(yintercept = 0, color = "#fb8072", linewidth = 1, lty = "dashed") +
  geom_text(data = . %>% group_by(group, interval) %>% summarise(mean = mean(diff, inf.rm = T)) %>% ungroup(), 
            aes(x = interval, y = mean, group = group,
                label = paste0(round(mean, digits = 1), " %")), 
            position = position_dodge(width = 0.75)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = " %")) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = c("grey80", "grey80")) + 
  labs(fill = NULL, 
       color = NULL, 
       x = NULL, 
       y = "Absolute error in fractional savings", 
       subtitle = str_glue("All {A_building} buildings with measured weather")) +
  coord_cartesian(ylim = c(0, 20)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.position = "bottom",
        axis.text.x = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

# with tmy version
df_seq_interval_nm_keep_stable %<>% 
  select(-c(sprt, temp, final)) %>% 
  bind_rows(df_sprt_all_stable %>% 
             filter(seq == "eob") %>% 
              select(-c(seq, n_weeks)) %>% 
              mutate(interval = 1) %>% 
              rename(eob = "annual")) 

df_seq_interval_nm_keep_variable %<>% 
  select(-c(sprt, temp, final)) %>% 
  bind_rows(df_sprt_all_variable %>% 
              filter(seq == "eob") %>% 
              select(-c(seq, n_weeks)) %>% 
              mutate(interval = 1) %>% 
              rename(eob = "annual")) 

df_drop <- df_seq_interval_nm_drop_stable %>% 
  bind_rows(df_seq_interval_nm_drop_variable) %>% 
  select(name, eob, interval) %>% 
  mutate(group = "drop")

df_keep <- df_seq_interval_nm_keep_stable %>% 
  bind_rows(df_seq_interval_nm_keep_variable) %>% 
  select(name, eob, interval) %>% 
  mutate(group = "keep")

df_TW_A <- df_drop %>% 
  bind_rows(df_keep) %>% 
  left_join(rbind(df_fs_stable %>% 
                    filter(scenario == "ref" & method == "true"), 
                  df_fs_variable %>% 
                    filter(scenario == "ref" & method == "true")), by = "name") %>% 
  mutate(diff = abs(eob - savings)) %>% 
  filter(is.finite(diff))

p_bottom <- df_TW_A %>% 
  mutate(interval = as.factor(interval), 
         interval = recode_factor(interval, 
                                  "1" = "1-day\nsampling",
                                  "2" = "2-day\nsampling", 
                                  "3" = "3-day\nsampling", 
                                  "7" = "7-day\nsampling"), 
         group = as.factor(group), 
         group = recode_factor(group, "drop" = "Dropped", "keep" = "Kept")) %>% 
  ggplot(aes(x = interval, y = diff, fill = group)) +
  geom_jitter(alpha = 0.8, 
              size = 0.5, 
              position = position_jitterdodge(dodge.width = 0.75, jitter.width = 0.2)) +
  geom_lv(k = 4, outlier.shape = NA, position = position_dodge(width = 0.8)) +
  geom_boxplot(outlier.alpha = 0, coef = 0, fill = "#00000000", aes(color = group), width = 0.8) +
  geom_hline(yintercept = 0, color = "#fb8072", linewidth = 1, lty = "dashed") +
  geom_text(data = . %>% group_by(group, interval) %>% summarise(mean = mean(diff, na.rm = T)) %>% ungroup(), 
            aes(x = interval, y = mean, group = group,
                label = paste0(round(mean, digits = 1), " %")), 
            position = position_dodge(width = 0.75)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = " %")) +
  scale_fill_manual(values = ls_colors) +
  scale_color_manual(values = c("grey80", "grey80")) + 
  labs(fill = NULL, 
       color = NULL, 
       x = NULL, 
       y = "Absolute error in fractional savings", 
       subtitle = str_glue("All {A_building} buildings with TMY weather")) +
  coord_cartesian(ylim = c(0, 20)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

ggarrange(p_top, p_bottom, 
          ncol = 1, nrow = 2, 
          align = "hv", 
          common.legend = T, 
          legend = "bottom") +
  plot_annotation(title = "Absolute error in savings estimation of randomized M&V after satisfying all stopping criteria")
```

## Sampling ratio impact

Another advantage of using randomized M&V is the flexibility of changing
sampling ratio after the target savings detected. For instance, the
building owner can continue sampling at a 50%/50% ratio between the
baseline and the intervention to further reduce the uncertainty
associated with the savings. Alternatively, they could switch to 100%
intervention to maximize savings on the utility bill, though this
approach risks the baseline becoming outdated. A middle-ground approach
would be to sample at an 20%/80% ratio between the baseline and the
intervention. Through this way, the software-as-a-service company can
accurately model future baseline changes and adjust customers' bill
accordinly with minimum baseline days sampled. To demonstrate, figure
\@ref(fig:cont) shows after the analyst reports the randomized M&V
results indicated in Figure \@ref(fig:timeline), a new schedule sampling
at 20%/80% was implemented till the end of the year. Similarly, we
calculated the accuracy metric as the absolute deviation from the annual
true savings.

The plot shows the deviation remains consistent in the stable subset
comapred to Figure \@ref(fig:stablefrac) but increased slightly in the
variable subset compare to to Figure \@ref(fig:variablefrac). With
unbalanced sampling ratio, there is a trade-off for estimation accuracy.
But compared with conventional method, this is still a preferred
approach. Most importantly, this approach allows customers to realize a
proportion of expected savings while documenting baseline measurement.
For example, opting for an early stop at 24 or 36 weeks with 50%
intervention, followed by re-sampling at 80% until the year's end
(\~another 20 weeks), would enable customers to capture about 65% of the
full-range savings.

## Limitations

We identify two key limitations in this study:

1.  Application of control intervention: Although the primary focus is
    on accurately detecting intervention effects rather than validating
    their broader impact, the simulated intervention remains somewhat
    generic considering the diverse building types and climate zones in
    the BDG2 dataset. For simplicity, we applied the parameters listed
    in Table 1 uniformly across all buildings. Yet, in some cases,
    raising the water temperature by 1°C might yield more or less than
    8% in electricity savings due to target buildings' demand
    flexibility.
2.  Design of the randomized switchback experiment: In this study, we
    assumed a daily sampling interval would suffice for most commercial
    buildings, but exceptions exist. Buildings with significant thermal
    lag, such as those with heavy concrete construction, hot water
    tanks, or Thermally Active Building Systems (TABS), may experience
    carryover effects. In these cases, the impact of one day's control
    strategy can influence subsequent measurements due to thermal
    storage. For example, if chilled water is pre-charged in the thermal
    mass at the end of a sampled intervention day and the control
    swicthes to baseline at 12 AM the following day, the analyst would
    observe lower energy consumption in that sampled baseline day. While
    this study does not account for carryover effects, we recommend
    using a 3-day sampling interval and excluding non-consecutive days
    in practice to 'wash out' residual effects from previous strategies.

# Conclusion

This research demonstrated the application of a novel whole-building
measurement and verification (M&V) method, comparing its performance to
the conventional approach outlined in ASHRAE Guideline 14 using a large,
open-source commercial building dataset. The proposed M&V method
leverages the randomized experimental design concept from other
scientific fields, along with statistical sequential inference
techniques, to determine when target savings are detected. We used a
virtual control retrofit case---resetting the chilled water setpoint
based on outdoor weather conditions---and applied it to over 500
filtered commercial buildings. By comparing the savings estimations of
the conventional method with the novel randomized method, we found that
the randomized approach provides faster and more robust savings
estimations.

Specifically, we showed that throughout 7 climate zones assessed in this
study, the randomized M&V can provide a saving estimation by 36 weeks
(with the majority finishes by 24 weeks) once all stopping criteria
satisfied. In contrast, the conventional method requires a full range of
baseline and intervention measurements under normal operating
conditions, typically taking 6-9 months for each phase. Most
importantly, Furthermore, we verified that with reduced M&V timeline the
randomized method can estimate savings more accurately than the two-year
conventional method.

We also evaluated the impact of non-routine events on the proposed M&V
method by: 1) introducing a known change, such as occupancy-induced
energy reductions, and 2) detecting when no intervention was applied in
buildings with marginal energy consumption differences. In both cases,
we demonstrated that baseline model fitting could be biased, while
randomization effectively blocked confounding effects, ensuring the
robustness of savings estimations.

Although the limitation of the method is that it only applies to a
subset of all M&V use cases (i.e., strategies that can be switched on
and off), we believe its true value lies in the usefulness and
convenience for most control retrofit validation in the field test.

# Acknowledgements

# References
