---
title: "Reliability Demonstration of A Novel Randomized Measurement and Verification Method for Switchable Control Retrofit Using Large-scale Public Dataset"
author:
  - Aoyu Zou ^[Center for the Built Environment, University of California Berkeley, USA] ^[Correspondence to aoyuzou@berkeley.edu], Paul Raftery ^1^, Stefano Schiavon ^1^, Carlos Durate ^1^
abstract: Conventional measurement and verification (M&V) methods for whole-building energy savings estimation are both time-consuming and unreliable, especially when non-routine events occur during the M&V process. Those events are unrelated to the proposed intervention strategy but have substaintial impacts on the building energy consumption. In this study, we argue that for switchable interventions (e.g. most of the control retrofits) can benefit from random sampling where the analyst randomly decide which strategy (i.e. baseline or intervention) to implement each day. We tested the novel randomized M&V method on a large public dataset which covers multiple climate zones and types of commercial buildings. We applied a virtual chilled water supply temperature reset based on outdoor weather as a control retrofit intervention. Our study shows that the new M&V method can estimate the savings accurately much quicker than the conventional method and most importantly, the estimation results are much more robust compared to the conventional method when non-routine events are present. 

output:
  bookdown::word_document2:
    reference_docx: "../paper/template.docx"
editor_options: 
  markdown: 
    wrap: 72
# bibliography: references.bib
# biblio-style: apalike

knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../paper"
    )
  })
---

```{r setup, include = FALSE, cache = FALSE}

# knitr setup
knitr::opts_chunk$set(echo = F, 
                      message = F,
                      warning = F,
                      dev = "jpeg",
                      # cache = T,
                      dpi = 300,
                      fig.show = "hold",
                      fig.pos = "b", 
                      fig.path = "../figs/manuscript/figs/")

# str(knitr::opts_chunk$get()) # see for all options

```

```{r prep, include = FALSE, cache = FALSE}
#### LIBRARIES ####
require(pacman)

# load packages using pacman
pacman::p_load(tidyverse, lubridate, here, stats, zoo, scales, ggpubr, patchwork, RColorBrewer)

# turn off scientific notation
options(scipen = 999, digits = 15)

# set directory
here::i_am("manuscript.rmd")

# set default theme for ggplot
theme_set(theme_minimal())

# define base ggplot theme
theme_update(plot.title = element_text(size = 14, colour = "grey20", face = "bold", hjust = 0.5),
             plot.subtitle = element_text(size = 10, colour = "grey20", face = "italic", hjust = 0.5, margin = margin(b = 10)),
             plot.caption = element_text(size = 10, colour = "grey20", face = "italic", hjust = 0.5),
             plot.background = element_rect(fill = "white", colour = NA),
             panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             axis.text = element_text(size = 10),
             strip.text = element_text(size = 10, color = "grey20", face = "bold"),
             strip.background = element_blank())

# colors
ls_colors <- c("Baseline" = "#a6bddb",
               "Measured baseline" = "#a6bddb",
               "Adjusted baseline" = "#a6bddb",
               "Projected baseline\n(no change)" = "#2b8cbe",
               "Intervention" = "#fdbb84",
               "Measured interv" = "#fdbb84",
               "True savings" = "black",
               "True normalized savings" = "black")

# parameters
ctr_params <- list(peak_hours = 10:16,
                   chwl_perc = 0.25,
                   step_perc = 0.08,
                   conv_swt = 6,
                   weather_knots = c(15, 25),
                   swt_knots = c(12, 7),
                   coe_peak = 0.8,
                   coe_off = 1.2,
                   enable_temp = 8)

occ_params <- list(change_start = 5,
                   change_end = 8,
                   change = 20)





#### FUNCTIONS ####
# Function defined to read downloaded tmy files
get_tmy <- function(all_sites, readfile_path){
  
  df_tmy <- data.frame()
  
  for (site in all_sites){
    df <- read_csv(paste0(readfile_path, "tmy/", str_glue("{site}.epw")),
                   skip = 8, col_types = "ddddd-d---------------------------------",
                   col_names = c("year", "month", "day", "hour", "min", "tmy")) %>%
      mutate(year = 2017,
             time = ymd_h(paste(paste(year, month, day, sep = "-"), hour, sep = " ")),
             temp = tmy) %>%
      dplyr::select(time, temp) %>% 
      mutate(site = site)
    
    df_tmy <- rbind(df_tmy, df)
  }
  
  return(df_tmy)
}

# Function defined to add chwst reset intervention
run_reset <- function(df_baseline){
  
  mean <- mean(df_baseline$base_eload, na.rm = T) * ctr_params$chwl_perc
  
  grad <- (ctr_params$swt_knots[2] - ctr_params$swt_knots[1]) / 
    (ctr_params$weather_knots[2] - ctr_params$weather_knots[1])
  
  interc <- ctr_params$swt_knots[2] - (ctr_params$weather_knots[2] * grad)
  
  df_interv <- df_baseline %>% 
    mutate(swt = t_out * grad + interc, 
           chwl = mean,
           hour = hour(datetime)) %>% 
    mutate(swt = ifelse(swt > ctr_params$swt_knots[1], ctr_params$swt_knots[1], ifelse(swt < ctr_params$swt_knots[2], ctr_params$swt_knots[2], swt)), 
           temp_savings = ifelse(t_out >= ctr_params$enable_temp, (swt - ctr_params$conv_swt) * ctr_params$step_perc, 0), 
           time_adj = ifelse(hour %in% ctr_params$peak_hours, ctr_params$coe_peak, ctr_params$coe_off), 
           perc_savings = temp_savings * time_adj, 
           savings = chwl * perc_savings, 
           interv_eload = base_eload - savings) %>% 
    select(datetime, base_eload, interv_eload, t_out)
  
  return(df_interv)
}

# Function defined to interpolate NAs
run_interpo <- function(df_all){
  
  na_counts <- df_all %>%
    mutate(date = date(timestamp)) %>%
    group_by(date) %>%
    summarize(na_hours = sum(is.na(eload)))
  
  # Filter out days with more than half of the hours having NAs
  valid_days <- na_counts %>%
    filter(na_hours <= 12) %>%
    pull(date)
  
  df_filtered <- df_all %>%
    filter(date(timestamp) %in% valid_days)
  
  df_filtered <- df_filtered %>%
    mutate(across(c(eload, t_out), ~ zoo::na.approx(., na.rm = FALSE))) %>% 
    rename(datetime = timestamp, 
           base_eload = eload)
  
  return(df_filtered)
}

# Function defined to adjust the plot scale
get_scale <- function(eload, range = 2){
  
  min_y <- mean(eload, na.rm = T) - range * sd(eload, na.rm = T)
  max_y <- mean(eload, na.rm = T) + range * sd(eload, na.rm = T)
  
  return(c(min_y, max_y))
}
```

```{r readdata, include=FALSE}

#### READ DATA ####
# Tidy dataset
readfile_tidy <- str_glue("../readfiles/tidy/")
fig_path = "../figs/manuscript/"

df_energy_tidy <- read_rds(paste0(readfile_tidy, "df_energy.rds"))
df_meta_tidy <- read_rds(paste0(readfile_tidy, "df_meta.rds"))
df_weather_tidy <- read_rds(paste0(readfile_tidy, "df_weather.rds"))
df_sprt_all_tidy <- read_rds(paste0(readfile_tidy, "df_sprt_all.rds"))
df_seq_FS_tidy <- read_rds(paste0(readfile_tidy, "df_seq_FS.rds"))
df_NRE_occ <- read_rds(paste0(readfile_tidy, "df_NRE_occ.rds"))
df_MD_tidy <- read_rds(paste0(readfile_tidy, "df_MD.rds"))
df_FS_tidy <- read_rds(paste0(readfile_tidy, "df_FS.rds"))
df_eui_tidy <- read_rds(paste0(readfile_tidy, "df_eui.rds"))
df_cont_MD_tidy <- read_rds(paste0(readfile_tidy, "df_cont_MD.rds"))
df_cont_FS_tidy <- read_rds(paste0(readfile_tidy, "df_cont_FS.rds"))
df_model_acc_tidy <- read_rds(paste0(readfile_tidy, "df_model_acc.rds"))

all_sites_tidy <- df_energy_tidy %>%
  select(site) %>%
  distinct() %>%
  arrange(site)

all_types_tidy <- df_energy_tidy %>%
  select(type) %>%
  mutate(type = as.factor(type)) %>%
  distinct()

all_names_tidy <- df_energy_tidy %>%
  select(name) %>%
  distinct(name)

# Messy dataset
readfile_messy <- str_glue("../readfiles/messy/")

df_energy_messy <- read_rds(paste0(readfile_messy, "df_energy.rds"))
df_meta_messy <- read_rds(paste0(readfile_messy, "df_meta.rds"))
df_weather_messy <- read_rds(paste0(readfile_messy, "df_weather.rds"))
df_sprt_all_messy <- read_rds(paste0(readfile_messy, "df_sprt_all.rds"))
df_seq_FS_messy <- read_rds(paste0(readfile_messy, "df_seq_FS.rds"))
df_MD_messy <- read_rds(paste0(readfile_messy, "df_MD.rds"))
df_FS_messy <- read_rds(paste0(readfile_messy, "df_FS.rds"))
df_eui_messy <- read_rds(paste0(readfile_messy, "df_eui.rds"))
df_cont_MD_messy <- read_rds(paste0(readfile_messy, "df_cont_MD.rds"))
df_cont_FS_messy <- read_rds(paste0(readfile_messy, "df_cont_FS.rds"))
df_FS_nsprt <- read_rds(paste0(readfile_messy, "df_FS_nsprt.rds"))
df_MD_nsprt <- read_rds(paste0(readfile_messy, "df_MD_nsprt.rds"))
df_seq_FS_nsprt <- read_rds(paste0(readfile_messy, "df_seq_FS_nsprt.rds"))
df_model_acc_messy <- read_rds(paste0(readfile_messy, "df_model_acc.rds"))

all_sites_messy <- df_energy_messy %>%
  select(site) %>%
  distinct() %>%
  arrange(site)

all_types_messy <- df_energy_messy %>%
  select(type) %>%
  mutate(type = as.factor(type)) %>%
  distinct()

all_names_messy <- df_energy_messy %>%
  select(name) %>%
  distinct(name)

# read functions
function_path <- "../functions/"
source(paste0(function_path, "model_fit.R"))
source(paste0(function_path, "model_pred.R"))
source(paste0(function_path, "prepost_plot.R"))
```

# Introduction

## Background

### Conventional M&V

The conventional M&V process can be found in ASHRAE Guideline 14
<!--# TODO:Add reference -->, which are summarized in Figure
\@ref(fig:convmethod). The process normally starting with baseline
measurement for a whole year before retrofit. After deploying the
intervention, building performance measurement continues for another
year. At the end of the two-year period,

### Randomized M&V

### Non-routine events

A common non-routine event in an energy-saving M&V project is a change
in occupancy or a substantial change in occupant behavior. Those changes
have a significant impact on measured building energy consumption and
are typically not caused by the intervention strategy. If the M&V
analyst are not aware of the change and has no reasonable approach to
make adjustment on the energy measurements, the saving estimation result
is largely biased. For example, most commercial buildings are unoccupied
during the pandemic in 2020 and thus building managers have observed a
drastically decrease in monthly energy bills despite no energy-efficient
measures were implemented. In addition, common adjustments by an
building analyst only consider the variations of outdoor weather
conditions such as by fitting a regression model (e.g. Time-Of-Week
Temperature model).

### BDG2 dataset

## Literature review

### Measurement and verification

### Randomized experimental design

## Objectives

As mentioned, the goal of a M&V project is to determine the effect
(normally savings) of an energy-efficient intervention. And in this
study, we further limit the study scope to switchable interventions,
which mostly encapsulates control retrofits. An example intervention of
such type can be a control retrofit developed by a software-as-a-service
company that enables the chilled water plant to reset its supply water
temperature based on outdoor weather condition. Therefore, we defined
the M&V scenario as follows:

*"A company wishes to sell their supply temperature reset control
software package to a building owner and guarantees its effect in
reducing building electricity usage. If the building owner decides to
purchase the service, the company agrees to charge the service fee based
on a fraction of the measured savings."*

As required by the M&V scenario, we assessed the performance of both the
conventional and the novel randomized M&V methods by estimating the
intervention energy savings for all valid buildings in the dataset. By
conducting such analysis, we hope to:

1.  Demonstrate the implementation of the proposed randomized M&V method
    using a public available dataset. We ensured the reproducibility of
    the M&V method by making the analysis code open source including
    randomized schedule generation, sequential statistical analysis,
    energy modeling and normalized saving calculation. Using the
    available open resources, building analysts should be able to
    seamlessly integrate and apply them in their own M&V projects.

2.  Compare the energy saving estimation accuracy between the
    conventional and the randomized method. In particular, this study
    extends the comparison to large samples of buildings of various
    types and across multiple climate zones.

3.  Verify the superior robustness of the randomized method over the
    conventional method. By using the realistic measurements from
    real-world buildings, which contains various sources of noises,
    could largely reflect the challenges that a building analyst would
    be facing in any real project. Particularly, as we will demonstrate
    in the following sections, non-routine events (i.e. 'noises' in the
    measurements) have less impact in the energy saving estimation when
    the analyst uses the randomized approach.

# Method

As mentioned, this study leverages a large public dataset to demonstrate
the energy saving estimation results of a novel M&V method inspired by
other scientific research fields. We outlined the methodology of the
study in Figure \@ref(fig:flowchart) and outlined several key components
in this section.

```{r flowchart, fig.cap = "Workflow summary of the methodology of this paper", out.width="50%"}

knitr::include_graphics(paste0(fig_path, "flowchart.png"))
```

## Building filtering

### 'Tidy' subset

In this study, we extracted the electricity measurements from the BDG2
dataset. On a first pass, we filtered out buildings with less noise
based on the following criteria:

1.  Missing values \< 1000: given the hourly resolution of all
    measurements, this is equivalent to more than a month of missing
    days.

2.  Mean electricity usage \> 0 kWh: target buildings should have active
    electricity usage.

3.  No statistical significant difference (P-value \> 0.05) between the
    two-year electricity usage: target buildings should have no change
    in the electricity usage between the two years.

4.  Target buildings should have known site location: buildings with
    anonymous location can not request typical meteorological weather.

5.  Warehouse and parking types are excluded: target buildings that are
    likely to have non-regular electricity and chilled water usage
    should be excluded.

As a summary, the resulting subset contains all buildings with 'tidy'
measurements. Figure \@ref(fig:tidysite) shows in total, the subset
contains `r nrow(all_names_tidy)` buildings in `r nrow(all_types_tidy)`
types from `r nrow(all_sites_tidy)` different climate zones.

```{r tidysite, fig.cap = "Site summary of the tidy building subset", fig.width=8, fig.height=6}

set3 <- colorRampPalette(brewer.pal('Set3',n=12))
type_colors <- setNames(set3(13), all_types_messy$type)
  
p1 <- df_energy_tidy %>%
    group_by(type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, n, .desc = F)) %>%
    ggplot(aes(x = 1, y = n, fill = as.factor(type))) +
    geom_col() +
    geom_text(aes(label = ifelse(n > 10, as.character(n), "")), color = "black", position = position_stack(vjust = 0.5)) +
    scale_y_continuous(expand = c(0, 0),
                       breaks = breaks_pretty(n = 4)) +
    scale_x_discrete(expand = c(0, 0.1)) +
    scale_fill_manual(values = type_colors) +
    labs(x = NULL,
         y = NULL,
         subtitle = "Across all sites",
         fill = NULL) +
    theme(axis.text.x = element_blank(),
          panel.grid.major.y = element_line(color = "grey80"),
          legend.direction = "horizontal",
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  p2 <- df_energy_tidy %>%
    group_by(site, type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    mutate(proportion = n / sum(n),
           total = sum(n),
           ymax = cumsum(proportion),
           ymin = c(0, head(ymax, n = -1))) %>%
    mutate(label_pos = (ymax + ymin) / 2) %>%
    ungroup() %>%
    group_by(type) %>%
    mutate(order = sum(n)) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, order, .desc = F)) %>%
    ggplot(aes(ymax = ymax, ymin = ymin, xmax = 4, xmin = 3, fill = type)) +
    geom_rect() +
    coord_polar(theta = "y") +
    xlim(c(2, 4)) +
    facet_wrap(~ site, nrow = 3) +
    labs(x = NULL,
         y = NULL,
         subtitle = "For each site",
         fill = NULL) +
    scale_fill_manual(values = type_colors) +
    geom_text(aes(x = 3.5, y = label_pos, label = n)) +
    geom_text(aes(x = 2, y = 0, label = paste0("Total\n", total)), color = "black") +
    theme(legend.direction = "horizontal",
          axis.text = element_blank(),
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  ggarrange(p1, p2,
            ncol = 2, nrow = 1,
            widths = c(0.2, 1),
            common.legend = TRUE,
            legend = "bottom") +
    plot_annotation(title = "Case study building type summary")
```

### 'Messy' subset

The 'tidy' subset might be presentative to the measurements collected
among the existing building stock since in reality, whole-building
electricity measurements collected over two years normally exhibit
larger changes. Those measured changes can be caused by sensor itself
such as lack of calibration, or inherent changes of the building, which
illustrated before, can bias the result of a M&V. Therefore, to compare
the robustness of the two M&V method more realistically, we included an
additional 'messy' subset which first exclude the 'tidy' subset and then
re-apply the filtering rule with one amendment:

3.  Absolute mean difference of the two-year electricity usage \< 25%:
    any increase or decrease of building electricity usage in the second
    year should be less than 25% of that in th first year.

Figure \@ref(fig:messysite) shows the summary of the 'messy' dataset. As
this is less aggressive, it contains `r nrow(all_names_messy)` buildings
in `r nrow(all_types_messy)` types from `r nrow(all_sites_messy)`
different climate zones.

```{r messysite, fig.cap = "Site summary of the messy building subset", fig.width=8, fig.height=8}

p1 <- df_energy_messy %>%
    group_by(type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, n, .desc = F)) %>%
    ggplot(aes(x = 1, y = n, fill = as.factor(type))) +
    geom_col() +
    geom_text(aes(label = ifelse(n > 10, as.character(n), "")), color = "black", position = position_stack(vjust = 0.5)) +
    scale_y_continuous(expand = c(0, 0),
                       breaks = breaks_pretty(n = 4)) +
    scale_x_discrete(expand = c(0, 0.1)) +
    scale_fill_manual(values = type_colors) +
    labs(x = NULL,
         y = NULL,
         subtitle = "Across all sites",
         fill = NULL) +
    theme(axis.text.x = element_blank(),
          panel.grid.major.y = element_line(color = "grey80"),
          legend.direction = "horizontal",
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  p2 <- df_energy_messy %>%
    group_by(site, type) %>%
    distinct(name) %>%
    summarise(n = n()) %>%
    mutate(proportion = n / sum(n),
           total = sum(n),
           ymax = cumsum(proportion),
           ymin = c(0, head(ymax, n = -1))) %>%
    mutate(label_pos = (ymax + ymin) / 2) %>%
    ungroup() %>%
    group_by(type) %>%
    mutate(order = sum(n)) %>%
    ungroup() %>%
    mutate(type = fct_reorder(type, order, .desc = F)) %>%
    ggplot(aes(ymax = ymax, ymin = ymin, xmax = 4, xmin = 3, fill = type)) +
    geom_rect() +
    coord_polar(theta = "y") +
    xlim(c(2, 4)) +
    facet_wrap(~ site, nrow = 3) +
    labs(x = NULL,
         y = NULL,
         subtitle = "For each site",
         fill = NULL) +
    scale_fill_manual(values = type_colors) +
    geom_text(aes(x = 3.5, y = label_pos, label = n)) +
    geom_text(aes(x = 2, y = 0, label = paste0("Total\n", total)), color = "black") +
    theme(legend.direction = "horizontal",
          axis.text = element_blank(),
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
  ggarrange(p1, p2,
            ncol = 2, nrow = 1,
            widths = c(0.2, 1),
            common.legend = TRUE,
            legend = "bottom") +
    plot_annotation(title = "Case study building type summary")
```

## Apply control intervention

Figure \@ref(fig:chwst) shows the algorithm for the proposed control
intervention that reset the chiller supply temperature based on the
outdoor weather conditions. Under both strategies, we assume the chiller
is enabled when outdoor temperature is higher than 10°C. The baseline,
which assumed to be the existing measurements from the dataset, runs at
a constant water supply temperature. The intervention as the figure
shows reset from 7 °C to 12 °C.

```{r chwst, fig.cap = "Proposed chilled water supply temperature reset based on outdoor temperature", fig.width=8, fig.height=4}

data.frame(
  OAT = c(10, 15, 25, 30),  
  Baseline = c(6, 6, 6, 6),      
  Intervention = c(12, 12, 7, 7)
  ) %>% 
  pivot_longer(cols = c("Baseline", "Intervention"), names_to = "strategy", values_to = "value") %>% 
  ggplot(aes(x = OAT, y = value, color = strategy)) +
  geom_line() +
  scale_color_manual(values = ls_colors) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = c(4, 6, 8, 10, 12),
                     limits = c(5.5, 12.5),
                     labels = number_format(suffix = " °C")) +
  scale_x_continuous(expand = c(0, 0), 
                     breaks = c(10, 15, 20, 25, 30), 
                     labels = number_format(suffix = " °C")) +
  labs(x = "Outdoor temperature", 
       y = "Chilled water supply temperature", 
       title = "Chilled water supply temperature reset on outdoor temperature") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

```

We map the chilled water supply temperature reset to the electrical
energy savings as: $$
\text{savings} = mean(E_{CHW}) \times 25\% \times \text{perc_savings}
$$

We assume in general, HVAC uses around 50% of whole-building electricity
and the chilled water plant further uses 50% electricity of the HVAC
system. This assumption largely simplifies the unique characteristics of
electricity usage in all types of commercial buildings, but for the
scope of this paper, we therefore assume 25% of whole-building
electricity is consumed by the chilled water plant ($E_{CHW}$).
<!--# TODO: Add reference -->In most cases, savings from an intervention
is unproportional to the building hourly electricity usage, which
renders M&V challenging. Therefore, we map the resulting electricity
savings as a percentage of the mean electricity consumption of the plant
determined by outdoor temperature ($OAT$), intervention supply water
temperature ($T_{interv}$), baseline supply water temperature
($T_{base}$) and hour of the day ($\delta_{occ}$, binary indicator
whether during peak hours from 9 AM to 4 PM).

$$
\text{perc_savings = }
\begin{cases}
0, & \text{ if } OAT < 10°C \\
\left[ \beta_{\text{temp}} \times (\text{T}_{\text{interv}} - \text{T}_{\text{base}}) \right] \times 
\left[ \beta_{\text{occ}} \times \delta_{occ} + \beta_{\text{unocc}} \times (1 - \delta_{occ})\right], & \text{ otherwise}
\end{cases}
$$

Parameters and their pre-defined values are summarized in the table
below. Those parameters were not rigorously calibrated for each
building, and were used uniformly across the dataset. Although this
calibration process should be included, it is considered tangential to
the study scope outlined in this paper.

| Parameter              | Description                                                      | Value |
|------------------|------------------------------------|------------------|
| $\beta_{temp}$         | \% savings from setting $T_{interv}$ 1 °C higher than $T_{base}$ | 0.08  |
| $\beta_{occ}$          | \% savings adjustment during occupied hours                      | 1.2   |
| $\beta_{\text{unocc}}$ | \% savings adjustment during unoccupied hours                    | 0.8   |

: Parameters for calculating the intervention savings

## Apply example non-routine event

To demonstrate the influence of non-routine events that are not properly
adjusted, we developed three scenarios to quantify the energy
consumption change associated with a hypothetical occupancy change. In
summary, we assume an increase in occupancy in 2016 (i.e. refered as the
'baseline year' if using conventional method) resulted in 20% increase
in whole-building measured electricity usage. The three scenarios
specifies the increase happened during January to April (S1); May to
August (S2); and September to December (S3). We applied such change only
to the buildings in the 'tidy' subset as originally they are more likely
to have no non-routine events.

## Run M&V methods

### Conventional M&V

```{r convmethod, fig.cap="Flow chart showing the conventional M&V process for an energy-saving intervention", out.width="50%"}

knitr::include_graphics(paste0(fig_path, "convmethod.png"))
```

As described in Section \@ref(introduction) and Figure
\@ref(fig:convmethod), conventional method for M&V is time-consuming and
unable to separate non-routine event impact from measured savings. If
using conventional method to estimate intervention energy savings, the
result (after 24 months) is calculated by the difference between
measured intervention and projected baseline in the post-retrofit
period. In this study, we leveraged a piece-wise linear regression
considering time-of-week and outdoor temperature (TOWT) as independent
variables for projection and normalization on typical meteorological
year <!--# TODO: Add reference -->.

### Randomized M&V

```{r randmethod, fig.cap="Flow chart showing the novel randomized M&V process for an energy-saving intervention", out.width="50%"}
 
knitr::include_graphics(paste0(fig_path, "randmethod.png"))
```

Compared to the conventional method, the randomized M&V can provide an
estimation more rapid and reliable. Section \@ref(introduction)
describes the application of randomization techniques and Figure
\@ref(fig:randmethod) specifically outlined the process of conducting a
randomized switchback experiment for control retrofit projects. The
method sequentially evaluate the intervention effect and returns a
saving estimation with 95% confidence interval when all stopping
criteria are satisfied. <!--# TODO: Add paper DOI --> describes all
stopping criteria and recommened threshold to consider in more details
and we breifly summarized the thresholds used in this paper here:

-   The HVAC system operates from 06:00 to 22:00 each day, so we use a
    daily sampling interval with the sampling time at midnight each day.

-   Block by day of the week with a block period of 12 weeks.

Stopping criteria are:

-   A minimum and maximum of 12 and 108 weeks respectively. The
    randomized schedule covers the entire two-year period but stopping
    criteria enables an early stop at the end of satisfied blocking
    period.

-   At least 80% of the drybulb temperature range in the annual TMY data
    sampled by both strategies.

-   Test for no carryover effect using a t-test with a p-value not
    exceeding a defined significance threshold of 0.05.

-   90% confidence that energy savings exceed or do not exceed 0% using
    the SPRT test. Medium effect size (d = 0.5) quantified by cohen's d
    and calculated SPRT statistics either falls below the lower
    threshold or exceeds the upper threshold.

-   As no baseline data is available, test with an equal sampling ratio
    (50% baseline, 50% intervention).

To remain consistency, savings normalization on typical meteorological
year is also modeled through TOWT.

# Results

## M&V methods comparison

In this section, we compare the performance of two M&V methods. The key
aspects of the assessment include: 1) time required to reach a saving
estimation: in most cases, a shorter M&V timeline reduces associated
cost and interruption for the building owner; and 2) saving estimation
accuracy: this is particularly important for any software-as-a-service
company to set a reasonable price with their customers.

### Savings estimation time

The timeline for the conventional M&V method is outlined in previous
sections. Although ASHRAE Guideline 14 offers minimum requirements for
each path, the required baseline period is either a full range of all
independent variables (typically outdoor weather conditions) under
normal facility operation or a 12-month worth of continuous
measurements. The same requirements also applies to intervention
installed in the post-retrofit period. Thus, the conventional M&V method
is likely to run over 24 months or even longer due to missing data. For
example if an analyst is asked to determine the energy savings of a
chilled water plant retrofit but missed most of the cooling season due
to delay in retrofit deployment, he/she needs to wait until the next
cooling season to measure the savings.

If the randomized M&V method is applicable, even if there is a delay in
the retrofit deployment, the building owner faces less risk and impact
since random sampling allows measuring the full range of all independent
variables simultaneously among all control strategies or modes. In
addition, sampling at an equal probability (i.e. 50%/50% between
baseline/intervention) effectively balances the level of independent
variables measured. In other words, unless one strategy is sampled with
a large amount of consecutive days (e.g. 7 days or more), it is rare
that the independent variable, such as outdoor weather condition, is
measured significantly different among all sampled control strategies.
Figure \@ref(fig:timeline) shows the average estimated timeline for each
building across all sites and climate conditions if using randomized M&V
for the applied chilled water supply temperature reset intervention. The
timeline figures for each individual building are attached in the
supplementary material. The figure shows for all climate zones, covering
a sufficient range outdoor weather condition is the most stringent
criterion. However, most buildings can satisfy the 80% range by 6 months
and all buildings can quantitatively determine the savings with
associated uncertainty by 9 months, which is even shorter than the
length of baseline measurement required by the conventional method.

```{r timeline, fig.cap="Average randomized M&V timeline summary for all buildings at each site", fig.width=8, fig.height=4}

df_sprt_all_messy %>% 
  filter(seq == "eob" | seq == "temp") %>% 
  bind_rows(df_sprt_all_tidy %>% filter(seq == "eob" | seq == "temp")) %>% 
  group_by(site, seq) %>% 
  summarise(n_weeks = mean(n_weeks)) %>% 
  ungroup() %>% 
  mutate(seq = as.factor(seq), 
         seq = recode_factor(seq, "temp" = "80% temperature range satisfied", "eob" = "remaining weeks till finish")) %>% 
  ggplot() +
  geom_col(aes(x = site, y = n_weeks, fill = seq), position = "identity") +
  labs(x = "Site", 
       y = "Number of weeks", 
       fill = NULL, 
       title = "Average finishing time for all buildings using randomized M&V") +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = seq(0, 36, by = 12)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

```

### Savings estimation accuracy

In this study, we define the overall target savings (i.e. the ground
truth) that an M&V analyst wishes to detect, either in normalized
savings, fractional savings or simply the measured difference between
the baseline and the intervention measurements, is the mean electricity
energy reduction over the two-year period from implementing the chilled
water supply temperature reset control. We estimate the M&V savings by
the two methods following the process given in Figure
\@ref(fig:convmethod) and Figure \@ref(fig:randmethod). In orther words,
we compare the conventional M&V savings at the end of the two-year
period with the randomized M&V savings after all criteria are satisfied.

Figure \@ref(fig:norm) shows the overall comparison of the savings
estimation from all buildings normalized on the typical meteorological
year weather conditions of each site. In subplot a), the narrower range
of the true savings indicates the dependence of the intervention effect
on the outdoor weather condition, which is intended as shown in Figure
\@ref(fig:chwst). Sites with mild climate all year round such as
locations in California shows higher savings potential above 10%, while
locations with more extreme climate such as Washington DC shows only 6%
savings annually. In addition, although the box plot indicates some
savings estimation are 'outliers', they should not be interpreted as
such since the distribution can be biased by the unbalanced sample sizes
across different site shown in Figure \@ref(fig:tidysite) and Figure
\@ref(fig:messysite). In subplot b), the results show that the
conventional M&V method tends to estimate savings with a greater
uncertainty and we will demonstrate in the following sections that
non-routine events can have a significant impact. For the randomized
method, we displayed two scenarios, the estimation at the end of
two-year period with 50%/50% sampling throughout and the estimation
provided after all stopping criteria satisfied (i.e. stops early at 24
weeks or 36 weeks). Since we found no significant difference between the
two distributions as shown in the figure, we demonstrated that an early
stop has no impact on the estimation accuracy but can significantly
reduces the cost associated with M&V.

```{r norm, fig.cap="Comparison of normalized annual fractional savings on site TMY between conventional and randomized method", fig.width=8, fig.height=4}

p1 <- df_FS_tidy %>% 
  bind_rows(df_FS_messy) %>% 
  filter(method == "true") %>% 
  mutate(method = "true savings") %>% 
  ggplot(aes(x = method, y = savings, fill = method)) +
  stat_boxplot(geom ='errorbar', width = 0.5) +
  geom_boxplot(outlier.size = 1) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     limits = c(0, 30), 
                     labels = number_format(suffix = "%")) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

p2 <- df_FS_tidy %>% 
  bind_rows(df_FS_messy) %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  left_join(df_seq_FS_tidy %>% 
              bind_rows(df_seq_FS_messy) %>% 
              filter(seq == "eob"), by = c("name", "site")) %>% 
  pivot_longer(c(conv, rand, FS), names_to = "parameter", values_to = "value") %>% 
  mutate(parameter = as.factor(parameter), 
         parameter = recode_factor(parameter, 
                                   "conv" = "Conventional M&V", 
                                   "rand" = "Randomized M&V", 
                                   "FS" = "Randomized M&V at early stop")) %>% 
  ggplot(aes(x = parameter, y = value, fill = parameter)) +
  stat_boxplot(geom ='errorbar', width = 0.5) +
  geom_boxplot(outlier.shape = NA) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     limits = c(0, 30)) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

  ggarrange(p1, p2,
            labels = c("a)", "b)"),
            ncol = 2, nrow = 1,
            widths = c(0.35, 1),
            legend = "bottom") +
    plot_annotation(title = "Normalized annual fractional savings estimation comparison", 
                    subtitle = "between convention and randomized M&V (with early stop)")
```

To assess the accuracy on individual building, we calculate the mean
absolute difference between the true savings and estimated savings.
Figure \@ref(fig:tidyfrac) shows the comparison between the two methods
using the tidy subset. Because of the large variation in electricity
energy usage, we express the savings as a normalized value which is a
fraction of energy reduction compared to baseline. Furthermore, we focus
on the median and 95% threshold of the mean absolute difference
distribution in the comparitive assessment to filter out outliers. As a
result, the conventional method performs better than the randomized
method. One hypothesis is that the tidy subset only includes buildings
exposed to similar usage throughout the two years, meaning the
measurement used for regression model fitting resembles the model
prediction results. Although the results indicate the conventional M&V
method is preferable if a building can achieve long-term consistency in
electricity usage, such condition is rarely garrenteed in reality.

```{r tidyfrac, fig.cap="Absolute deviation of M&V estimated fractional savings from true target savings using tidy subset", fig.width=8, fig.height=4}

plot_data <- df_seq_FS_tidy %>% 
  filter(seq == "eob") %>% 
  left_join(df_FS_tidy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - FS), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p1 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_tidy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.5)) +
  labs(x = "Number of buildings", 
       y = "Absolute difference in measured savings", 
       subtitle = "Randomized M&V at early stop") +
  coord_cartesian(ylim = c(0, 10)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

plot_data <- df_FS_tidy %>% 
  filter(method != "rand") %>% 
  group_by(name) %>% 
  summarise(abs_diff = abs(diff(savings))) %>% 
  ungroup() %>% 
  mutate(plot_max = max(abs_diff))

median_conv <- median(plot_data$abs_diff)
upper_conv <- quantile(plot_data$abs_diff, probs = 0.95)

p2 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_tidy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_conv + 0.5,
           x = 20, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_conv, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_conv + 0.5, 
           x = 20,
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_conv, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.1)) +
  labs(x = "Number of buildings", 
       y = NULL, 
       subtitle = "Conventional M&V") +
  coord_cartesian(ylim = c(0, 10)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 0, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          align = "hv",
          legend="bottom") +
  plot_annotation(title = str_glue("M&V method accuracy comparison"), 
                  subtitle = "fractional savings in tidy subset")


```

A more realistic condition is shown in the messy subset in Figure
\@ref(fig:messyfrac). The randomized method demonstrates consistent
accuracy in savings estimation, even in the presence of significant
measurement noise, indicating strong robustness. On the contrary, the
conventional method shows approximately in 50% of all cases, the
deviations of M&V estimation are more than 5%. We showed the
non-normalized mean absolute difference comparison plots in the
supplementary material. The poor performance shows if the there exist a
change in electricity usage, baseline projection based on regression
models are unreliable. Those changes are normally referred as
non-routine events, which summarizes all the influential factors that
are not considered in the regression model either because they are
impractical to measure, such as hourly occupancy rate, or too random
with no expected patterns to quantify, such as a sudden change of use
from an office to a warehouse. We will demonstrate this in more details
in the following sections.

```{r messyfrac, fig.cap="Absolute deviation of M&V estimated fractional savings from true target savings using messy subset", fig.width=8, fig.height=4}

plot_data <- df_seq_FS_messy %>% 
  filter(seq == "eob") %>% 
  left_join(df_FS_messy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - FS), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p1 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_messy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 1, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 1, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.5)) +
  labs(x = "Number of buildings", 
       y = "Absolute difference in measured savings", 
       subtitle = "Randomized M&V at early stop") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

plot_data <- df_FS_messy %>% 
  filter(method != "rand") %>% 
  group_by(name) %>% 
  summarise(abs_diff = abs(diff(savings))) %>% 
  ungroup() %>% 
  mutate(plot_max = max(abs_diff))


median_conv <- median(plot_data$abs_diff)
upper_conv <- quantile(plot_data$abs_diff, probs = 0.95)

p2 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_messy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_conv + 1,
           x = 220, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_conv, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_conv, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_conv + 1, 
           x = 220,
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_conv, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.1)) +
  labs(x = "Number of buildings", 
       y = NULL, 
       subtitle = "Conventional M&V") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 0, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          align = "hv",
          legend="bottom") +
  plot_annotation(title = str_glue("M&V method accuracy comparison"), 
                  subtitle = "fractional savings in messy subset")


```

## Non-routine events impact on savings estimation

In this section, we demonstrate the influence of non-routine events
through two scenarios. In the first scenario, we quantified how much
occupancy would influence the electricity usage and added to the tidy
measurement set as they originally indicates no such change. In the
second scenario, we simply run the two M&V methods on the messy
measurement sets before adding the chilled water supply temperature
reset. Since no intervention is added, the more reliable method should
detect a 'saving' closer to 0.

### Occupancy change

Although occupancy can be approximated in several ways in commercial
buildings, such as through counting check-in, or WIFI connections, or
monitoring indoor CO2 concentration as a proxy. However, it is not a
cost-efficient measures to add to the routine operation for most of the
buildings and there might be privacy concerns associated. The scenario
proposed hypothesize that during the baseline measurement period of the
target building, a floor of tenants moved out leaving the space
unoccupied for four months before new tenants moved in and this led to a
20% electricity decrease as a result. The M&V protocol only requires the
M&V analyst to fit a TOWT model so he/she only measures outdoor
temperature. We showed one example in Figure \@ref(fig:occchange) where
from May 1st 2016 to August 31st 2016, the target building electricity
decreased 20% due to reduced occupancy. Thus subplot (a) shows measured
baseline with such change, which are then used for TOWT model fitting.
Starting from 2017, the occupancy returns normal and only intervention
strategy was measured and the projected baseline is the prediction
results from the fitted model. To demonstrate, we also plotted the
original measurements before adding the intervention effect as 'adjusted
baseline'. This assumes that we can accurately adjust the baseline in
the post-retrofit period on back-to-normal occupancy. Subplot (b) shows
that the fitted regression underestimate the true baseline condition
leading to underestimated savings.

```{r occ}

n = 6

site_info <- df_energy_tidy %>%
    filter(name == all_names_tidy$name[n]) %>%
    select(site, type) %>%
    distinct()
  
site <- site_info$site
  
site_weather <- df_weather_tidy %>%
    filter(site == site_info$site) %>%
    select(timestamp, t_out) %>%
    group_by(timestamp) %>%
    summarise(t_out = mean(t_out)) %>%
    ungroup()

df_all <- df_energy_tidy %>%
    filter(name == all_names_tidy$name[n]) %>%
    select(timestamp, eload) %>%
    left_join(site_weather, by = "timestamp")
  
# Linear interpolation of baseline
df_all <- df_all %>%
  run_interpo()

plot_scale <- get_scale(df_all$base_eload)

df_hourly_conv <- df_all %>%
  run_reset()
  
df_base_conv <- df_hourly_conv %>%
    select(datetime,
           eload = base_eload,
           t_out) %>% 
    drop_na()

df_interv_conv <- df_hourly_conv %>%
    select(datetime,
           eload = interv_eload,
           t_out) %>% 
    drop_na()

    
change_start <- occ_params$change_start
change_end <- occ_params$change_end
change <- occ_params$change

base_pre_meas <- df_base_conv %>%
  filter(datetime < as.Date("2017-01-01")) %>%
  mutate(eload = ifelse(month(datetime) >= month(change_start) & month(datetime) <= month(change_end), eload * (1 - change / 100), eload))

# Baseline projection
towt_base <- base_pre_meas %>%
  model_fit()

df_towt <- df_interv_conv %>%
  filter(datetime >= as.Date("2017-01-01")) %>%
  select(time = datetime,
         temp = t_out,
         eload)

base_pos_proj <- model_pred(df_towt, towt_base) %>%
  rename("datetime" = "time") %>%
  select(datetime, towt, eload)

base_pos_true <- df_base_conv %>%
  filter(datetime >= as.Date("2017-01-01"))

interv_pos_meas <- df_interv_conv %>%
  filter(datetime >= as.Date("2017-01-01"))
```

```{r occchange, fig.cap="Occupancy change impact on TOWT model fitting and baseline projection", fig.width=8, fig.height=4}

p1 <- ggplot() +
    geom_point(data = base_pre_meas %>%
                 slice_sample(n = 500),
               aes(x = datetime, y = eload, color = "Measured baseline"),
               size = 0.5,
               alpha = 0.3) +
    geom_smooth(data = base_pre_meas,
                aes(x = datetime, y = eload, color = "Measured baseline"), 
                formula = y ~ x, method = "loess") +
    scale_x_datetime(date_breaks = "2 months",
                     date_labels = "%b")  +
    scale_y_continuous(expand = c(0, 0),
                       breaks = breaks_pretty(n = 3),
                       labels = number_format(suffix = " kW")) +
    scale_color_manual(values = ls_colors) +
    coord_cartesian(ylim = plot_scale) +
    labs(x = NULL,
         y = NULL,
         color = NULL,
         title = NULL,
         subtitle = "Pre-retrofit period") +
    theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
          legend.direction = "horizontal",
          legend.position = "bottom",
          plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
  
p2 <- ggplot() +
  geom_point(data = base_pos_proj %>%
               slice_sample(n = 500),
             aes(x = datetime, y = towt, color = "Projected baseline\n(no change)"),
             size = 0.5,
             alpha = 0.3) +
  geom_smooth(data = base_pos_proj,
              aes(x = datetime, y = towt, color = "Projected baseline\n(no change)"), 
              formula = y ~ x, method = "loess") +
  geom_point(data = base_pos_true %>%
               slice_sample(n = 500),
             aes(x = datetime, y = eload, color = "Adjusted baseline"),
             size = 0.5,
             alpha = 0.3) +
  geom_smooth(data = base_pos_true,
              aes(x = datetime, y = eload, color = "Adjusted baseline"), 
              formula = y ~ x, method = "loess") +
  geom_point(data = interv_pos_meas %>%
               slice_sample(n = 500),
             aes(x = datetime, y = eload, color = "Measured interv"),
             size = 0.5,
             alpha = 0.3) +
  geom_smooth(data = interv_pos_meas,
              aes(x = datetime, y = eload, color = "Measured interv"), 
              formula = y ~ x, method = "loess") +
  scale_x_datetime(date_breaks = "2 months",
                   date_labels = "%b")  +
  scale_y_continuous(expand = c(0, 0),
                     breaks = breaks_pretty(n = 3),
                     labels = number_format(suffix = " kW")) +
  scale_color_manual(values = ls_colors) +
  coord_cartesian(ylim = plot_scale) +
  labs(x = NULL,
       y = NULL,
       color = NULL,
       title = NULL,
       subtitle = "post-retrofit period") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        axis.text.y = element_blank(),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          widths = c(1, 1),
          align = "v",
          legend="bottom") +
  plot_annotation(title = "Building energy consumption with added occupancy change")
```

Figure \@ref(fig:occsavings) shows the normalized fractional savings
estimation accuracy comparison between the two M&V methods. To compare
which method is more robust to the applied occupancy change, a
difference-in-difference value is plotted in the figure. Each bar in the
plot represent the difference in the deviation of savings estimated by
the convention method and the randomized method:

$$
|S_{conv} - S_{true}| - |S_{rand} - S_{true}|
$$

In other words, a positive value indicate the randomized method provides
an estimation more aligned to the true target saving. Thus the plot
shows the randomized method shows uniformly superior robustness for all
sites. The red-dotted line shows the absolute deviation in savings
estimation by the randomized method as a reference for scaling. The
deviation is consistent with the distribution shown in Figure
\@ref(fig:tidysite) and thus further reinforce its robustness to
non-routine events.

```{r occsavings, fig.cap="Savings estimation accuracy comparison between two M&V methods with added occupancy change (with the site average difference-in-difference displayed at the bottom of each site).", fig.width=8, fig.height=4}
s <- 'S4'
dev_conv <- df_NRE_occ %>%
  filter(scenario == s) %>%
  filter(method != "rand") %>%
  group_by(name, site, scenario) %>%
  summarise(conv = abs(diff(savings))) %>%
  ungroup()

dev_rand <- df_NRE_occ %>%
  filter(scenario == s) %>%
  filter(method != "conv") %>%
  group_by(name, site, scenario) %>%
  summarise(rand = abs(diff(savings))) %>%
  ungroup()

dev_FS <- dev_rand %>%
  left_join(dev_conv, by = c("name", "site", "scenario")) %>% 
  mutate(diff_in_diff = conv - rand)

mean_diff <- dev_FS %>%
  group_by(site) %>%
  summarise(mean = round(mean(diff_in_diff), digits = 1),
            pos = round(n() / 2) + 1,
            .groups = 'keep')

dev_FS %>%
  ggplot(aes(group = site)) +
  geom_col(aes(x = name, y = diff_in_diff), position = "identity", alpha = 0.3) +
  facet_wrap(~site, nrow = 1, scales = "free_x") +
  scale_y_continuous(expand = c(0.1, 0),
                     breaks = breaks_pretty(n = 4), 
                     labels = number_format(suffix = "%")) +
  geom_text(data = mean_diff,
            aes(x = pos, y = -.5, group = site, label = paste0(mean, "%"))) +
  geom_line(aes(x = name, y = rand, color = "Absolute deviation of randomized method"), alpha = 0.5) +
  geom_point(aes(x = name, y = rand, color = "Absolute deviation of randomized method"), alpha = 0.5) +
  labs(x = NULL,
       fill = NULL,
       y = NULL,
       color = NULL, 
       title = "Difference-in-difference of fractional savings") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        axis.text.x = element_blank(),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

### No-saving detection

The outbreak of pandemic in 2020 followed by working-from-home policy is
a practical example of building energy consumption influenced by
non-routine events. During unoccupied months, building energy
consumption dropped compared to the same time period in the previous
years. If an energy-efficient intervention was deployed onsite starting
at 2020, given by the whole-building electricity measurements, the large
difference in measured electricity usage in 2020 and projected baseline
usage in 2019 can hardly indicate the retrofit intervention effect.

In other words, if there is no intervention applied or the intervention
effect is known to be null (e.g. constantly being overriden by the
baseline), an ideal M&V method should detect no savings. To test this,
we kept the randomized schedule as sampled earlier but removed added
chilled water plant intervention effect. Therefore, all sampled
intervention days are considered additional sampled baseline days.
Earlier we mentioned the 'messy' dataset contains some change in the
two-year electricity measurement, which is purely due to various
non-routine events such as occupancy change or even other measures
implemented by the building manager but irrelevant to the chilled water
set point reset intervention. Thus, when the proposed chilled water
reset intervention was removed, a more reliable M&V method should
overcome the influence of those confounding factors and inform no
savings.

```{r nosavings, fig.cap="Distribution of savings estimation results by the convention M&V method and randomized method.", fig.width=8, fig.height=4}

df_FS_nsprt %>% 
  pivot_wider(names_from = method, values_from = savings) %>% 
  left_join(df_seq_FS_nsprt %>% filter(seq == "eob"), by = c("name", "site")) %>% 
  pivot_longer(c(true, conv, rand, FS), names_to = "parameter", values_to = "value") %>% 
  filter(parameter != "true") %>% 
  mutate(parameter = as.factor(parameter), 
         parameter = recode_factor(parameter, 
                                   "conv" = "Conventional M&V", 
                                   "rand" = "Randomized M&V", 
                                   "FS" = "Randomized M&V at early stop")) %>% 
  ggplot(aes(x = parameter, y = value, fill = parameter)) +
  stat_boxplot(geom ='errorbar', width = 0.2) +
  geom_boxplot(outlier.shape = NA, width = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1.2) +
  annotate(geom = "text", 
           x = 0.5, 
           y = 0.5, 
           color = 'red',
           label = "True savings = 0") +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 5), 
                     limits = c(0, 16), 
                     labels = number_format(suffix = "%")) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL, 
       title = "Savings estimation comparison", 
       subtitle = "between convention and randomized M&V (with early stop)") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

Figure \@ref(fig:nosavings) shows that the conventional M&V method in
average detected 5% savings when there should be none. The randomized
method even when stops right after satisfying all criteria, the savings
estimation is much closer to 0%. This is mostly because when there is a
change or disturbance applied to the target building such as lighting
retrofit or thermal leakage in the envelope, randomly sample the two
strategies at 50%/50% ensures the resulting effect either as a decrease
in lighting electricity or an increase in reheat electricity is balanced
between the two implemented strategies.

# Discussion

## TOWT modeling accuracy

Although this paper argues that regression model prediction results can
be largely biased by various non-routine events, we further clarify that
this is not related to regression model fitting accuracy. Time-of-week
temperature model is used here for its simplicity and convenience since
the model only requires outdoor temperature measurements and time of
week as independent variables. The model assumes a linear composition of
building energy consumption as temperature-dependent and time-dependent
load. The time-dependent component accounts for day-to-day variations
and the temperature dependent component considers a piecewise linear
relationship across a range of temperature intervals.
<!--# TODO: add reference -->

To assess modeling accuracy, we used the Coefficient of Variation of
Root-Mean Squared Error, or CV(RMSE) as the error metric. Since this
metric is calculated as a normalized value, it is useful to compare
different model fitting results. Guideline 14 requires that
whole-building baseline model fitting accuracy should maintain a
CV(RMSE) lower than 30%. In addition One study focusing on the baseline
energy data-driven model fitting indicates that TOWT performs as
accurate as other more advanced machine learning models
<!--# TODO: reference --> and the calculated CV(RMSE) distribution for a
large sample of commercial buildings indicates a median of 20%. Figure
\@ref(fig:towtacc) plots the distribution of the model fitting accuracy
calculated separately for the two measurement sets. The result shows
that the TOWT model performance is even better in this study compared to
the literature and no significant difference between the two subsets.
This is mostly related to low-quality measurement sets filtering in the
pre-processing mentioned in Section \@ref(method).

```{r towtacc, fig.cap="TOWT model fitting accuracy distribution on filtered measurement set", fig.width=8, fig.height=4}

df_model_acc_messy %>% 
  mutate(group = "messy") %>% 
  rbind(df_model_acc_tidy %>% mutate(group = "tidy")) %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x = group, y = cvrmse, fill = group)) +
  stat_boxplot(geom ='errorbar', width = 0.5) +
  geom_boxplot(outlier.shape = NA) +
  geom_text(aes(x = group, y = median, label = paste0(round(median, digits = 0), " %")), 
            data = . %>% 
              group_by(group) %>% 
              summarise(median = median(cvrmse)) %>% 
              ungroup(), 
            position = position_nudge(y = 1)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %")) +
  coord_cartesian(ylim = c(0, 38)) +
  labs(x = NULL, 
       y = NULL, 
       fill = NULL, 
       title = "TOWT model accuracy", 
       subtitle = "CV(RMSE)") +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))
```

In general, the limitation is that despite the regression model tends to
capture well on the mean building energy consumption but can
underestimate and peak or overestimate the lower base load
<!--# TODO: reference -->, which is not useful for assessing demand
response. Additionally, a data-driven model can learn the training
dataset very efficiently with high accuracy, but the well-trained model
is challenging to generalize prediction outside the range of training
set.

## Sampling ratio for randomized M&V

Another advantage of using randomized M&V is the flexibility of changing
sampling ratio after the M&V. For example, the building owner can
continue sampling at 50%/50% between the baseline and the intervention
to further reduce the uncertainty range associated with the savings. Or
he/she can switch to 100% intervention to optimize energy savings, but
whether the existing baseline becomes outdated is unknown. A compromised
approach is to continue sampling at 80%/20% between the baseline and the
intervention. Figure \@ref(fig:cont) shows after the analyst reports the
randomized M&V results, a new schedule sampling at 80%/20% was
implemented for another 36 weeks.

```{r cont, fig.cap="Savings estimation accuracy for continue sampling at 80%/20% for 36 weeks after all stopping criteria are satisfied", fig.width=8, fig.height=4}

plot_data <- df_cont_FS_tidy %>% 
  rename(cont = FS) %>% 
  left_join(df_FS_tidy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - cont), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p1 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_tidy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 0.5, 
           x = 20, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.5)) +
  labs(x = "Number of buildings", 
       y = "Absolute difference in measured savings", 
       subtitle = "Tidy subset") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        plot.margin = margin(t = 2, r = 7, b = 2, l = 2, unit = "mm"))


plot_data <- df_cont_FS_messy %>% 
  rename(cont = FS) %>% 
  left_join(df_FS_messy %>% filter(scenario == "ref" & method == "true"), by = c("name", "site")) %>% 
  mutate(abs_diff = abs(savings - cont), 
         plot_max = max(abs_diff))

median_rand <- median(plot_data$abs_diff)
upper_rand <- quantile(plot_data$abs_diff, probs = 0.95)

p2 <- plot_data %>% 
  arrange(abs_diff) %>% 
  ggplot() +
  geom_bar(aes(x = seq(1, nrow(all_names_messy), by = 1), y = abs_diff), 
           stat = "identity", alpha = 0.5) +
  geom_hline(yintercept = median_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = median_rand + 0.5, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "50% buildings: < ", round(median_rand, digits = 1), " %)"),
           size = 4) +
  geom_hline(yintercept = upper_rand, 
             linetype = "dashed", 
             color = "red") +
  annotate(y = upper_rand + 0.5, 
           x = 220, 
           geom = "text", 
           label = paste0("(", "95% buildings: < ", round(upper_rand, digits = 1), " %)"),
           size = 4) +
  scale_x_continuous(expand = c(0.02, 0), 
                     breaks = breaks_pretty(n = 3)) +
  scale_y_continuous(expand = c(0, 0), 
                     breaks = breaks_pretty(n = 3), 
                     labels = number_format(suffix = " %"),
                     limits = c(0, max(plot_data$plot_max) + 0.1)) +
  labs(x = "Number of buildings", 
       y = NULL, 
       subtitle = "Messy subset") +
  coord_cartesian(ylim = c(0, 25)) +
  theme(panel.grid.major.y = element_line(color = "grey80", linewidth = 0.25),
        legend.direction = "horizontal",
        legend.position = "bottom",
        axis.text.y = element_blank(),
        plot.margin = margin(t = 2, r = 7, b = 2, l = 0, unit = "mm"))

ggarrange(p1, p2,
          ncol=2, nrow=1,
          labels = c("a)", "b)"),
          align = "hv",
          legend="bottom") +
  plot_annotation(title = str_glue("Continue sampling accuracy comparison"), 
                  subtitle = "with 80%/20% intervention/baseline")
```

The deviation remains consistent in the tidy subset but increases
slightly in the messy subset, suggesting that changes in energy usage in
the target building create a trade-off between measured savings and
estimation accuracy when down-sampling the baseline. In this scenario,
opting for an early stop at 24 or 36 weeks with 50% intervention and
resampling 80% of the data for 36 weeks still allows the building owner
to capture 65% of the full-range savings.

## Limitations and future study

Considering the diversity of the measurement sets, the assumption made
in this study is relative general. For example,

# Conclusion
